# 01 Preliminary Knowledge

## 1.3 Gradient Dscent

### 梯度下降(Gradient Descent)
梯度下降是一种广发应用于机器学习和优化问题的数值优化算法，用于最小化或最大化目标函数。它通过迭代地调整参数，使得每次调整后的参数都能使目标函数的值减小（或增大）。

**梯度下降算法一般流程：**
1. 初始化：初始化模型参数的初值 $\theta_0$；
2. 计算梯度：在当前参数下，计算目标函数对参数的梯度 $\nabla_{\theta}J(\theta)$；
3. 更新参数：按照负梯度的方向调整参数，即 $\theta_{t+1} = \theta_t - \eta \cdot \nabla_{\theta}J(\theta)$，其中 $\eta$ 是学习率（步长）；
4. 重复步骤2和3，直到达到某个停止条件，如达到最大迭代次数或梯度变化小于某个阈值。


```python
# compute_gradient
def compute_gradient(theta, x, y):
    """ Compute the gradient for linear regression. """

    gradients = 2 / y.shape[0] * (x.T @ (x @ theta - y))
    return gradients.ravel()

# gradient_descent
def gradient_descent(x, y, learning_rate=0.01, num_iters=1000):
    """ Gradient descent algorithm """

    # Initialize theta parameters
    theta = np.zeros(x.shape[1])
    for _ in range(num_iters):
        gradients = compute_gradient(theta, x, y)
        theta -= learning_rate * gradients

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = gradient_descent(X, y)
print("Theta:", theta)
```

    Theta: [2.60102170e-04 4.99834982e-01 5.00095084e-01]


### 批量梯度下降（Batch Gradient Descent）
是梯度下降的一个变种，每次迭代使用所有训练数据来计算梯度。特点是每次更新参数时都需要遍历整个数据集，因此优化方向准确、稳定性高、易于实现，但收敛速度较慢、计算量大、内存占用大，需要小一点的学习率，但是最终结果较为准确。

**批量梯度下降算法一般流程：**
1. 初始化：初始化参数向量 $\theta$;
2. 计算梯度：遍历整个训练集，计算损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta^{(t)}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta^{(t)})$;
3. 更新参数：使用学习率 $\eta$ 更新参数向量 $\theta$，即 $\theta = \theta - \eta \cdot g(\theta^{(t)})$;
4. 重复步骤2和3，直到达到预定的迭代次数或损失函数 $J(\theta)$ 收敛到某个阈值。


```python
# batch gradient descent
def batch_gradient_descent(x, y, learning_rate=0.01, num_iters=1000):
    """ Batch Gradient Descent algorithm """

    # Initialize parameters
    theta = np.zeros(x.shape[1])

    for _ in range(num_iters):
        gradients = compute_gradient(theta, x, y)

        theta -= learning_rate * gradients

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = batch_gradient_descent(X, y)
print("Theta:", theta)
```

    Theta: [2.60102170e-04 4.99834982e-01 5.00095084e-01]


### 随机梯度下降（Stochastic Gradient Descent）
随机梯度下降（SGD）是一种优化算法，用于最小化一个函数。它通过迭代地调整模型参数来逐步减少损失函数的值。与批量梯度下降不同，SGD在每次迭代中只使用一个样本或一个小批量的样本来计算梯度。这使得SGD比批量梯度下降更快、能够有效避免陷入局部最优解，特别是在数据集非常大的情况下。但由于其随机性，可能会导致收敛过程更加不稳定，并且对学习率的选择比较敏感，同时由于随机选择样本或批量样本，导致梯度估计存在一定的噪声。

**随机梯度下降一般流程：**
1. 初始化：初始化参数向量 $\theta$;
2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta^{(t)}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta^{(t)})$，其中 $m$ 是样本数量;
4. 更新参数：使用学习率 $\eta$ 更新参数向量 $\theta$，即 $\theta = \theta - \eta \cdot g(\theta^{(t)})$;
5. 重复步骤2-4，直到达到预定的迭代次数或损失函数 $J(\theta)$ 收敛到某个阈值。


```python
# Stochastic Gradient Descent
def stochastic_gradient_descent(x, y, batch_size=2, learning_rate=0.01, num_iters=1000):
    """ Stochastic Gradient Descent algorithm """

    # Initialize parameters
    theta = np.zeros(x.shape[1])
    for _ in range(num_iters):
        # Shuffle the data
        indices = np.random.choice(x.shape[0], batch_size, replace=False)
        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # Mini-batch gradient descent
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)
        # Update parameters
        theta -= learning_rate * gradients
    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = stochastic_gradient_descent(X, y, learning_rate=0.01, num_iters=1000)
print("Theta:", theta)
```

    Theta: [2.52004635e-04 4.99846850e-01 5.00098855e-01]


### 动量梯度下降（Momententum Gradient Descent）
动量梯度下降是一种优化算法，用于加速训练过程。它通过引入一个动量项来减少震荡并加快收敛速度。

**动量梯度下降一般流程：**
1. 初始化：设置学习率 $\eta$ 和动量系数 $\beta$（通常在0到1之间），以及初始参数 $\theta$；
2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta^{(t)}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta^{(t)})$，其中 $m$ 是样本数量;
4. 更新速度：使用动量公式更新速度 $v_{t+1} = \beta v_{t} - \eta g(\theta^{(t)})$，其中 $v_0 = 0$;
5. 更新参数：使用学习率 $\eta$ 更新参数向量 $\theta$，即 $\theta_{t+1} = \theta_{t} + v_t$，其中 $v_0 = 0$;
6. 重复步骤2-5，直到达到预定的迭代次数或损失函数 $J(\theta)$ 收敛到某个阈值。


```python
# Momententum Gradient Descent
def momentum_gradient_descent(x, y, learning_rate=0.1, batch_size=None, momentum=0.9, num_iters=1000):
    """ Momentum Gradient Descent Algorithm """

    # Initialize velocity
    v = np.zeros(x.shape[1])
    # Initialize parameters
    theta = np.zeros(x.shape[1])
    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)
        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # Compute gradients
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)
        # Update velocity
        v = momentum * v - learning_rate * gradients
        # Update parameters
        theta += v  # notice the difference here
    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = momentum_gradient_descent(X, y, learning_rate=0.01, num_iters=1000)
print("Theta:", theta)
```

    Theta: [-4.88015023e-16  5.00000000e-01  5.00000000e-01]


### Nesterov 加速梯度下降 (Nesterov Accelerated Gradient, NAG)
Nesterov 加速梯度下降是一种改进的动量梯度下降算法，它结合了动量和当前梯度的信息。NAG 的核心思想是使用未来的梯度来更新参数，而不是当前的梯度。这使得 NAG 在某些情况下比标准动量方法表现得更好。

**Nesterov Accelerated Gradient (NAG) 算法一般流程：**
1. 初始化：初始化学习率 $\eta$、动量系数 $\beta$ 和参数向量 $\theta_0$；
2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 更新未来位置：使用动量更新参数$\tilde{\theta} = \theta_t + \beta \cdot v_t$;
4. 计算损失函数关于未来位置的梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\tilde{\theta}$ 的梯度：$g(\tilde{\theta}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\tilde{\theta})$，其中 $m$ 是样本数量;
5. 更新速度：$v_{t+1} = \beta \cdot v_t + \eta \cdot g(\tilde{\theta})$;
6. 更新参数：$\theta_{t+1} = \theta_{t} - v_{t+1}$;
7. 重复步骤2-6，直到达到预定的迭代次数或损失函数 $J(\theta)$ 收敛到某个阈值。


```python
# Nesterov Accelerated Gradient (NAG)
def nesterov_accelerated_gradient(x, y, batch_size=2, learning_rate=0.1, momentum=0.9, num_iters=1000):
    """ Nesterov Accelerated Gradient (NAG) 优化算法实现 """

    # initialize parameters
    theta = np.zeros(x.shape[1])
    # initialize velocity
    v = np.zeros_like(theta)

    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)
        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # update theta
        theta_lookahead = theta + momentum * v
        # Compute gradients
        gradients = compute_gradient(theta_lookahead, x_shuffled, y_shuffled)
        # Update velocity
        v = momentum * v - learning_rate * gradients
        # Update parameters
        theta += v  # notice the difference here
    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = nesterov_accelerated_gradient(X, y, learning_rate=0.01, num_iters=1000)
print("Theta:", theta)
```

    Theta: [6.20745421e-16 5.00000000e-01 5.00000000e-01]


### 自适应梯度下降（AdaGrad）
AdaGrad是一种自适应学习率的优化算法，它根据参数的历史梯度平方和来调整每个参数的学习率。这样可以确保在不同维度上使用不同的学习率，从而加速收敛并避免震荡。

**自适应梯度下降一般流程：**
1. 初始化：初始化学习率 $\eta$、累积梯度向量和$\mathbf{G}_0$和参数向量 $\theta_0$；
2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta_{t}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta_{t})$，其中 $m$ 是样本数量;
4. 更新学习率：计算新的学习率 $ \eta_t = \frac{\eta}{\sqrt{G_t + \epsilon}} $，其中 $\mathbf{G}_t$ 是累积梯度向量, $\epsilon$ 是一个小的常数以避免除零错误；
5. 更新参数：$\theta_{t+1} = \theta_t - \eta_t \cdot g(\theta_{t})$；
6. 更新累积梯度：$\mathbf{G}_{t+1} = \mathbf{G}_t + g(\theta_{t})^2$；
7. 重复步骤2-6，直到达到预定的迭代次数或损失函数 $J(\theta)$ 收敛到某个阈值。


```python
# AdaGrad
def adagrad(x, y, batch_size=2, learning_rate=0.1, num_iters=1000, epsion=1e-6):
    """ AdaGrad """
    # initialize parameters
    theta = np.zeros(x.shape[1])
    # initialize gradients
    G = 0.0

    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)

        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # compute gradients
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)
        # update G
        G += gradients ** 2
        # update learning rate
        lr = learning_rate / np.sqrt(G + epsion)
        # update parameters
        theta -= lr * gradients

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = adagrad(X, y, batch_size=1, learning_rate=0.5, num_iters=1000)
print("Theta:", theta)
```

    Theta: [0.04197379 0.54197379 0.45802621]


### Adadelta
Adadelta是一种自适应学习率优化算法，它是对Adagrad算法的扩展，旨在减少其学习率单调递减的问题。在Adagrad算法中，学习率会随着时间逐渐减小，这可能导致学习过程过早停止。Adadelta通过限制累积历史梯度的大小来解决这个问题。**Adadelta算法的主要思想** 是使用一个窗口来累积过去梯度的平方，而不是从开始到现在的所有梯度。这样，学习率就不会随着时间无限减小。具体来说，Adadelta使用了一个参数ρ（通常设置为一个接近1的值，如0.9）来控制窗口的大小。

**Adadelta一般流程：**
1. 初始化：初始化参数$\theta_0$，累积平方梯度的指数加权平均$s=0$，累积更新量的指数加权平均$r=0$；
2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta_{t}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta_{t})$，其中 $m$ 是样本数量;
4. 更新累积平方梯度的指数加权平均：$s_{t}= \rho s_{t-1} + (1 - \rho)(g(\theta_{t}))^2$
5. 修正梯度：$g'_{t} = \frac{\sqrt{r_{t-1} + \epsilon}}{\sqrt{s_{t-1}} + \epsilon} g(\theta_{t-1})$；
6. 更新累积更新量的指数加权平均：$r_{x_t} = \rho r_{x_{t-1}} + (1 - \rho) {g'_{t}}^2$；
7. 更新模型参数：$\theta_{t} = \theta_{t} - g'_{t}$；
8. 重复步骤2-7直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\theta)$ 的值小于某个阈值）。


```python
# Adadelta
def adadelta(x, y, batch_size=None, rho=0.9, num_iters=1000, epsilon=1e-6):
    """ Adadelta """

    # initialize parameters
    theta = np.zeros(x.shape[1])
    # initialize square of gradients
    S = np.zeros_like(theta)
    # initialize square of updates
    R = np.zeros_like(theta)

    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)

        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # compute gradients
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)
        # update S
        S = rho * S + (1 - rho) * gradients ** 2
        # update gradient
        gradients = np.sqrt(R + epsilon) / np.sqrt(S + epsilon) * gradients
        # compute the update
        R = rho * R + (1 - rho) * gradients ** 2
        # update parameters
        theta -= gradients

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = adadelta(X, y, batch_size=3, rho=0.9, num_iters=1000)
print("Theta:", theta)
```

    Theta: [0.05984473 0.55792767 0.43311901]


### RMSProp(Root Mean Square Propagation)
是一种自适应学习率优化算法，由Geoffrey Hinton提出。它通过使用梯度平方的移动平均值来自动调整每个参数的学习率。

**RMSProp一般流程：**
1. 初始化：初始化参数$\theta_0$，累积梯度平方和$G=0$；
2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta_{t}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta_{t})$，其中 $m$ 是样本数量;
4. 计算累积平方梯度：$G_t = \beta G_{t-1} + (1 - \beta) g(\theta_{t})^2$；
5. 更新参数：$\theta_{t} = \theta_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} g(\theta_{t})$，其中 $\eta$ 是学习率，$\beta$ 是衰减系数，$\epsilon$ 是一个小的常数以防止除零错误;
6. 重复步骤2-5直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\theta)$ 的值小于某个阈值）。


```python
# RMSProp
def rmsprop(x, y, batch_size=None, learning_rate=0.1, beta=0.9, num_iters=1000, epsilon=1e-8):
    # initialize parameters
    theta = np.zeros(x.shape[1])
    # initialize square of gradients
    G = np.zeros_like(theta)

    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)

        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # compute gradients
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)

        # update square of gradients
        G = beta * G + (1 - beta) * np.square(gradients)

        # Update parameters
        theta = theta - learning_rate * gradients / np.sqrt(G + epsilon)

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = rmsprop(X, y, batch_size=3, beta=0.9, learning_rate=0.1, num_iters=1000)
print("Theta:", theta)
```

    Theta: [0.08844242 0.60498504 0.51646766]


### Adam(Adaptive Moment Estimation)

结合了动量和RMSprop的思想，不仅存储了动量项，还维护了一个类似于RMSprop的梯度平方的移动平均值，适合处理带有噪声数据的问题。

**Adam优化算法一般流程：**
1. 初始化：初始化参数$\theta_0$、动量$v=0$和二次矩$s=0$；
2. 机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta_{t}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta_{t})$，其中 $m$ 是样本数量;
4. 更新动量和二次矩：$v_t=\beta_1 v_{t-1}+(1-\beta_1)g(\theta_t)$，$s_t=\beta_2 s_{t-1}+(1-\beta_2)(g(\theta_t))^2$；
5. 修正动量和二次矩：$\hat{v}_t = \frac{v_t}{1-\beta_1^t}$，$\hat{s}_t = \frac{s_t}{1-\beta_2^t}$；
6. 修正梯度：$g'_t=\eta \cdot \frac{\hat{v}_t}{\sqrt{\hat{s}_t}+\epsilon}$，其中 $\eta$ 是学习率，$\beta_1$ 和 $\beta_2$ 是动量和二次矩的衰减系数，$\epsilon$ 是一个小常数以避免除零;
7. 更新参数：$\theta_{t+1} = \theta_t - g'_t$;
8. 重复步骤2-7直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\theta)$ 的值小于某个阈值）。


```python
# Adam
def adam(x, y, batch_size=None, learning_rate=0.1, beta1=0.9, beta2=0.999, num_iters=1000, epsilon=1e-8):

    # initialize parameters
    theta = np.zeros(x.shape[1])
    # initialize v
    v = np.zeros_like(theta)
    # initialize s
    s = np.zeros_like(theta)

    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)

        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # compute gradients
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)

        # update v
        v = beta1 * v + (1 - beta1) * gradients
        # update s
        s = beta2 * s + (1 - beta2) * (gradients ** 2)
        # bias correction
        v_corrected = v / (1 - beta1)
        s_corrected = s / (1 - beta2)

        # update gradients
        gradients = (learning_rate / (np.sqrt(s_corrected) + epsilon)) * v_corrected

        # update theta
        theta -= gradients

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = adam(X, y, batch_size=3, learning_rate=0.9, num_iters=1000)
print("Theta:", theta)
```

    Theta: [0.03976516 0.53976516 0.46023484]


### Nadam（Nesterov-accelerated Adaptive Moment Estimation）
是一种结合了Nesterov加速梯度下降和Adam优化算法的改进版本。Nadam通过在动量项中引入Nesterov加速的思想，进一步提升了优化效果。

**Nadam一般流程：**
1. 初始化：初始化参数$\theta_0$、动量$m=0$和二次矩$v=0$；
2. 机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；
3. 计算梯度：使用选定的样本损失函数 $J(\theta)$ 对 $\theta$ 的梯度：$g(\theta_{t}) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta_{t})$，其中 $m$ 是样本数量;
4. 更新动量和二次矩：$m_t=\beta_1 m_{t-1}+(1-\beta_1)g(\theta_t)$，$v_t=\beta_2 v_{t-1}+(1-\beta_2)(g(\theta_t))^2$；
5. 修正动量和二次矩：$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$，$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$；
6. 计算Nesterov加速动量：$\tilde{m_t} = \beta_1 \hat m_{t-1} + (1-\beta_1)g(\theta_{t})$
7. 更新参数：使用计算得到的Nesterov加速动量来更新参数 $\theta_t = {\theta}_{t-1} - \eta \cdot \frac{\tilde{m_t}}{\sqrt{\hat{v_t}} + \epsilon} \cdot g_t$；
8. 重复步骤2-7直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\theta)$ 的值小于某个阈值）。


```python
# Nadam
def nadam(x, y, batch_size=2, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iters=1000):

    # initialize parameters
    theta = np.zeros(x.shape[1])
    # initialize v
    m = np.zeros_like(theta)
    # initialize s
    v = np.zeros_like(theta)

    for _ in range(num_iters):
        if batch_size is None:
            indices = np.arange(x.shape[0])
        else:
            indices = np.random.choice(x.shape[0], batch_size, replace=False)

        x_shuffled = x[indices]
        y_shuffled = y[indices]
        # compute gradients
        gradients = compute_gradient(theta, x_shuffled, y_shuffled)

        # update m
        m = beta1 * m + (1 - beta1) * gradients
        # update v
        v = beta2 * v + (1 - beta2) * (gradients ** 2)
        # bias correction
        m_corrected = m / (1 - beta1)
        v_corrected = v / (1 - beta2)

        # update m
        m_bar = beta1 * m_corrected + (1 - beta1) * gradients

        # update theta
        theta -= learning_rate * m_bar / (np.sqrt(v_corrected) + epsilon)

    return theta
```


```python
x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
y = np.array([1.5, 3.5, 5.5, 7.5])

# Compute the theta parameters using gradient descent
X = np.hstack((np.ones((x.shape[0], 1)), x))

theta = nadam(X, y, batch_size=3, learning_rate=0.9, num_iters=1000)
print("Theta:", theta)
```

    Theta: [0.03629115 0.53629115 0.46370885]


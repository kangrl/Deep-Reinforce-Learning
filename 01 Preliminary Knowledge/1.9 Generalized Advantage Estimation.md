# 01 Preliminary Knowledge

## 1.9 Generalized Advantage Estimation (GAE)

### Generalized Advantage Estimation (GAE)
Generalized Advantage Estimation (GAE) 是一种在强化学习（Reinforcement Learning, RL）中用于估计优势函数（Advantage Function）的方法。优势函数衡量的是在某个状态下，采取某个动作相对于采取该状态下的平均动作的优势。GAE由John Schulman等人于2016年在论文《High-Dimensional Continuous Control Using Generalized Advantage Estimation》中提出。

### 基本信息
- **目的：** 提供一个更稳定的优势函数估计，以改善强化学习算法的性能；
- **应用：** 常用语策略梯度方法（如PPO、TRPO等）中，用于估计动作的优势值；
- **核心思想：** 结合了蒙特卡洛估计(Monte Carlo estimation)和时序差分学习（Temporal Difference Learning, TD）的优点，引入一个权衡因子$\lambda$来调节蒙特卡洛估计和时序差分学习（当$\lambda$接近1时，GAE更接近于蒙特卡洛方法，优势估计的偏差较小，但方差较高；当$\lambda$接近0时，GAE更接近于时序差分方法，优势估计的方差较小，但偏差较高；）；此外，$\lambda$还控制回报的聚合方式，当$\lambda$接近1时，长期奖励的影响更大，反之亦然。

### 基本概念
- **优势函数：** $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$，其中$Q(s_t, a_t)$是状态动作价值函数，$V(s_t)$是状态值函数；
- **蒙特卡洛方法：** 通过采样整个轨迹来估计优势函数，即$A(s_t, a_t) \approx G_{t+1} - V(s_t)$，其中$G_{t+1}$是从时间步$t+1$开始的回报；
- **时序差分方法：** 通过当前的奖励和下一状态的估计值来估计优势函数，即$A(s_t, a_t) \approx r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$；
- **$\lambda$回报：** $G_t^{(\lambda)} = (1-\lambda)\sum_{k=0}^{\infty}\lambda^{k}G_{t+k+1}$，是$k$步回报的加权平均，权重随步数增加而指数衰减，其中$\lambda$是一个衰减系数；
- **GAE公式：** 
$$A_t = \delta_t + \gamma \lambda \delta_{t+1} + \gamma^2 \lambda^2 \delta_{t+2} + \cdots$$
其中$\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$。

### $\lambda$的影响分析

当$\lambda=0$时，只有一步回报被考虑，GAE退化为：
$$A_t = G_t^{(1)} = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$$
这相当于只考虑单步奖励的TD误差；

当$\lambda=1$时，所有未来回报都被考虑，GAE退化为：
$$
\begin{aligned}
A_t &= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots \\
&= r_{t+1} + \gamma V(s_{t+1}) - V(s_t) + \gamma r_{t+2} + \gamma^2 V(s_{t+2}) - \gamma V(s_{t+1}) + \gamma^2 r_{t+3} + \gamma^3 V(s_{t+3}) - \gamma^2 V(s_{t+2}) + \cdots \\
&= r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots - V(s_t) \\
&= G_{t+1} - V(s_t)
\end{aligned}
$$
这相当于考虑所有未来奖励的累积和；

当$\lambda$在$(0, 1)$之间时，GAE公式考虑了多步时序差分误差的加权累加。权重随时间步的增加而指数衰减，由
$\lambda$和$\gamma$共同决定。这种加权累加既考虑了未来奖励的影响，又限制了远期奖励的权重，从而在偏差和方差之间找到了平衡。

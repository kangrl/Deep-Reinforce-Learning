# 01 Preliminary Knowledge

## 1.5 Entropy

### Entropy
熵通常用于描述系统的不确定性或混乱程度。在信息论中，熵被用来衡量一个随机变量的不确定性。对于离散随机变量 $X$，其熵定义为：
$$H(X) = -\sum_{i=1}^{n} p(x_i) \log_2p(x_i)$$
其中，$p(x_i)$ 表示实践$x_i$发生的概率；$\log_2$ 是以2为底的对数，因为信息通常用二进制表示。

**进一步解释：**
- 概率：$p(x_i)$ 是某个特定事件（比如抽到某个颜色的球）发生的概率；
- 对数：$\log_2$ 是以2为底的对数，表示信息量的大小——要多少位二进制信息才能描述这个事件的概率；
- 负号：因为熵的定义是“不确定性的度量”，而概率越大，不确定性越小，所以用负号来保证熵是非负的；另外，由于概率小于1，对数运算结果为负数，再加个负号就变成正数了；
- 乘以概率：因为我们要计算的是整个系统的平均信息量（即所有可能事件的信息量的期望）；
- 求和：因为我们有多个事件需要考虑，所以要对所有可能的事件进行求和，以掌握整体的不确定水平。

如果一个系统只有一种可能的状态，那么这个系统的熵为0，因为没有任何不确定性；如果有多种状态，每种状态的概率相等，那么熵最大，因为每种状态都具有相同的不确定性。

### Example
假设有一个骰子，有6个面，每个面出现的概率都是1/6。我们可以计算这个系统的熵：
$$H(X) = -\sum_{i=1}^{6} p_i \log_2(p_i)$$
其中，$p_i$ 是第 $i$ 个面的概率，$\log_2$ 是以2为底的对数。
因为每个面出现的概率都是1/6，所以：
$$H(X) = -6 \times \frac{1}{6} \log_2(\frac{1}{6}) = -\log_2(\frac{1}{6})$$
这个值大约等于2.585。

### Entropy Gain
熵增益（Entropy Gain）是信息论和决策树学习中常用的一个概念，用于衡量在给定数据集上，根据某个特征进行划分后，数据集的熵（即混乱度或不确定性）的减少程度。熵增益是决策树算法如ID3、C4.5和CART中用于选择分裂属性的重要指标。

在决策树学习中，熵增益表示在某个属性A上分裂后，数据集D的熵的减少量。熵增益的计算公式为：
$$ \text{Gain}(D, A) = H(D) - \sum_{i=1}^v \frac{|D_i|}{|D|} H(D_v) $$
其中，$H(D)$是数据集D的熵；$v$是属性$A$的取值个数，$|D|$和$|D_i|$分别是数据集D及其子集的样本数量。

熵增益越大，表示根据属性$A$进行分裂后，数据集D的不确定性减少得越多，即信息增益越大，这个属性对于分类就越有价值。因此，选择熵增益最大的属性作为分裂属性。

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Preliminary Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gradient Dscent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降(Gradient Descent)\n",
    "梯度下降是一种广发应用于机器学习和优化问题的数值优化算法，用于最小化或最大化目标函数。它通过迭代地调整参数，使得每次调整后的参数都能使目标函数的值减小（或增大）。\n",
    "\n",
    "**梯度下降算法一般流程：**\n",
    "1. 初始化：初始化模型参数的初值 $\\theta_0$；\n",
    "2. 计算梯度：在当前参数下，计算目标函数对参数的梯度 $\\nabla_{\\theta}J(\\theta)$；\n",
    "3. 更新参数：按照负梯度的方向调整参数，即 $\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta}J(\\theta)$，其中 $\\eta$ 是学习率（步长）；\n",
    "4. 重复步骤2和3，直到达到某个停止条件，如达到最大迭代次数或梯度变化小于某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gradient\n",
    "def compute_gradient(theta, x, y):\n",
    "    \"\"\" Compute the gradient for linear regression. \"\"\"\n",
    "\n",
    "    gradients = 2 / y.shape[0] * (x.T @ (x @ theta - y))\n",
    "    return gradients.ravel()\n",
    "\n",
    "# gradient_descent\n",
    "def gradient_descent(x, y, learning_rate=0.01, num_iters=1000):\n",
    "    \"\"\" Gradient descent algorithm \"\"\"\n",
    "\n",
    "    # Initialize theta parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    for _ in range(num_iters):\n",
    "        gradients = compute_gradient(theta, x, y)\n",
    "        theta -= learning_rate * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [2.60102170e-04 4.99834982e-01 5.00095084e-01]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = gradient_descent(X, y)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量梯度下降（Batch Gradient Descent）\n",
    "是梯度下降的一个变种，每次迭代使用所有训练数据来计算梯度。特点是每次更新参数时都需要遍历整个数据集，因此优化方向准确、稳定性高、易于实现，但收敛速度较慢、计算量大、内存占用大，需要小一点的学习率，但是最终结果较为准确。\n",
    "\n",
    "**批量梯度下降算法一般流程：**\n",
    "1. 初始化：初始化参数向量 $\\theta$;\n",
    "2. 计算梯度：遍历整个训练集，计算损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta^{(t)}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta^{(t)})$;\n",
    "3. 更新参数：使用学习率 $\\eta$ 更新参数向量 $\\theta$，即 $\\theta = \\theta - \\eta \\cdot g(\\theta^{(t)})$;\n",
    "4. 重复步骤2和3，直到达到预定的迭代次数或损失函数 $J(\\theta)$ 收敛到某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch gradient descent\n",
    "def batch_gradient_descent(x, y, learning_rate=0.01, num_iters=1000):\n",
    "    \"\"\" Batch Gradient Descent algorithm \"\"\"\n",
    "\n",
    "    # Initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        gradients = compute_gradient(theta, x, y)\n",
    "\n",
    "        theta -= learning_rate * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [2.60102170e-04 4.99834982e-01 5.00095084e-01]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = batch_gradient_descent(X, y)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降（Stochastic Gradient Descent）\n",
    "随机梯度下降（SGD）是一种优化算法，用于最小化一个函数。它通过迭代地调整模型参数来逐步减少损失函数的值。与批量梯度下降不同，SGD在每次迭代中只使用一个样本或一个小批量的样本来计算梯度。这使得SGD比批量梯度下降更快、能够有效避免陷入局部最优解，特别是在数据集非常大的情况下。但由于其随机性，可能会导致收敛过程更加不稳定，并且对学习率的选择比较敏感，同时由于随机选择样本或批量样本，导致梯度估计存在一定的噪声。\n",
    "\n",
    "**随机梯度下降一般流程：**\n",
    "1. 初始化：初始化参数向量 $\\theta$;\n",
    "2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta^{(t)}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta^{(t)})$，其中 $m$ 是样本数量;\n",
    "4. 更新参数：使用学习率 $\\eta$ 更新参数向量 $\\theta$，即 $\\theta = \\theta - \\eta \\cdot g(\\theta^{(t)})$;\n",
    "5. 重复步骤2-4，直到达到预定的迭代次数或损失函数 $J(\\theta)$ 收敛到某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(x, y, batch_size=2, learning_rate=0.01, num_iters=1000):\n",
    "    \"\"\" Stochastic Gradient Descent algorithm \"\"\"\n",
    "\n",
    "    # Initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    for _ in range(num_iters):\n",
    "        # Shuffle the data\n",
    "        indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # Mini-batch gradient descent\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "        # Update parameters\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [2.52004635e-04 4.99846850e-01 5.00098855e-01]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = stochastic_gradient_descent(X, y, learning_rate=0.01, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动量梯度下降（Momententum Gradient Descent）\n",
    "动量梯度下降是一种优化算法，用于加速训练过程。它通过引入一个动量项来减少震荡并加快收敛速度。\n",
    "\n",
    "**动量梯度下降一般流程：**\n",
    "1. 初始化：设置学习率 $\\eta$ 和动量系数 $\\beta$（通常在0到1之间），以及初始参数 $\\theta$；\n",
    "2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta^{(t)}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta^{(t)})$，其中 $m$ 是样本数量;\n",
    "4. 更新速度：使用动量公式更新速度 $v_{t+1} = \\beta v_{t} - \\eta g(\\theta^{(t)})$，其中 $v_0 = 0$;\n",
    "5. 更新参数：使用学习率 $\\eta$ 更新参数向量 $\\theta$，即 $\\theta_{t+1} = \\theta_{t} + v_t$，其中 $v_0 = 0$;\n",
    "6. 重复步骤2-5，直到达到预定的迭代次数或损失函数 $J(\\theta)$ 收敛到某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momententum Gradient Descent\n",
    "def momentum_gradient_descent(x, y, learning_rate=0.1, batch_size=None, momentum=0.9, num_iters=1000):\n",
    "    \"\"\" Momentum Gradient Descent Algorithm \"\"\"\n",
    "\n",
    "    # Initialize velocity\n",
    "    v = np.zeros(x.shape[1])\n",
    "    # Initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # Compute gradients\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "        # Update velocity\n",
    "        v = momentum * v - learning_rate * gradients\n",
    "        # Update parameters\n",
    "        theta += v  # notice the difference here\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [-4.88015023e-16  5.00000000e-01  5.00000000e-01]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = momentum_gradient_descent(X, y, learning_rate=0.01, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov 加速梯度下降 (Nesterov Accelerated Gradient, NAG)\n",
    "Nesterov 加速梯度下降是一种改进的动量梯度下降算法，它结合了动量和当前梯度的信息。NAG 的核心思想是使用未来的梯度来更新参数，而不是当前的梯度。这使得 NAG 在某些情况下比标准动量方法表现得更好。\n",
    "\n",
    "**Nesterov Accelerated Gradient (NAG) 算法一般流程：**\n",
    "1. 初始化：初始化学习率 $\\eta$、动量系数 $\\beta$ 和参数向量 $\\theta_0$；\n",
    "2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 更新未来位置：使用动量更新参数$\\tilde{\\theta} = \\theta_t + \\beta \\cdot v_t$;\n",
    "4. 计算损失函数关于未来位置的梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\tilde{\\theta}$ 的梯度：$g(\\tilde{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\tilde{\\theta})$，其中 $m$ 是样本数量;\n",
    "5. 更新速度：$v_{t+1} = \\beta \\cdot v_t + \\eta \\cdot g(\\tilde{\\theta})$;\n",
    "6. 更新参数：$\\theta_{t+1} = \\theta_{t} - v_{t+1}$;\n",
    "7. 重复步骤2-6，直到达到预定的迭代次数或损失函数 $J(\\theta)$ 收敛到某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesterov Accelerated Gradient (NAG)\n",
    "def nesterov_accelerated_gradient(x, y, batch_size=2, learning_rate=0.1, momentum=0.9, num_iters=1000):\n",
    "    \"\"\" Nesterov Accelerated Gradient (NAG) 优化算法实现 \"\"\"\n",
    "\n",
    "    # initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    # initialize velocity\n",
    "    v = np.zeros_like(theta)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # update theta\n",
    "        theta_lookahead = theta + momentum * v\n",
    "        # Compute gradients\n",
    "        gradients = compute_gradient(theta_lookahead, x_shuffled, y_shuffled)\n",
    "        # Update velocity\n",
    "        v = momentum * v - learning_rate * gradients\n",
    "        # Update parameters\n",
    "        theta += v  # notice the difference here\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [6.20745421e-16 5.00000000e-01 5.00000000e-01]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = nesterov_accelerated_gradient(X, y, learning_rate=0.01, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自适应梯度下降（AdaGrad）\n",
    "AdaGrad是一种自适应学习率的优化算法，它根据参数的历史梯度平方和来调整每个参数的学习率。这样可以确保在不同维度上使用不同的学习率，从而加速收敛并避免震荡。\n",
    "\n",
    "**自适应梯度下降一般流程：**\n",
    "1. 初始化：初始化学习率 $\\eta$、累积梯度向量和$\\mathbf{G}_0$和参数向量 $\\theta_0$；\n",
    "2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta_{t}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta_{t})$，其中 $m$ 是样本数量;\n",
    "4. 更新学习率：计算新的学习率 $ \\eta_t = \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} $，其中 $\\mathbf{G}_t$ 是累积梯度向量, $\\epsilon$ 是一个小的常数以避免除零错误；\n",
    "5. 更新参数：$\\theta_{t+1} = \\theta_t - \\eta_t \\cdot g(\\theta_{t})$；\n",
    "6. 更新累积梯度：$\\mathbf{G}_{t+1} = \\mathbf{G}_t + g(\\theta_{t})^2$；\n",
    "7. 重复步骤2-6，直到达到预定的迭代次数或损失函数 $J(\\theta)$ 收敛到某个阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad\n",
    "def adagrad(x, y, batch_size=2, learning_rate=0.1, num_iters=1000, epsion=1e-6):\n",
    "    \"\"\" AdaGrad \"\"\"\n",
    "    # initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    # initialize gradients\n",
    "    G = 0.0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # compute gradients\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "        # update G\n",
    "        G += gradients ** 2\n",
    "        # update learning rate\n",
    "        lr = learning_rate / np.sqrt(G + epsion)\n",
    "        # update parameters\n",
    "        theta -= lr * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.04197379 0.54197379 0.45802621]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = adagrad(X, y, batch_size=1, learning_rate=0.5, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adadelta\n",
    "Adadelta是一种自适应学习率优化算法，它是对Adagrad算法的扩展，旨在减少其学习率单调递减的问题。在Adagrad算法中，学习率会随着时间逐渐减小，这可能导致学习过程过早停止。Adadelta通过限制累积历史梯度的大小来解决这个问题。**Adadelta算法的主要思想** 是使用一个窗口来累积过去梯度的平方，而不是从开始到现在的所有梯度。这样，学习率就不会随着时间无限减小。具体来说，Adadelta使用了一个参数ρ（通常设置为一个接近1的值，如0.9）来控制窗口的大小。\n",
    "\n",
    "**Adadelta一般流程：**\n",
    "1. 初始化：初始化参数$\\theta_0$，累积平方梯度的指数加权平均$s=0$，累积更新量的指数加权平均$r=0$；\n",
    "2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta_{t}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta_{t})$，其中 $m$ 是样本数量;\n",
    "4. 更新累积平方梯度的指数加权平均：$s_{t}= \\rho s_{t-1} + (1 - \\rho)(g(\\theta_{t}))^2$\n",
    "5. 修正梯度：$g'_{t} = \\frac{\\sqrt{r_{t-1} + \\epsilon}}{\\sqrt{s_{t-1}} + \\epsilon} g(\\theta_{t-1})$；\n",
    "6. 更新累积更新量的指数加权平均：$r_{x_t} = \\rho r_{x_{t-1}} + (1 - \\rho) {g'_{t}}^2$；\n",
    "7. 更新模型参数：$\\theta_{t} = \\theta_{t} - g'_{t}$；\n",
    "8. 重复步骤2-7直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\\theta)$ 的值小于某个阈值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adadelta\n",
    "def adadelta(x, y, batch_size=None, rho=0.9, num_iters=1000, epsilon=1e-6):\n",
    "    \"\"\" Adadelta \"\"\"\n",
    "\n",
    "    # initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    # initialize square of gradients\n",
    "    S = np.zeros_like(theta)\n",
    "    # initialize square of updates\n",
    "    R = np.zeros_like(theta)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # compute gradients\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "        # update S\n",
    "        S = rho * S + (1 - rho) * gradients ** 2\n",
    "        # update gradient\n",
    "        gradients = np.sqrt(R + epsilon) / np.sqrt(S + epsilon) * gradients\n",
    "        # compute the update\n",
    "        R = rho * R + (1 - rho) * gradients ** 2\n",
    "        # update parameters\n",
    "        theta -= gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.05984473 0.55792767 0.43311901]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = adadelta(X, y, batch_size=3, rho=0.9, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp(Root Mean Square Propagation)\n",
    "是一种自适应学习率优化算法，由Geoffrey Hinton提出。它通过使用梯度平方的移动平均值来自动调整每个参数的学习率。\n",
    "\n",
    "**RMSProp一般流程：**\n",
    "1. 初始化：初始化参数$\\theta_0$，累积梯度平方和$G=0$；\n",
    "2. 随机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta_{t}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta_{t})$，其中 $m$ 是样本数量;\n",
    "4. 计算累积平方梯度：$G_t = \\beta G_{t-1} + (1 - \\beta) g(\\theta_{t})^2$；\n",
    "5. 更新参数：$\\theta_{t} = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g(\\theta_{t})$，其中 $\\eta$ 是学习率，$\\beta$ 是衰减系数，$\\epsilon$ 是一个小的常数以防止除零错误;\n",
    "6. 重复步骤2-5直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\\theta)$ 的值小于某个阈值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "def rmsprop(x, y, batch_size=None, learning_rate=0.1, beta=0.9, num_iters=1000, epsilon=1e-8):\n",
    "    # initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    # initialize square of gradients\n",
    "    G = np.zeros_like(theta)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # compute gradients\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "\n",
    "        # update square of gradients\n",
    "        G = beta * G + (1 - beta) * np.square(gradients)\n",
    "\n",
    "        # Update parameters\n",
    "        theta = theta - learning_rate * gradients / np.sqrt(G + epsilon)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.08844242 0.60498504 0.51646766]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = rmsprop(X, y, batch_size=3, beta=0.9, learning_rate=0.1, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam(Adaptive Moment Estimation)\n",
    "\n",
    "结合了动量和RMSprop的思想，不仅存储了动量项，还维护了一个类似于RMSprop的梯度平方的移动平均值，适合处理带有噪声数据的问题。\n",
    "\n",
    "**Adam优化算法一般流程：**\n",
    "1. 初始化：初始化参数$\\theta_0$、动量$v=0$和二次矩$s=0$；\n",
    "2. 机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta_{t}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta_{t})$，其中 $m$ 是样本数量;\n",
    "4. 更新动量和二次矩：$v_t=\\beta_1 v_{t-1}+(1-\\beta_1)g(\\theta_t)$，$s_t=\\beta_2 s_{t-1}+(1-\\beta_2)(g(\\theta_t))^2$；\n",
    "5. 修正动量和二次矩：$\\hat{v}_t = \\frac{v_t}{1-\\beta_1^t}$，$\\hat{s}_t = \\frac{s_t}{1-\\beta_2^t}$；\n",
    "6. 修正梯度：$g'_t=\\eta \\cdot \\frac{\\hat{v}_t}{\\sqrt{\\hat{s}_t}+\\epsilon}$，其中 $\\eta$ 是学习率，$\\beta_1$ 和 $\\beta_2$ 是动量和二次矩的衰减系数，$\\epsilon$ 是一个小常数以避免除零;\n",
    "7. 更新参数：$\\theta_{t+1} = \\theta_t - g'_t$;\n",
    "8. 重复步骤2-7直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\\theta)$ 的值小于某个阈值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "def adam(x, y, batch_size=None, learning_rate=0.1, beta1=0.9, beta2=0.999, num_iters=1000, epsilon=1e-8):\n",
    "\n",
    "    # initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    # initialize v\n",
    "    v = np.zeros_like(theta)\n",
    "    # initialize s\n",
    "    s = np.zeros_like(theta)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # compute gradients\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "\n",
    "        # update v\n",
    "        v = beta1 * v + (1 - beta1) * gradients\n",
    "        # update s\n",
    "        s = beta2 * s + (1 - beta2) * (gradients ** 2)\n",
    "        # bias correction\n",
    "        v_corrected = v / (1 - beta1)\n",
    "        s_corrected = s / (1 - beta2)\n",
    "\n",
    "        # update gradients\n",
    "        gradients = (learning_rate / (np.sqrt(s_corrected) + epsilon)) * v_corrected\n",
    "\n",
    "        # update theta\n",
    "        theta -= gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.03976516 0.53976516 0.46023484]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = adam(X, y, batch_size=3, learning_rate=0.9, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam（Nesterov-accelerated Adaptive Moment Estimation）\n",
    "是一种结合了Nesterov加速梯度下降和Adam优化算法的改进版本。Nadam通过在动量项中引入Nesterov加速的思想，进一步提升了优化效果。\n",
    "\n",
    "**Nadam一般流程：**\n",
    "1. 初始化：初始化参数$\\theta_0$、动量$m=0$和二次矩$v=0$；\n",
    "2. 机选择样本或小批量样本：从训练集中随机选择一个样本或一个小批量的样本；\n",
    "3. 计算梯度：使用选定的样本损失函数 $J(\\theta)$ 对 $\\theta$ 的梯度：$g(\\theta_{t}) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta_{t})$，其中 $m$ 是样本数量;\n",
    "4. 更新动量和二次矩：$m_t=\\beta_1 m_{t-1}+(1-\\beta_1)g(\\theta_t)$，$v_t=\\beta_2 v_{t-1}+(1-\\beta_2)(g(\\theta_t))^2$；\n",
    "5. 修正动量和二次矩：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$，$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$；\n",
    "6. 计算Nesterov加速动量：$\\tilde{m_t} = \\beta_1 \\hat m_{t-1} + (1-\\beta_1)g(\\theta_{t})$\n",
    "7. 更新参数：使用计算得到的Nesterov加速动量来更新参数 $\\theta_t = {\\theta}_{t-1} - \\eta \\cdot \\frac{\\tilde{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} \\cdot g_t$；\n",
    "8. 重复步骤2-7直到达到预定的停止条件（例如，达到最大迭代次数或损失函数 $J(\\theta)$ 的值小于某个阈值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadam\n",
    "def nadam(x, y, batch_size=2, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iters=1000):\n",
    "\n",
    "    # initialize parameters\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    # initialize v\n",
    "    m = np.zeros_like(theta)\n",
    "    # initialize s\n",
    "    v = np.zeros_like(theta)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        if batch_size is None:\n",
    "            indices = np.arange(x.shape[0])\n",
    "        else:\n",
    "            indices = np.random.choice(x.shape[0], batch_size, replace=False)\n",
    "\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        # compute gradients\n",
    "        gradients = compute_gradient(theta, x_shuffled, y_shuffled)\n",
    "\n",
    "        # update m\n",
    "        m = beta1 * m + (1 - beta1) * gradients\n",
    "        # update v\n",
    "        v = beta2 * v + (1 - beta2) * (gradients ** 2)\n",
    "        # bias correction\n",
    "        m_corrected = m / (1 - beta1)\n",
    "        v_corrected = v / (1 - beta2)\n",
    "\n",
    "        # update m\n",
    "        m_bar = beta1 * m_corrected + (1 - beta1) * gradients\n",
    "\n",
    "        # update theta\n",
    "        theta -= learning_rate * m_bar / (np.sqrt(v_corrected) + epsilon)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.03629115 0.53629115 0.46370885]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "\n",
    "# Compute the theta parameters using gradient descent\n",
    "X = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "theta = nadam(X, y, batch_size=3, learning_rate=0.9, num_iters=1000)\n",
    "print(\"Theta:\", theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

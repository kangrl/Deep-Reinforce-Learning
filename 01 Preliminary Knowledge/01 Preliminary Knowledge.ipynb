{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Preliminary Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机过程(Stochastic Process)\n",
    "随机过程是指一系列随机变量的集合，这些随机变量的取值随时间而变化，并且这些随机变量的概率分布随时间而变化。令随机变量在时间 $t$ 时刻的取值为 $S_t$，则称 $S_t$ 为 $t$ 时刻的随机变量，所有随机变量的集合称为随机过程，记为 $\\cal S$;在随机过程的某个时刻 $t$，随机变量/随机状态 $S_t$ 通常取决于$t$时刻之前的状态，即$P(S_{t+1}|S_1, S_2, \\cdots, S_t)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫性质(Markov Property)\n",
    "当且仅当一个随机过程某时刻$t$的状态 $S_t$ 只依赖于 $t-1$ 时刻之前的状态时，这个随机过程被称为具有马尔可夫性质，用公式表示为：\n",
    "$$\n",
    "    P(S_{t+1}|S_t) = P(S_{t+1}|S_1, S_2, \\cdots, S_t)\n",
    "$$\n",
    "\n",
    "**值得注意的是：一个随机过程具有马尔可夫性质并不代表这个随机过程就和历史完全没有关系。** 因为虽然$t+1$时刻的状态只依赖于$t$时刻之前的状态，但$t$时刻的状态可能依赖于$t-1$时刻之前的状态，以此类推，历史信息通过这种链式的关系被传递到当前时刻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫过程(Markov Process)\n",
    "具有马尔可夫性质的随机过程被称为马尔可夫过程，也被称为马尔可夫链（Markov Chain）。令$\\cal S$表示有限数量的状态集合，令$\\cal P$表示状态转移矩阵，则马尔可夫过程可以用$(\\cal S, \\cal P)$来描述。其中：\n",
    "  $$\n",
    "      \\cal S = \\{s_1, s_2, \\cdots, s_n\\}\n",
    "  $$\n",
    "  $$\n",
    "      \\cal P = \\begin{bmatrix}\n",
    "          P_{11} & P_{12} & \\cdots & P_{1n} \\\\\n",
    "          P_{21} & P_{22} & \\cdots & P_{2n} \\\\\n",
    "          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "          P_{n1} & P_{n2} & \\cdots & P_{nn}\n",
    "      \\end{bmatrix}\n",
    "  $$\n",
    "  其中，$P_{ij}$表示从状态$s_i$转移到状态$s_j$的概率，即$P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$。\n",
    "  另外，从某个状态出发，到达其他状态的概率和必须为1，即：\n",
    "  $\n",
    "      \\sum_{j=1}^n P_{ij} = 1\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个马尔可夫过程的例子\n",
    "<center>\n",
    "<img src=\"../Images/markov-process.png\" alt=\"Markov Process\">\n",
    "</center>\n",
    "\n",
    "状态集合/空间：\n",
    "$$\n",
    "\\cal S = (S_1, S_2, S_3, S_4, S_5, S_6)\n",
    "$$\n",
    "状态转移矩阵：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.9 & 0.1 & 0 & 0 & 0 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0.6 & 0 & 0.4 \\\\\n",
    "0 & 0 & 0 & 0 & 0.3 & 0.7 \\\\\n",
    "0 & 0.2 & 0.3 & 0.5 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**终止状态：** 不再转移到其他状态，即没有更多的动作可以选择(如示例中的状态$S_6$)。  \n",
    "**状态采样：** 从某个状态出发，根据状态转移矩阵生成一个状态序列$\\text{（Episode）}$的过程被称作状态采样$\\text{(Sampling)}$。如状态采样（从$s_1$出发）：可以生成$s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow s_6$或$s_1 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow s_4 \\rightarrow s_6$等状态序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫奖励过程(Markov Reward Process)\n",
    "在马尔可夫过程基础上加入奖励函数$r$和折旧因子$\\gamma$，即马尔可夫奖励过程（Markov Reward Process，MRP），用$(\\cal S, \\cal P, r, \\gamma)$来表示，其中$\\cal S$为状态空间，$\\cal P$为状态转移概率矩阵，$r$为转移某个状态$s$时可以获得奖励的期望——记为$r(s)$——与状态有关，$\\gamma$为折旧因子——取值范围$[0,1)$, 用于衡量未来收益的衰减程度。\n",
    "\n",
    "**回报：** 从$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减值和称为回报，记为$G_t$，即：\n",
    "\n",
    "$$\n",
    "G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots =  \\sum_{k=0}^{\\infty}\\gamma^k r(S_{t+k+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个马尔可夫奖励过程的例子\n",
    "<center>\n",
    "<img src=\"../Images/markov-reward-process.png\" alt=\"Markov Reward Process\">\n",
    "</center>\n",
    "\n",
    "设$\\gamma = 0.5$，状态采样得到一条状态序列为$s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow s_6$，就可以计算$s_1$的回报$G_1$，得到：\n",
    "\n",
    "$$\n",
    "G_1 = r_1 + \\gamma r_2 + \\gamma^2 r_3 + \\gamma^3 r_6 = -1 + 0.5 * (-2) + 0.5^2 * (-2) + 0.5^3 * 0 = -2.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程(Markov Decision Process)\n",
    "在马尔可夫奖励过程基础上引入智能体的动作，使得智能体能够在有限的时间内做出最优决策，通常使用$\\langle \\cal S,\\cal A,\\cal P,r,\\gamma\\rangle$来表示马尔可夫决策过程。\n",
    "\n",
    "- $\\cal S$是状态空间\n",
    "- $\\cal A$是动作空间\n",
    "- $\\cal P(s'|s,a)$是状态转移函数，表示在状态$s$执行动作$a$后到达状态$s'$的概率\n",
    "- $r(s,a)$是奖励函数，奖励取同时取决于状态$s$和动作$a$\n",
    "- $\\gamma$是折旧因子\n",
    "\n",
    "**简言之，马尔可夫决策过程是一种引入智能体动作的马尔可夫奖励过程（在奖励函数和转移函数上加入了智能体的动作），马尔可夫奖励过程是一种引入奖励函数和折旧因子的马尔可夫过程，马尔可夫过程是一种具备马尔可夫性质的随机过程。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个马尔可夫决策过程的例子\n",
    "<center>\n",
    "<img src=\"../Images/markov-decision-process.png\" alt=\"Markov Decision Process\">\n",
    "</center>\n",
    "\n",
    "在这个例子中有五个状态，分别为$S_1, S_2, S_3, S_4, S_5$；例子中有七个动作，分别为保持$S_1$、前往$S_1$、前往$S_2$、前往$S_3$、前往$S_4$、前往$S_5$、概率前往。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程转化为马尔可夫奖励过程\n",
    "\n",
    "使用**边缘化**的方法将MDP转化为MRP。对于某一个状态，对所有动作的概率进行加权求和，得到一个关于奖励的边缘分布，就可以认为是一个MRP在该状态下的奖励，即：\n",
    "\n",
    "$$r'(s) = \\sum_{a \\in A} \\pi(a|s) r(s,a)$$\n",
    "同理，计算采取动作的概率使状态$s$转移到$s'$的概率的乘积后求和，得到一个关于转移的边缘分布，就可以认为是一个MRP在状态$s$到$s'$的转移分布，即：\n",
    "\n",
    "$$P'(s'|s) = \\sum_{a \\in A} \\pi(a|s) P(s'|s,a)$$\n",
    "\n",
    "这样，MDP就可以转化为MRP: $\\langle \\cal S,\\cal P',r',\\gamma\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状态空间\n",
    "S = ['s1', 's2', 's3', 's4', 's5']\n",
    "\n",
    "# 动作空间\n",
    "A = ['保持s1', '前往s1', '前往s2','前往s3', '前往s4','前往s5', '概率前往']\n",
    "\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    's1->保持s1-s1': 1.,\n",
    "    's1->前往s2-s2': 1.,\n",
    "    's2->前往s1-s1': 1.,\n",
    "    's2->前往s3-s3': 1.,\n",
    "    's3->前往s4-s4': 1.,\n",
    "    's3->前往s5-s5': 1.,\n",
    "    's4->保持s5-s5': 1.,\n",
    "    's4->概率前往-s2': 0.2,\n",
    "    's4->概率前往-s3': 0.4,\n",
    "    's4->概率前往-s4': 0.4\n",
    "}\n",
    "\n",
    "# 奖励函数\n",
    "R = {\n",
    "    's1->保持s1': -1,\n",
    "    's1->前往s2': 0,\n",
    "    's2->前往s1': -1,\n",
    "    's2->前往s3': -2,\n",
    "    's3->前往s4': -2,\n",
    "    's3->前往s5': 0,\n",
    "    's4->保持s5': 10,\n",
    "    's4->概率前往': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义马尔可夫决策过程\n",
    "gamma = 0.5\n",
    "\n",
    "MDP = (S, A, P, R, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 策略\n",
    "Pi = {\n",
    "    's1->保持s1': 0.5,\n",
    "    's1->前往s2': 0.5,\n",
    "    's2->前往s1': 0.5,\n",
    "    's2->前往s3': 0.5,\n",
    "    's3->前往s4': 0.5,\n",
    "    's3->前往s5': 0.5,\n",
    "    's4->保持s5': 0.5,\n",
    "    's4->概率前往': 0.5\n",
    "}\n",
    "\n",
    "# 转换后的状态转移矩阵: Pi & P -> P'\n",
    "mdp2mrp_p = np.array([\n",
    "    [0.5, 0.5, 0, 0, 0],\n",
    "    [0.5, 0, 0.5, 0, 0],\n",
    "    [0, 0, 0, 0.5, 0.5],\n",
    "    [0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0, 0, 0, 0, 1.0]\n",
    "])\n",
    "\n",
    "# 转换后的奖励矩阵: Pi & R -> R'\n",
    "mdp2mrp_r = [-0.5, -1.5, -1.0, 5.5, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Monte Carlo Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 蒙特卡洛方法\n",
    "一种基于随机采样的方法，它的核心思想是通过大量随机实验来近似求解问题的解，尤其适用于那些难以通过解析方法或确定性算法解决的问题（原理：大数定律），也被称为统计模拟方法，是一种基于概率统计的数值计算方法。\n",
    "\n",
    "**核心特点：**\n",
    "- 随机性：依赖于随机采样，通过生成大量随机样本进行模拟；\n",
    "- 统计性：利用统计学原理对结果进行分析，估计目标值及其误差；\n",
    "- 近似性：通过大量实验逼近真实值，结果的精度岁样本数量的增加而提高；\n",
    "\n",
    "**基本步骤：**\n",
    "- 1.定义问题：将问题转化为概率模型，明确需要计算的量（如期望值、积分、概率等）\n",
    "- 2.生成随机样本：根据问题的概率分布，生成大量随机样本；\n",
    "- 3.计算样本值：对每个样本进行计算，得到目标量的估计值；\n",
    "- 4.统计分析：对结果进行统计分析，计算估计值及其误差（如方差、置信区间等）。\n",
    "\n",
    "**优点：**\n",
    "- 适应性广：可以处理高维、非线性、非解析的问题；适用于复杂的概率分布和随机过程；\n",
    "- 易于实现：算法简单直观、编程实现相对容易；\n",
    "- 并行性强：随机试验相互独立，适合并行计算，提高效率；\n",
    "\n",
    "**缺点：**\n",
    "- 误差通常以$1/\\sqrt{N}$的速度减小；要达到高精度，需要大量的样本；\n",
    "- 结果具有随机性：由于以来随机采样，结果可能存在波动，需要多次试验以确保稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个蒙特卡罗方法的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过蒙特卡洛方法估计圆周率$\\pi$：\n",
    "\n",
    "- 1.定义问题：将圆周率估计问题转化成估计单位圆面积与单位正方形面积比例；\n",
    "- 2.生成随机样本：在单位正方形$[0,1] \\times [0,1]$内随机撒点；\n",
    "- 3.统计分析：统计落在单位元内的点的比例；\n",
    "- 4.问题求解：圆的面积公式，估计$\\pi$值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi(N):\n",
    "    \"\"\" Estimate Pi by Monte Carlo: pi / 4 = M / N \"\"\"\n",
    "\n",
    "    M = 0\n",
    "    for _ in range(N):\n",
    "        x = random.uniform(0, 1)\n",
    "        y = random.uniform(0, 1)\n",
    "\n",
    "        if x ** 2 + y ** 2 <= 1:\n",
    "            M += 1\n",
    "\n",
    "    return 4 * M / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = estimate_pi(100000)\n",
    "pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

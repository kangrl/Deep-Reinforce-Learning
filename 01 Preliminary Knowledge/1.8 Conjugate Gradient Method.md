# 01 Preliminary Knowledge

## 1.8 Conjugate Gradient Method

### Conjugate Gradient Method
**共轭梯度法（Conjugate Gradient Method）** 是一种用于求解线性方程组$Ax=b$的迭代算法，其中$A$是一个对称正定矩阵。这种方法特别适用于大规模稀疏矩阵问题，常见于数值分析和科学计算领域。共轭梯度法由Magnus Hestenes和Eduard Stiefel于1952年提出。

#### 基本思想
共轭梯度法利用了共轭方向的概念，即在每次迭代中，搜索方向不仅与之前的搜索方向共轭（即$A$-正交），而且还是当前残差向量（即
$b−Ax$）的方向。这样可以在迭代过程中逐步逼近解，同时避免像最速下降法那样的锯齿形路径。

#### 算法步骤
1. **初始化**：选择一个初始点 $x_0$ 和初始搜索方向 $p_0 = r_0 = b - Ax_0$，其中 $A$ 是系数矩阵，$p$ 是常数向量;
2. **迭代更新**：对于第 $k$ 次迭代：
   - 计算步长 $\alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}$;
   - 更新当前点 $x_{k+1} = x_k + \alpha_k p_k$;
   - 计算新的残差 $r_{k+1} = r_k - \alpha_k A p_k$;
   - 当残差 $r_{k+1}$ 足够小（通常通过某种误差准则判断）时，停止迭代;
   - 计算参数$\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$；
   - 更新搜索方向 $p_{k+1} = r_{k+1} + \beta_k p_k$；
3. **终止条件**：当残差 $r_{k+1}$ 足够小（通常通过某种误差准则判断）时，停止迭代。

### Example

我们以一个简单的二次函数为例来演示共轭梯度法的使用。假设我们要最小化以下二次函数：
$$f(x) = \frac{1}{2} x^T A x - b^T x$$
其中，$A = \begin{pmatrix} 4 & 2 \\ 2 & 3 \end{pmatrix}$，$b = (1, 1)^T$。
我们使用Python实现共轭梯度法来求解这个二次函数的最小值。


```python
import numpy as np
```


```python
def function(A, b, x):
    """_summary_

    Args:
        A (_type_): _description_
        b (_type_): _description_
        x (_type_): _description_
    """

    return 1 / 2 * x.T @ A @ x - b.T @ x

def conjugate_gradient(A, b, x0, tol=1e-6, max_iter=1000):
    """
    Conjugate Gradient Method to minimize f(x) = 0.5 * x^T A x - b^T x
    where A is a symmetric positive definite matrix.

    Parameters:
    A : 2D numpy array
        Symmetric positive definite matrix.
    b : 1D numpy array
        Vector.
    x0 : 1D numpy array
        Initial guess.
    tol : float
        Tolerance for convergence.
    max_iter : int
        Maximum number of iterations.

    Returns:
    x : 1D numpy array
        The solution.
    """
    x = x0
    r = b - A @ x
    p = r
    rsold = r.T @ r

    for i in range(max_iter):
        Ap = A @ p
        alpha = rsold / (p.T @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rsnew = r.T @ r
        if np.sqrt(rsnew) < tol:
            break
        p = r + (rsnew / rsold) * p
        rsold = rsnew

    return x, function(A, b, x), i
```


```python
# Define the matrix A and vector b
A = np.array([[4, 2], [2, 3]])
b = np.array([1, 1])

# Initial guess
x0 = np.zeros_like(b)

# Run the conjugate gradient method
solution = conjugate_gradient(A, b, x0)
print("Minimum point:", solution[0])
print("Function value at minimum point:", solution[1])
print("Number of iterations:", solution[2])
```

    Minimum point: [0.125 0.25 ]
    Function value at minimum point: -0.1875
    Number of iterations: 1


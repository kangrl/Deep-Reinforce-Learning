{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "在3.11-3.13章节中，我们学习了Deep Q-Network算法及改进算法。\n",
    "\n",
    "首先，我们介绍了Deep Q-Network算法的基本原理和实现方法，Deep Q-Network定义了两个用于近似Q值函数的神经网络：主网络$Q(s,a,w)$和目标网络$Q(s,a,w')$，其中目标网络用于近似表示最优动作价值，而主网络被用来更新动作价值，在训练过程中，主网络的参数会被缓慢更新以接近目标网络的参数：\n",
    "$$r + \\gamma \\max_{a \\in \\cal A(s)} Q_{w'} \\left(s', a\\right)$$\n",
    "\n",
    "但由于神经网络输出会产生或正或负的偏差，而Deep Q-Network在每次更新时都选择最大的动作价值作为TD Target，这会导致估计的Q值过高，从而导致过估计问题。为了解决这个问题，我们引入了Double Deep Q-Network算法，它通过使用两个独立的网络来选择动作和评估动作价值：\n",
    "$$r + \\gamma Q_{w'} \\left(s', \\arg\\max_{a \\in \\cal A(s)} Q_w(s', a)\\right)$$\n",
    "\n",
    "紧接着，我们定义了动作优势$A(s,a) = Q(s,a)-V(s)$，表示动作价值与状态价值的差值，从而动作价值可以分解为状态价值和动作优势：\n",
    "\n",
    "$$Q(s,a) = V(s) + A(s,a)$$\n",
    "\n",
    "进一步的可以构造新的Dueling网络来分别计算$V(s)$和$A(s,a)$从而估计动作价值$Q(s, a)$：\n",
    "\n",
    "$$\n",
    "Q_{\\eta, \\alpha, \\beta}(s,a)=V_{\\eta, \\alpha}(s)+A_{\\eta, \\beta}(s,a)-\\frac{1}{|A|}\\sum_{a'}A_{\\eta, \\beta}(s,a')\n",
    "$$\n",
    "或\n",
    "$$\n",
    "Q_{\\eta, \\alpha, \\beta}(s,a)=V_{\\eta, \\alpha}(s)+A_{\\eta, \\beta}(s,a)-\\max_{a'}A_{\\eta, \\beta}(s,a')\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$和$\\beta$分别表示状态值网络和动作优势网络（表示采取不同动作的差异）的参数；$\\eta$表示状态价值网络和动作优势网络共享的网络参数，一般用来提取特征的前几层；\n",
    "\n",
    "减去的动作优势最大值或均值是强制最优动作的优势函数的实际输出为0，这样做的目的在于保证网络的稳定性（即使得网络在某些状态下所有动作的价值都是一个常数，也能保证网络的输出训练稳定——因为在$Q(s,a)=V(s)+A(s,a)$中即使动作价值$Q$值相同，$V$和$A$也可能不同，因此存在多解，从而导致网络训练不稳定）。\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

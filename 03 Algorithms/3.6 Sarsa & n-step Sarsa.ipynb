{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 03 Algorithms",
   "id": "18a44b6c5d69b2b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.6 Sarsa & n-step Sarsa",
   "id": "3e7a409e17153f94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sarsa\n",
    "既然可以使用Temporal-Difference算法来估计状态价值，那么也可以直接使用其来估计动作价值。这一点非常重要，因为可以将估计的动作价值与策略提升相结合，来学习最优策略。\n",
    "\n",
    "给定策略$\\pi$，我们的目标是：对于所有的$s \\in \\mathcal{S}$和动作$a \\in \\mathcal{A}(s)$，估计状态价值$q_{\\pi}(s, a)$。\n",
    "\n",
    "假设我们有遵循策略$\\pi$生成的样本集$ (s_0, a_0, r_1, s_1, a_1, r_2, s_2, ... ,s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, ...) $，也可以表示为$\\{(s_i, a_i, r_{i+1}, s_{i+1})\\}^{T}_{i=0}$。\n",
    "\n",
    "Sarsa算法利用这些样本来估计动作价值:\n",
    "$$\n",
    "\\begin{cases}\n",
    "q_{t+1}(s_t, a_t) = q_t(s_t) - \\alpha_t(s_t, a_t)[q_t(s_t,a_t) - (r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1}))] & (s, t)=(s_t, a_t) \\\\\n",
    "q_{t+1}(s, a) = q_t(s, a) & (s, a) \\neq (s_t, a_t)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，$t=0,1,2,...$，$\\alpha_t(s_t, a_t)$是一个很小的正数，代表学习率。\n",
    "\n",
    "Sarsa的推导过程与Temporal-Difference类似，感兴趣的话可以回顾上一章节。"
   ],
   "id": "6dd8e472ff4f3b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sarsa Algorithms\n",
    "- 初始化：对于所有的状态-动作对$(s,a)$和时刻$t$初始化$\\alpha_t(s,a)=\\alpha>0$、初始化$q_0(s,a)$、初始化贪婪策略$\\pi_0$，$\\epsilon \\in (0,1)$\n",
    "- 对于每一个episode:\n",
    "- $\\qquad$ 遵循策略$\\pi_0(s_0)$在$t_0$时刻生成动作$a_0$\n",
    "- $\\qquad$ 如果$s_t(t=0,1,2,\\cdots)$不是目标状态，则：\n",
    "- $\\qquad\\qquad$ 给定$(s_t,a_t)$采样得到$(r_{t+1},s_{t+1}, a_{t+1})$，其中$a_{t+1}$遵循$\\pi_t(s_{t+1})$\n",
    "- $\\qquad\\qquad$ 更新动作价值：$q_{t+1}(s_t, a_t) \\leftarrow q_{t}(s_t, a_t)-\\alpha_{t}(s_t, a_t)[q_{t}(s_t,a_t) - (r_{t+1}+\\gamma q_{t}(s_{t+1},a_{t+1}))]$\n",
    "- $\\qquad\\qquad$ 更新策略：如果$a = \\arg\\max_a q_{t+1}(s_t, a)$， $\\pi_{t+1}(a|s_t)=1-\\frac{\\epsilon}{|\\cal A(s_t)|}(|\\cal A(s_t)| - 1)$，否则$\\pi_{t+1}(a|s_t) = \\frac{\\epsilon}{|\\cal A(s_t)|}$\n",
    "- $\\qquad$ $s_t \\leftarrow s_{t+1}$, $a_t \\leftarrow a_{t+1}$"
   ],
   "id": "109d21a2ce25cea3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example",
   "id": "c44c551af3d07f29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:53:37.138127Z",
     "start_time": "2025-02-16T09:53:37.038261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ],
   "id": "f5057ac35421ed02",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:53:37.178282Z",
     "start_time": "2025-02-16T09:53:37.161584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsa:\n",
    "    \"\"\" Sarsa Algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.99):\n",
    "\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.returns = []\n",
    "        self.q_tables = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\" Take an epsilon-greedy action based on the Q-table \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(range(self.env.action_space.n), p=self.policy[state])\n",
    "        else:\n",
    "            return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" Return the best action based on the Q-table \"\"\"\n",
    "        return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def update_policy_and_values(self, state, action, reward, next_state, next_action):\n",
    "        td_error = self.q_tables[state][action] - (reward + self.gamma * self.q_tables[next_state][next_action])\n",
    "        self.q_tables[state][action] -= self.alpha * td_error\n",
    "\n",
    "        best_action = self.best_action(state)\n",
    "        policy = np.ones(self.env.action_space.n) * self.epsilon / self.env.action_space.n\n",
    "        policy[best_action] = 1 - self.epsilon / self.env.action_space.n * (self.env.action_space.n - 1)\n",
    "        self.policy[state] = policy\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for i in range(10):\n",
    "            with tqdm(total=episodes // 10, desc=f'Episode {i + 1}') as pbar:\n",
    "                for episode in range(episodes // 10):\n",
    "                    state, info = self.env.reset()\n",
    "                    action = self.take_action(state)\n",
    "                    done = False\n",
    "\n",
    "                    gamma_power = 1\n",
    "                    episode_return = 0\n",
    "                    while not done:\n",
    "                        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                        next_action = self.take_action(next_state)\n",
    "\n",
    "                        done = terminated or truncated\n",
    "                        reward = self.custom_reward(done, reward)\n",
    "\n",
    "                        self.update_policy_and_values(state, action, reward, next_state, next_action)\n",
    "                        state, action = next_state, next_action\n",
    "\n",
    "                        episode_return += reward * gamma_power\n",
    "                        gamma_power *= self.gamma\n",
    "\n",
    "                    self.returns.append(episode_return)\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                'epoch': episodes / 10 * i + episode + 1,\n",
    "                                'return': np.mean(self.returns),\n",
    "                                'epsilon': self.epsilon\n",
    "                            }\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    self.epsilon = max(self.epsilon, 0.01)\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()"
   ],
   "id": "bbd96b763277a95a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:53:38.111436Z",
     "start_time": "2025-02-16T09:53:37.277029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='human')\n",
    "environment.reset()"
   ],
   "id": "6dba985bc3465c79",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:53:37.874 python[96012:5287876] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-16 17:53:37.874 python[96012:5287876] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:53:38.120209Z",
     "start_time": "2025-02-16T09:53:38.118735Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsa(environment, gamma=0.9, epsilon=0.99, alpha=0.1, epsilon_decay=0.99)",
   "id": "85f78282109327c7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T09:59:48.912458Z",
     "start_time": "2025-02-16T09:53:38.132160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent.train(100)\n",
    "print(f\"Optimal policy: {agent.policy}\")\n",
    "print(f\"Optimal Q-tables: {agent.q_tables}\")"
   ],
   "id": "2fc38530a1cc379d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 10/10 [00:18<00:00,  1.88s/it, epoch=10, return=-3.4, epsilon=0.904]\n",
      "Episode 2: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, epoch=20, return=-3.37, epsilon=0.818]\n",
      "Episode 3: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it, epoch=30, return=-3.39, epsilon=0.74]\n",
      "Episode 4: 100%|██████████| 10/10 [00:31<00:00,  3.19s/it, epoch=40, return=-3.16, epsilon=0.669]\n",
      "Episode 5: 100%|██████████| 10/10 [00:41<00:00,  4.19s/it, epoch=50, return=-2.97, epsilon=0.605]\n",
      "Episode 6: 100%|██████████| 10/10 [00:50<00:00,  5.04s/it, epoch=60, return=-2.85, epsilon=0.547]\n",
      "Episode 7: 100%|██████████| 10/10 [00:45<00:00,  4.52s/it, epoch=70, return=-2.7, epsilon=0.495]\n",
      "Episode 8: 100%|██████████| 10/10 [00:44<00:00,  4.47s/it, epoch=80, return=-2.54, epsilon=0.448]\n",
      "Episode 9: 100%|██████████| 10/10 [00:44<00:00,  4.49s/it, epoch=90, return=-2.38, epsilon=0.405]\n",
      "Episode 10:  10%|█         | 1/10 [00:06<01:00,  6.76s/it]2025-02-16 17:59:04.003 python[96012:5287876] TSM AdjustCapsLockLEDForKeyTransitionHandling - _ISSetPhysicalKeyboardCapsLockLED Inhibit\n",
      "Episode 10: 100%|██████████| 10/10 [00:53<00:00,  5.34s/it, epoch=100, return=-2.35, epsilon=0.366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: [[0.72547574 0.09150809 0.09150809 0.09150809]\n",
      " [0.09336607 0.09336607 0.09336607 0.71990179]\n",
      " [0.71990179 0.09336607 0.09336607 0.09336607]\n",
      " [0.09526178 0.09526178 0.09526178 0.71421466]\n",
      " [0.72547574 0.09150809 0.09150809 0.09150809]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.71990179 0.09336607 0.09336607 0.09336607]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.09150809 0.09150809 0.09150809 0.72547574]\n",
      " [0.09150809 0.72547574 0.09150809 0.09150809]\n",
      " [0.72547574 0.09150809 0.09150809 0.09150809]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.09150809 0.09150809 0.72547574 0.09150809]\n",
      " [0.09150809 0.72547574 0.09150809 0.09150809]\n",
      " [0.25       0.25       0.25       0.25      ]]\n",
      "Optimal Q-tables: [[-0.98674919 -1.00040282 -1.20326934 -1.0142414 ]\n",
      " [-2.43332946 -1.05580691 -1.26019265 -0.88527454]\n",
      " [-0.85879455 -0.87508781 -0.8625635  -0.92430642]\n",
      " [-1.66224271 -1.23718936 -2.0627924  -0.82230948]\n",
      " [-0.98381606 -1.32997952 -1.46608409 -2.11923105]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.05751073 -1.7195     -1.56532579 -1.55299207]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.06684224 -2.29469992 -1.86195884 -0.86079379]\n",
      " [-0.92129    -0.45549025 -0.82580654 -0.95      ]\n",
      " [ 0.7125045  -0.88470342 -0.68200232 -0.5707943 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.5921222  -0.91481     0.14107346 -0.5       ]\n",
      " [ 0.14668478  4.06928589  0.          1.67450977]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T10:00:39.719416Z",
     "start_time": "2025-02-16T09:59:48.943501Z"
    }
   },
   "cell_type": "code",
   "source": "agent.visualize_policy(delay=0.005)",
   "id": "8d479aac075443e4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### n-step Sarsa\n",
    "我们来回顾一下动作价值的定义：\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t | S_t=s, A_t=a]\n",
    "$$\n",
    "其中 $G_t$ 是在时间步$t$之后收到的截断回报：\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_3 + \\cdots\n",
    "$$\n",
    "\n",
    "事实上，可以对截断回报进行展开：\n",
    "$$\n",
    "\\begin{align*}\n",
    "Sarsa \\leftarrow G_t^1 &= R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) \\\\\n",
    "G_t^2 &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_{\\pi}(S_{t+2}, A_{t+2}) \\\\\n",
    "&\\vdots \\\\\n",
    "n-step \\ Sarsa \\leftarrow G_t^n &= R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n q_{\\pi}(S_{t+n}, A_{t+n}) \\\\\n",
    "&\\vdots \\\\\n",
    "Monte \\ Carlo \\leftarrow G_t^{\\infty} &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "**当$n=1$时（Sarsa）**：\n",
    "$$\n",
    "G_t^1 = \\mathbb{E} [G_t^1|s,a] = \\mathbb{E} [R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1})|S=s,A=a]\n",
    "$$\n",
    "相应地，根据Robbins-Monro算法进行求解，可以得到近似求解算法：\n",
    "$$\n",
    "q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \\alpha [q_t(s_t, a_t) - (R_{t+1} + \\gamma q_t(S_{t+1}, A_{t+1}))]\n",
    "$$\n",
    "\n",
    "\n",
    "**当$n=\\infty$时（Monte Carlo）**：\n",
    "$$\n",
    "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_{t}^{\\infty}|S=s,A=a] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... |S=s,A=a]\n",
    "$$\n",
    "相应地，求解算法：\n",
    "$$\n",
    "q_{t+1}(s_t, a_t) = r_{t+1} + \\gamma r_{t+1} + \\gamma^2 r_{t+3} ...\n",
    "$$\n",
    "\n",
    "\n",
    "**当$\\infty > n >1$时（n-step Sarsa）**：\n",
    "$$\n",
    "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_{t}^{n}|S=s,A=a] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n}q_{\\pi}(S_{t+n},A_{t+n}) |S=s,A=a]\n",
    "$$\n",
    "相应地，根据Robbins-Monro算法进行求解，可以得到近似求解算法：\n",
    "$$\n",
    "q_{t+1}(s_t, a_t) = q_{t}(s_t, a_t) - \\alpha_t(s_t, a_t)[q_{t}(s_t, a_t) - (r_{t+1} + \\gamma r_{t+2} + ... + \\gamma^{n-1}r_{t+n-1} + \\gamma^n q_{t}(s_{t+n}, a_{t+n}))]\n",
    "$$\n",
    "由于在时刻$t$无法采集到$(r_{t+n}, s_{t+n}, a_{t+n})$，根据n-step Sarsa要求，直到$t+n$时刻$q_{t+1}(s,a)$才能被更新。\n",
    "\n",
    "因此，对上式进行重写：\n",
    "$$\n",
    "q_{t+n}(s_t, a_t) = q_{t+n-1}(s_t, a_t) - \\alpha_{t+n-1}(s_t, a_t)[q_{t+n-1}(s_t, a_t) - (r_{t+1} + \\gamma r_{t+2} + ... + \\gamma^{n-1}r_{t+n-1} + \\gamma^n q_{t+n-1}(s_{t+n}, a_{t+n}))]\n",
    "$$\n",
    "其中，$q_{t+n}(s_t, a_t)$是$q_{\\pi}(s_t, a_t)$在时刻$t+n$的估计。"
   ],
   "id": "4dc8f3b8d949fbaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example",
   "id": "e00617ea5c29a09a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T10:00:39.741271Z",
     "start_time": "2025-02-16T10:00:39.731202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sarsas:\n",
    "    \"\"\" n-step Sarsa algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, env, steps=20, alpha=0.1, gamma=0.95, epsilon=0.1, epsilon_decay=0.99):\n",
    "        self.env = env\n",
    "        self.steps = steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.returns = []\n",
    "        self.q_tables = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\" Take an epsilon-greedy action based on the Q-table \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(range(self.env.action_space.n), p=self.policy[state])\n",
    "        else:\n",
    "            return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" Return the best action based on the Q-table \"\"\"\n",
    "        return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def update_policy_and_values(self, state, action, reward, next_state, next_action, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        g = self.q_tables[next_state][next_action]\n",
    "        if len(self.states) == self.steps or (done and len(self.states) > 0):\n",
    "            for i in reversed(range(len(self.states))):\n",
    "                g = self.rewards[i] + self.gamma * g\n",
    "                td_error = g - self.q_tables[self.states[i]][self.actions[i]]\n",
    "                self.q_tables[self.states[i]][self.actions[i]] -= self.alpha * td_error\n",
    "\n",
    "        if done:\n",
    "            self.states = []\n",
    "            self.actions = []\n",
    "            self.rewards = []\n",
    "\n",
    "        best_action = self.best_action(state)\n",
    "        policy = np.ones(self.env.action_space.n) * self.epsilon / self.env.action_space.n\n",
    "        policy[best_action] = 1 - self.epsilon / self.env.action_space.n * (self.env.action_space.n - 1)\n",
    "        self.policy[state] = policy\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for i in range(10):\n",
    "            with tqdm(total=episodes // 10, desc=f'Episode {i + 1}') as pbar:\n",
    "                for episode in range(episodes // 10):\n",
    "                    state, info = self.env.reset()\n",
    "                    action = self.take_action(state)\n",
    "                    done = False\n",
    "\n",
    "                    gamma_power = 1\n",
    "                    episode_return = 0\n",
    "                    while not done:\n",
    "                        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                        next_action = self.take_action(next_state)\n",
    "\n",
    "                        done = terminated or truncated\n",
    "                        reward = self.custom_reward(done, reward)\n",
    "\n",
    "                        self.update_policy_and_values(state, action, reward, next_state, next_action, done)\n",
    "                        state, action = next_state, next_action\n",
    "\n",
    "                        episode_return += reward * gamma_power\n",
    "                        gamma_power *= self.gamma\n",
    "\n",
    "                    self.returns.append(episode_return)\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                'epoch': episodes / 10 * i + episode + 1,\n",
    "                                'return': np.mean(self.returns),\n",
    "                                'epsilon': self.epsilon\n",
    "                            }\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    self.epsilon = max(self.epsilon, 0.01)"
   ],
   "id": "39f69dfc692d8c19",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T10:00:40.065530Z",
     "start_time": "2025-02-16T10:00:39.757001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='human')\n",
    "environment.reset()"
   ],
   "id": "d09cc2a1f486e351",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T10:00:40.140513Z",
     "start_time": "2025-02-16T10:00:40.138103Z"
    }
   },
   "cell_type": "code",
   "source": "agent = Sarsas(environment, steps=20, gamma=0.9, epsilon=0.99, alpha=0.1, epsilon_decay=0.99)",
   "id": "f7cc54e360facb98",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T10:03:26.123855Z",
     "start_time": "2025-02-16T10:00:40.166928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent.train(100)\n",
    "print(f\"Optimal policy: {agent.policy}\")\n",
    "print(f\"Optimal Q-tables: {agent.q_tables}\")"
   ],
   "id": "eb54195ffc36e454",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 10/10 [00:22<00:00,  2.30s/it, epoch=10, return=-3.25, epsilon=0.904]\n",
      "Episode 2: 100%|██████████| 10/10 [00:16<00:00,  1.61s/it, epoch=20, return=-3.42, epsilon=0.818]\n",
      "Episode 3: 100%|██████████| 10/10 [00:11<00:00,  1.10s/it, epoch=30, return=-3.66, epsilon=0.74]\n",
      "Episode 4: 100%|██████████| 10/10 [00:13<00:00,  1.38s/it, epoch=40, return=-3.7, epsilon=0.669]\n",
      "Episode 5: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, epoch=50, return=-3.6, epsilon=0.605]\n",
      "Episode 6: 100%|██████████| 10/10 [00:17<00:00,  1.76s/it, epoch=60, return=-3.59, epsilon=0.547]\n",
      "Episode 7: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it, epoch=70, return=-3.61, epsilon=0.495]\n",
      "Episode 8: 100%|██████████| 10/10 [00:17<00:00,  1.71s/it, epoch=80, return=-3.61, epsilon=0.448]\n",
      "Episode 9: 100%|██████████| 10/10 [00:15<00:00,  1.53s/it, epoch=90, return=-3.62, epsilon=0.405]\n",
      "Episode 10: 100%|██████████| 10/10 [00:15<00:00,  1.50s/it, epoch=100, return=-3.63, epsilon=0.366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: [[0.09150809 0.72547574 0.09150809 0.09150809]\n",
      " [0.09336607 0.71990179 0.09336607 0.09336607]\n",
      " [0.71421466 0.09526178 0.09526178 0.09526178]\n",
      " [0.12495926 0.62512223 0.12495926 0.12495926]\n",
      " [0.09150809 0.72547574 0.09150809 0.09150809]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.09817776 0.70546673 0.09817776 0.09817776]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.09150809 0.72547574 0.09150809 0.09150809]\n",
      " [0.09150809 0.09150809 0.09150809 0.72547574]\n",
      " [0.10533356 0.10533356 0.10533356 0.68399933]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.25       0.25       0.25       0.25      ]\n",
      " [0.23774751 0.28675746 0.23774751 0.23774751]\n",
      " [0.28675746 0.23774751 0.23774751 0.23774751]\n",
      " [0.25       0.25       0.25       0.25      ]]\n",
      "Optimal Q-tables: [[1.99603384e+01 4.75052680e+06 2.60527521e+01 4.21945377e+01]\n",
      " [5.48021705e+00 8.59587619e+03 3.66939938e+00 1.26770259e+01]\n",
      " [8.24974743e+01 1.20411592e+00 1.31600557e+00 3.97501270e+00]\n",
      " [7.34880000e-01 2.05371744e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.78307025e+00 2.31754777e+03 2.79447660e+00 5.00018542e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 7.35425409e+00 1.65500000e+00 5.00000000e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.35999203e+00 8.89813323e+01 2.10829600e+00 2.71873037e+00]\n",
      " [7.22078760e-01 9.26400000e-01 1.61100000e+00 5.99065301e+00]\n",
      " [4.24000000e-01 4.60000000e-01 0.00000000e+00 2.61050311e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 5.00000000e-01 0.00000000e+00 0.00000000e+00]\n",
      " [4.60000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T10:03:26.146005Z",
     "start_time": "2025-02-16T10:03:26.143699Z"
    }
   },
   "cell_type": "code",
   "source": "agent.visualize_policy(delay=0.005)",
   "id": "18e49e8d15927930",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '｜' (U+FF5C) (4086545296.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[11], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    ｜agent.visualize_policy(delay=0.005)\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid character '｜' (U+FF5C)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3241a7ac6314802"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

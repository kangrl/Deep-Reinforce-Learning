{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Dyna Q-Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna Q-Planning\n",
    "Dyna Q-Planning是一个经典的基于模型的强化学习算法，其中使用了一种叫做Q-Planning的方法来基于模型生成一些模拟数据，然后将这些模拟数据与实际经验结合起来进行策略更新。Q-Planning每次选取一个曾经访问过的状态$s$，采取一个曾经在改状态下执行过的动作$a$，通过模型得到转移后的状态$s'$和奖励$r$，然后使用这个混合数据$(s,a,r,s')$依照Q-learning的更新规则来更新动作价值。\n",
    "\n",
    "<center>\n",
    "<img src='../Images/Dyna Q-Planning.png'>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna Q-Planning Algorithms\n",
    "- 初始化Q值表$q_0(s, a)$和模型$M_0(s, a)$、初始化$\\epsilon$-greedy策略$\\pi_0(a|s)$、学习率$\\alpha$、折扣因子$\\gamma$、探索概率$\\epsilon$；\n",
    "- for episode in episodes：\n",
    "  - 得到初始状态$s_0$\n",
    "  - for t=0 to T-1 do:\n",
    "    - 根据$\\epsilon$-greedy策略选择当前状态下的动作作$a_t$，并执行该动作得到奖励$r_{t+1}$和下一个状态$s_{t+1}$；\n",
    "      - 更新Q值：$q_{t+1}(s_t, a_t) \\leftarrow q_t(s_t, a_t) - \\alpha [q_t(s_t, a_t) - (r_{t+1} + \\gamma \\max_a q_t(s_{t+1}, a))]$；\n",
    "      - 更新模型：$M_{t+1}(s_t, a_t) \\leftarrow r_{t+1}, s_{t+1}$；\n",
    "      - for k=0 to K-1 do:  (Q-Planning部分)\n",
    "        - 随机选择一个曾经访问过的状态$s$和执行过的动作$a\n",
    "        - 从模型中采样一个转移$(r, s')$\n",
    "        - 更新Q值：$q(s, a) \\leftarrow q(s, a) - \\alpha [q(s, a) - (r + \\gamma \\max_a q(s', a))]$；\n",
    "      - end for\n",
    "  - end for\n",
    "- end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlanning:\n",
    "    \"\"\" Dyna Q-Planning Algorithms \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.95, epsilon=0.1, epsilon_decay=0.99, n_planning=5):\n",
    "\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # decay rate for exploration\n",
    "        self.n_planning = n_planning  # number of planning steps\n",
    "\n",
    "        self.returns = []\n",
    "        self.q_tables = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n  # uniform policy\n",
    "        self.model = {}  # model of the environment (s, a) -> s'\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\" Take action according to the current policy. \"\"\"\n",
    "\n",
    "        return np.random.choice(range(self.env.action_space.n), p=self.policy[state])\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" Return the best action based on the Q-table \"\"\"\n",
    "\n",
    "        return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def generate_episode(self, state):\n",
    "        episode = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            reward = self.custom_reward(done, reward)\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def q_learning(self, state, action, reward, next_state):\n",
    "        \"\"\" Update the Q-table using the generated episode. \"\"\"\n",
    "\n",
    "        td_target = reward + self.gamma * np.max(self.q_tables[next_state])\n",
    "        td_error = self.q_tables[state][action] - td_target\n",
    "        self.q_tables[state][action] -= self.alpha * td_error\n",
    "\n",
    "    def update_policy(self, state):\n",
    "        \"\"\" Update the policy based on the Q-table. \"\"\"\n",
    "\n",
    "        policy = np.zeros(self.env.action_space.n)\n",
    "        policy[self.best_action(state)] = 1.\n",
    "        self.policy[state] = policy\n",
    "\n",
    "    def update_policy_and_values(self, episode):\n",
    "        \"\"\" Update the policy and values using the generated episode. \"\"\"\n",
    "\n",
    "        gamma_power = 1\n",
    "        episode_return = 0\n",
    "        for state, action, reward, next_state in reversed(episode):\n",
    "            self.q_learning(state, action, reward, next_state)\n",
    "            self.update_policy(state)\n",
    "\n",
    "            self.model[(state, action)] = (reward, next_state)  # Update model with transition\n",
    "\n",
    "            episode_return += reward * gamma_power\n",
    "            gamma_power *= self.gamma\n",
    "\n",
    "        return episode_return\n",
    "\n",
    "    def q_planning(self):\n",
    "        \"\"\" Perform Q-Planning for a number of episodes. \"\"\"\n",
    "\n",
    "        for _ in range(self.n_planning):\n",
    "            (state, action), (reward, next_state) = random.choice(list(self.model.items()))\n",
    "            self.q_learning(state, action, reward, next_state)\n",
    "            self.update_policy(state)\n",
    "\n",
    "    def train(self, episodes=500):\n",
    "        \"\"\" Train the agent for a number of episodes. \"\"\"\n",
    "\n",
    "        for i in range(10):\n",
    "            with tqdm(total=episodes // 10, desc=f'Episode {i + 1}') as pbar:\n",
    "                for idx in range(episodes // 10):\n",
    "                    state, info = self.env.reset()\n",
    "\n",
    "                    # Generate an episode using the current policy\n",
    "                    episode = self.generate_episode(state)\n",
    "                    episode_return = self.update_policy_and_values(episode)\n",
    "                    self.returns.append(episode_return)\n",
    "\n",
    "                    # Perform Q-Planning for a number of episodes\n",
    "                    self.q_planning()\n",
    "\n",
    "                    # Update progress bar\n",
    "                    if (idx + 1) % 10 == 0:\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                'epoch': episodes / 10 * i + idx + 1,\n",
    "                                'return': np.mean(self.returns),\n",
    "                                'epsilon': self.epsilon\n",
    "                            }\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    self.epsilon = max(self.epsilon, 0.01)\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        \"\"\" Visualize the policy learned by the agent \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='rgb_array')\n",
    "environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DynaQPlanning(environment, gamma=0.9, epsilon=0.99, alpha=0.1, epsilon_decay=0.99, n_planning=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 10/10 [00:00<00:00, 1111.02it/s, epoch=10, return=-2.38, epsilon=0.904]\n",
      "Episode 2: 100%|██████████| 10/10 [00:00<00:00, 769.51it/s, epoch=20, return=-4, epsilon=0.818]\n",
      "Episode 3: 100%|██████████| 10/10 [00:00<00:00, 322.53it/s, epoch=30, return=-4.56, epsilon=0.74]\n",
      "Episode 4: 100%|██████████| 10/10 [00:00<00:00, 500.10it/s, epoch=40, return=-4.83, epsilon=0.669]\n",
      "Episode 5: 100%|██████████| 10/10 [00:00<00:00, 357.13it/s, epoch=50, return=-3.81, epsilon=0.605]\n",
      "Episode 6: 100%|██████████| 10/10 [00:00<00:00, 322.61it/s, epoch=60, return=-3.65, epsilon=0.547]\n",
      "Episode 7: 100%|██████████| 10/10 [00:00<00:00, 416.65it/s, epoch=70, return=-3.32, epsilon=0.495]\n",
      "Episode 8: 100%|██████████| 10/10 [00:00<00:00, 296.95it/s, epoch=80, return=-2.5, epsilon=0.448]\n",
      "Episode 9: 100%|██████████| 10/10 [00:00<00:00, 416.52it/s, epoch=90, return=-2.37, epsilon=0.405]\n",
      "Episode 10: 100%|██████████| 10/10 [00:00<00:00, 333.35it/s, epoch=100, return=-2.27, epsilon=0.366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: [[0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "Optimal Q-tables: [[-0.69977049 -0.75717105 -0.70720194 -0.69079698]\n",
      " [-3.85616038 -3.56704926 -3.2566078  -0.94977619]\n",
      " [-0.96776166 -2.16247353 -1.71105487 -0.97011691]\n",
      " [-1.06173258 -3.85616038 -1.02604526 -0.93041163]\n",
      " [-0.6506329  -3.01717736 -4.24952682 -3.85844806]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.68261113 -4.07133943 -3.32616564 -3.81561641]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.44607882 -3.86002434 -2.84766395 -0.52481377]\n",
      " [-0.45296685 -0.19197845 -2.84766395 -3.97445627]\n",
      " [-1.22863274 -2.2595655  -4.39211673 -2.71878562]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-3.8539749   0.28506042  1.18837061 -2.84766395]\n",
      " [ 0.          0.          0.          4.92561278]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent.train(100)\n",
    "print(f\"Optimal policy: {agent.policy}\")\n",
    "print(f\"Optimal Q-tables: {agent.q_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='human')\n",
    "environment.reset()\n",
    "\n",
    "agent.env = environment\n",
    "agent.visualize_policy(delay=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

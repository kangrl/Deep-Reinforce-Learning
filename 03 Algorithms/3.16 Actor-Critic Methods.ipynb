{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.15 Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic\n",
    "\n",
    "Actor-Critic方法将Policy-Based和Value-Based的方法结合起来，使用一个策略网络（Actor：与环境交互更新策略）和一个价值网络（Critic：评估Actor的价值）来估计状态值函数。策略网络用于选择动作，而价值网络用于评估当前策略的好坏。\n",
    "\n",
    "开始之前，我们来回顾一下策略梯度方法：通过最大化一个标量指标$J(\\theta)$寻找一个最优策略。最大化标量指标的$J(\\theta)$通过：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t+1} &= \\theta_t + \\nabla_{\\theta} J(\\theta) \\\\\n",
    "&= \\theta_t + \\alpha \\mathbb{E}_{S \\sim \\eta, A \\sim \\pi} [ \\nabla_{\\theta} \\log \\pi(A|S; \\theta_t)q_{\\pi}(S, A)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "使用随机梯度进行近似有：\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\nabla_{\\theta} \\log \\pi(a_t|s_t; \\theta_t) q_{\\pi}(s_t, a_t)\n",
    "$$\n",
    "\n",
    "**上式非常重要**，直观的揭示了Policy-Based和Value-Based方法能够被结合在一起的原因：一方面可以根据上面的式子直接更新策略参数$\\theta$；另一方面，上式要求我们估计一个动作价值函数$q_{\\pi}(s_t, a_t)$，这个值函数可以通过MC学习(REINFORCE or Monte Carlo Policy Gradient)或者TD学习(Q Actor-Critic)或者来估计。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Algorithm\n",
    "目标：学习一个最优策略最大化$J(\\theta)$\n",
    "- 初始化一个Actor网络和一个Critic网络、初始化学习率$\\alpha_\\theta > 0$和$\\alpha_w > 0$\n",
    "- $for \\ e \\rightarrow E$:\n",
    "    - 遵循策略$\\pi_{a|s_t,\\theta_t}$执行动作$a_t$，并获得环境反馈的奖励$r_t$和下一个状态$s_{t+1}$，以及遵循策略$\\pi_{a|s_{t+1},\\theta_t}$选择动作$a_{t+1}$——**Sarsa**\n",
    "    - Actor(Policy Update): \n",
    "      - $\\theta_{t+1} = \\theta_t + \\alpha_\\theta \\nabla_{\\theta} \\log \\pi(a_t|s_t; \\theta_t) q_{\\pi}(s_t, a_t; w_t)$\n",
    "    - Critic(Value Update): \n",
    "      - $\\delta_t = r_t + \\gamma q_{\\pi}(s_{t+1}, a_{t+1}; w_t) - q_{\\pi}(s_t, a_t; w_t)$\n",
    "      - $w_{t+1} = w_t + \\alpha_w \\delta_t \\nabla_w q_{\\pi}(s_t, a_t; w_t)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Truncated Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Value Iteration and Policy Iteration\n",
    "- 策略迭代从任意的初始策略$\\pi_k$开始，第$k$步遵循以下两个步骤：\n",
    "  - 步骤Policy Evaluation(PE)：给定当前的策略$\\pi_k$，计算相应的价值函数$v_{\\pi_k}$，$$v_{\\pi_k} = r_{\\pi_k} + \\gamma P_{\\pi_k} v_{\\pi_k} \\ \\rightarrow \\ v_{\\pi_{k+1}} = r_{\\pi_k} + \\gamma P_{\\pi_k} v_{\\pi_k}$$\n",
    "  - 步骤Policy Improvement(PI)：基于当前的价值函数$v_{\\pi_k}$，更新策略$\\pi_{k+1}$，$$\\pi_{k+1} = \\arg\\max_{\\pi} (r_{\\pi} + \\gamma p_{\\pi} v_{\\pi_{k+1}})$$\n",
    "- 价值迭代从任意的初始值函数$v_0$开始，第$k$步遵循以下两个步骤：\n",
    "  - 步骤Policy Update(PU)：给定当前的值函数$v_k$，更新策略函数$\\pi_{k+1}$，$$\\pi_{k+1} = \\max_{\\pi} (r_{\\pi} + \\gamma P_{\\pi} v_k)$$\n",
    "  - 步骤Value Update(VU)：给定的策略函数$\\pi_{k+1}$，更新价值函数$v_{k+1}$，$$v_{k+1} = r_{\\pi_{k+1}} + \\gamma p_{\\pi_{k+1}} v_{k}$$\n",
    "\n",
    "- 对比Policy Iteration(PI)和Value Iteration(VI)，我们可以看到两个算法是相似的：\n",
    "  - Policy Iteration: $\\pi_0 \\xrightarrow{PE} v_{\\pi_0} \\xrightarrow{PI} \\pi_1 \\xrightarrow{PE} v_{\\pi_1} \\xrightarrow{PI} \\pi_2 \\xrightarrow{PE} v_{\\pi_2} ... \\xrightarrow{PE} \\pi_{opt}$\n",
    "  - Value Iteration: $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v_0 \\xrightarrow{PU} \\pi'_1 \\xrightarrow{VU} v_2 \\xrightarrow{PU} \\pi'_2 \\xrightarrow{VU} v_3 ... \\xrightarrow{VU} v_{opt} \\xrightarrow{PU} \\pi_{opt}$\n",
    "\n",
    "\n",
    "- 进一步地，对$v$值迭代进行拆解：\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  v_{\\pi_1}^0 &= v_0 \\\\\n",
    "  Value Policy Iteration  \\leftarrow \\ v_1 \\leftarrow v_{\\pi_1}^1 &= r_{\\pi_1} + \\gamma P_{\\pi_1}v_{\\pi_1}^{0} \\\\\n",
    "  v_{\\pi_1}^2 &= r_{\\pi_1} + \\gamma P_{\\pi_1}v_{\\pi_1}^{1} \\\\\n",
    "  \\vdots \\\\\n",
    "  Truncated Policy Iteration \\leftarrow \\ \\hat v_1 \\leftarrow v_{\\pi_1}^j &= r_{\\pi_1} + \\gamma P_{\\pi_1}v_{\\pi_1}^{j-1} \\\\\n",
    "  \\vdots \\\\\n",
    "  Policy Evaluation \\leftarrow \\leftarrow \\ v_{\\pi_1}^{\\infty} &= r_{\\pi_1}^{\\infty} + \\gamma P_{\\pi_1}v_{\\pi_1}^{\\infty} \\\\\n",
    "  \\end{align*}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Policy Iteration Algorithm\n",
    "* 对于所有状态动作对$(s, a)$环境模型的$p(r|s, a)$和$p(s'|s, a)$都已知，随机初始化$\\pi_0$\n",
    "* $while \\ v_{\\pi_k} - v_{\\pi_{k-1}} > \\epsilon$ do:\n",
    "* $\\qquad$ $Policy \\ Evaluation:$\n",
    "* $\\qquad$ 选择一个初始猜测值$v_{\\pi_k}^{0}=v_{\\pi_{k-1}}$, 确定一个最大迭代次数$j_{truncated}$\n",
    "* $\\qquad$ $while \\ j < j_{truncated} \\ do$:\n",
    "* $\\qquad\\qquad$ $for \\ s \\in \\cal S \\ do:$\n",
    "* $\\qquad\\qquad\\qquad$ $v_{\\pi_k}^{j+1}(s) \\leftarrow \\sum_{a \\in \\cal A(s)} \\pi_k (a|s) \\left (\\sum_{r \\in \\cal R(s'|s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi_k}^{j}(s') \\right )$\n",
    "* $\\qquad\\qquad$ $end \\ for$\n",
    "* $\\qquad$ $v_{\\pi_k} \\leftarrow v_{\\pi_k}^{j_{truncated}}$\n",
    "* $\\qquad$ $end \\ while$\n",
    "* $\\qquad$ $Policy \\ Improvement:$\n",
    "* $\\qquad$ $for \\ s \\in \\cal S \\ do:$\n",
    "* $\\qquad\\qquad$ $for \\ a \\in \\cal A(s) \\ do:$\n",
    "* $\\qquad\\qquad\\qquad$ $q_{\\pi_k}(s, a) \\leftarrow \\sum_{r \\in \\cal R(s'|s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi_k}(s')$\n",
    "* $\\qquad\\qquad$ $end \\ for$\n",
    "* $\\qquad\\qquad$ $a_k^*(s) \\leftarrow \\arg\\max_a q_k(s, a)$\n",
    "* $\\qquad\\qquad$ $\\pi_{k+1}(a|s) = 1 \\ if \\ a_k^*(s)=a, \\ else \\ 0$\n",
    "* $\\qquad$ $end \\ for$\n",
    "* $end \\ while$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmac import new\n",
    "\n",
    "\n",
    "class TruncatedPolicyIteration:\n",
    "    \"\"\" Truncated Policy Iteration Algorithm for Freezing-Lake environment \"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma=0.95, delta=1e-6, max_iterations=None):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "        self.values = np.zeros(self.env.observation_space.n)  # Value function initialization\n",
    "        self.policy = np.zeros([self.env.observation_space.n, self.env.action_space.n])  # Policy initialization (uniform random policy)\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        \"\"\" Policy Evaluation Step \"\"\"\n",
    "        steps = 0\n",
    "        delta = float('inf')\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(self.env.observation_space.n):\n",
    "                value = 0  # Temporary values for each state\n",
    "                for action, action_prob in enumerate(self.policy[state]):\n",
    "                    for prob, next_state, reward, done in self.env.unwrapped.P[state][action]:\n",
    "                        value += action_prob * prob * (reward + self.gamma * self.values[next_state] * (1 - done))  # p(r|s,a) & p(s'|s,a) -> prob\n",
    "                delta = max(delta, np.abs(self.values[state] - value))\n",
    "                self.values[state] = value\n",
    "            if self.max_iterations is not None and steps >= self.max_iterations: break\n",
    "            if delta < self.delta: break\n",
    "            steps += 1\n",
    "        print(\"Policy Evaluation Steps:\", steps)  # Print number of steps taken for policy evaluation\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        \"\"\" Policy Improvement Step \"\"\"\n",
    "        policy = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            q_values = np.zeros(self.env.action_space.n)  # Temporary values for each state\n",
    "            for action in range(self.env.action_space.n):\n",
    "                for prob, next_state, reward, done in self.env.unwrapped.P[state][action]:  # p(s'|s,a) & r(s,a,s') -> prob\n",
    "                    q_values[action] += prob * (reward + self.gamma * self.values[next_state] * (1 - done))\n",
    "            best_action = np.argmax(q_values)  # Best action in this state according to current policy values\n",
    "            policy[state][best_action] = 1.0  # Set policy to take best action in this state\n",
    "\n",
    "        self.policy = policy\n",
    "        print(\"Policy after improvement Done.\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Run the Policy Iteration Algorithm \"\"\"\n",
    "        while True:\n",
    "            self.policy_evaluation()\n",
    "            old_policy = np.copy(self.policy)\n",
    "            self.policy_improvement()\n",
    "            new_policy = np.copy(self.policy)\n",
    "\n",
    "            if np.max(np.abs(old_policy - new_policy).flatten()) < self.delta:\n",
    "                break\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        \"\"\" Visualize the policy in the environment \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()  # Close the environment after visualization is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation Steps: 0\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 17\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Policy Evaluation Steps: 20\n",
      "Policy after improvement Done.\n",
      "Optimal Policy: [[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function: [0.41386816 0.42652957 0.44555878 0.46781057 0.49200289 0.51618172\n",
      " 0.53489379 0.54061654 0.4109334  0.42052835 0.43689672 0.45786898\n",
      " 0.48279684 0.51316696 0.5454323  0.55704993 0.39601061 0.39316503\n",
      " 0.37490205 0.         0.42130378 0.49347141 0.56091204 0.5855775\n",
      " 0.36855973 0.35235085 0.30603205 0.2001316  0.30044383 0.\n",
      " 0.56875082 0.62801373 0.32948445 0.28988852 0.19665379 0.\n",
      " 0.28901568 0.3617129  0.53458564 0.68950237 0.30092788 0.\n",
      " 0.         0.0859781  0.21367637 0.27252127 0.         0.77190122\n",
      " 0.28225089 0.         0.05619317 0.04691652 0.         0.25043753\n",
      " 0.         0.87770095 0.27302838 0.19507529 0.12364191 0.\n",
      " 0.23955958 0.48638366 0.7370737  0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name='8x8', is_slippery=True, render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "agent = TruncatedPolicyIteration(env, gamma=0.99, delta=1e-6, max_iterations=20)\n",
    "agent.run()\n",
    "print(f\"Optimal Policy: {agent.policy}\")\n",
    "print(f\"Optimal Value Function: {agent.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.visualize_policy(delay=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

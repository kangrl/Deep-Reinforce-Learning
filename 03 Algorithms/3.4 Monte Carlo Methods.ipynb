{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods\n",
    "在Policy Iteration计算过程有两个关键步骤：1. Policy Evaluation，2. Policy Improvement。其中Policy Evaluation需要通过迭代求解Bellman Expectation Equation来实现，这个过程的计算量非常大，尤其是当状态空间很大的时候。特别地，在计算状态价值$v_{\\pi_k}(s)$的过程中，是通过计算状态$s$下所有可能的动作$a$的价值期望值来实现的，即：\n",
    "$$v_{\\pi_k}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} p(s'|s, a)[r(s, a, s') + \\gamma v_{\\pi_k}(s')]$$\n",
    "这个公式的计算量非常大，尤其是当状态空间很大的时候。\n",
    "\n",
    "Mente Carlo Methods从动作价值的定义出发（动作价值是从当前状态出发能够获得的期望回报），通过采样大量的episode来估计$v_{\\pi_k}(s)$的值，从而避免了复杂的矩阵运算，称为**Model-free近似**：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi_k} (s, a) &= \\mathbb{E} \\left[ G_t | S_t = s, A_t = a \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s, A_t = a \\right] \\\\\n",
    "&\\approx \\frac{1}{N} \\sum_{i=1}^N g_{\\pi_k}^i(s, a) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$g_{\\pi_k}^i(s, a)$是从状态$s$出发，经过动作$a$后，按照策略$\\pi_k$所得到的总回报，$N$是采样的episode数量。\n",
    "\n",
    "具体来说，在Policy Evaluation的过程中，我们只需要知道当前策略$\\pi$下从状态$s$出发，经历一系列的状态转移到达终止状态所得到的回报之和即可。这个过程不需要考虑状态之间的转移概率和即时奖励，只需要采样足够多的episode，然后计算每个状态的平均回报值即可。\n",
    "\n",
    "**动作价值计算流程:**\n",
    "1. 使用第$k$步的策略$\\pi_k$采样若干条轨迹（episodes），每条轨迹包含多个状态和动作：\n",
    "   $$s_0^{(i)} \\xrightarrow{a_0^{(i)}} s_1^{(i)} \\xrightarrow{a_1^{(i)}} s_2^{(i)} \\xrightarrow{a_2^{(i)}} \\dots s_{T-1}^{(i)} \\xrightarrow{a_{T-1}^{(i)}} s_T^{(i)}$$\n",
    "\n",
    "    $$s_0^{(i)} \\xrightarrow{a_0^{(i)}} r_0^{(i)}, s_1^{(i)} \\xrightarrow{a_1^{(i)}} r_1^{(i)}, s_2^{(i)} \\xrightarrow{a_2^{(i)}} r_2^{(i)}, \\dots s_{T-1}^{(i)} \\xrightarrow{a_{T-1}^{(i)}} r_{T-1}^{(i)}, s_T^{(i)}$$\n",
    "    其中$s_t^{(i)}$表示第$i$条轨迹在第$t$步的状态，$a_t^{(i)}$表示第$i$条轨迹在第$t$步的动作，$r_t^{(i)}$表示第$i$条轨迹在第$t$步的奖励。\n",
    "\n",
    "2. 计算每条轨迹的回报：\n",
    "   $$G_t^{(i)} = r_t^{(i)} + \\gamma r_{t+1}^{(i)} + \\gamma^2 r_{t+2}^{(i)} + \\dots + \\gamma^{T-t} r_{T}^{(i)}$$\n",
    "   其中$\\gamma$是折扣因子。\n",
    "3. 计算每条轨迹的平均回报：\n",
    "   $$\\bar{G}_t = \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)}$$\n",
    "   其中$N$是轨迹的数量。\n",
    "   这里通常使用增量法进行计算，即：\n",
    "   $$ N(s) \\leftarrow N(s) + 1 $$\n",
    "\n",
    "   $$ \\bar{G}(s) \\leftarrow \\bar{G}(s) + \\frac{1}{N(s)} (G_t^{(i)} - \\bar{G}(s)) $$\n",
    "   这样可以避免存储所有的回报值，节省内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Basic\n",
    "* 随机初始化$\\pi_0$\n",
    "* $for\\ k = 0, 1, 2, ... \\ do:$\n",
    "* $\\qquad$ $for \\ s \\in \\cal S \\ do:$\n",
    "* $\\qquad\\qquad$ $for \\ a \\in \\cal A(s) \\ do:$\n",
    "* $\\qquad\\qquad\\qquad$ 遵循策略$\\pi_k$，生成从$(s, a)$出发的轨迹，直到终止状态;\n",
    "* $\\qquad\\qquad\\qquad$ $Policy Evaluation:$\n",
    "* $\\qquad\\qquad\\qquad$ 对于轨迹中的每一个状态-动作对$(s, a)$，计算$G_t^{(i)}$，并更新$N(s) \\leftarrow N(s) + 1$和$\\bar{G}(s) \\leftarrow \\bar{G}(s) + \\frac{1}{N(s)} (G_t^{(i)} - \\bar{G}(s))$\n",
    "* $\\qquad\\qquad$ $end \\ for$\n",
    "* $\\qquad\\qquad$ $Policy \\ Improvement:$\n",
    "* $\\qquad\\qquad$ $a_k^*(s) \\leftarrow \\arg\\max_a \\bar{G}(s)$\n",
    "* $\\qquad\\qquad$ $\\pi_{k+1}(a|s) = 1 \\ if \\ a_k^*(s)=a, \\ else \\ 0$\n",
    "* $\\qquad\\qquad$ 如果$\\pi_{k+1} = \\pi_k$，则停止; 否则继续。\n",
    "* $\\qquad$ $end \\ for$\n",
    "* $end \\ for$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Exploring Starts Algorithm\n",
    "- 对于所有的状态-动作对，初始化策略$\\pi_0(s, a)$、动作价值$q(s, a)$、回报$\\bar{G}(s, a)$为0、回报计数$N(s, a)$为0\n",
    "- 对于每个episode:\n",
    "  - 随机选择初始状态$s_0 \\sim \\mu(s)$和初始动作$a_0 \\sim \\pi_0(s_0)$，确保所有的状态-动作对都能够被选择，其中$\\mu(s)$是初始状态分布。\n",
    "  - 生成一个episode $s_0, a_0, r_1, s_1, a_1, ..., s_{T-1}, a_{T-1}, r_T$，根据策略$\\pi_k$和环境模型。\n",
    "  - 初始化每一个episode的回报$g_{\\pi_k}^i (s, a) \\leftarrow 0$。\n",
    "  - $for \\ t = T-1 \\ down \\ to \\ 0$：\n",
    "  - $\\qquad$ $g_{\\pi_k}^i (s, a) \\leftarrow r_{t+1} + \\gamma g_{\\pi_k}^i (s_{t+1}, a_{t+1})$\n",
    "  - $\\qquad$ $\\bar{G}(s, a) \\leftarrow \\bar{G}(s, a) + g_{\\pi_k}^i (s, a)$\n",
    "  - $\\qquad$ $N(s, a) \\leftarrow N(s, a) + 1$\n",
    "  - $\\qquad$ Policy Evaluation: $q_{\\pi_k}(s, a) \\leftarrow \\bar{G}(s, a) / N(s, a)$\n",
    "  - $\\qquad$ Policy Improvement: $\\pi_{k+1} (s) \\leftarrow argmax_{a} q_{\\pi_k}(s, a)$\n",
    "  - $\\qquad$ $if \\ \\pi_{k+1} (s) = \\pi_k(s) \\ for \\ all \\ s$:  $break$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:32:33.021774Z",
     "start_time": "2025-02-13T09:32:32.943997Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:32:33.038930Z",
     "start_time": "2025-02-13T09:32:33.028409Z"
    }
   },
   "source": [
    "class MonteCarloExploringStarts:\n",
    "    \"\"\" Monte Carlo Exploring Starts Algorithm for Frozen-Lake environment \"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma=0.9, num_episodes=1000, epsilon=0.1, epsilon_decay=0.99):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.returns = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.counts = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def generate_episode(self, state):\n",
    "        episode = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            trans = self.env.unwrapped.P[state][action]\n",
    "            idx = np.random.choice(range(len(trans)))\n",
    "            prob, next_state, reward, done = trans[idx]\n",
    "\n",
    "            reward = self.custom_reward(done, reward)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def update(self, episode):\n",
    "        G = 0\n",
    "        visited_state_actions = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in visited_state_actions:\n",
    "                self.returns[state, action] += G\n",
    "                self.counts[state, action] += 1\n",
    "                self.Q[state, action] = self.returns[state, action] / self.counts[state, action]\n",
    "                best_action = np.argmax(self.Q[state])\n",
    "                self.policy[state] = np.zeros_like(self.Q[state])\n",
    "                self.policy[state, best_action] = 1\n",
    "                visited_state_actions.add((state, action))\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.num_episodes):\n",
    "            state = self.env.observation_space.sample()\n",
    "            episode = self.generate_episode(state)\n",
    "            self.update(episode)\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, 0.1)\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:32:33.936652Z",
     "start_time": "2025-02-13T09:32:33.099243Z"
    }
   },
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='8x8', is_slippery=True, render_mode='human')\n",
    "environment.reset()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 17:32:33.699 python[71245:45435508] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-13 17:32:33.699 python[71245:45435508] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:32:36.685162Z",
     "start_time": "2025-02-13T09:32:33.953662Z"
    }
   },
   "source": [
    "agent = MonteCarloExploringStarts(environment, gamma=0.9, num_episodes=5000, epsilon=0.9)\n",
    "agent.run()\n",
    "print(f\"Optimal Policy: {agent.policy}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:33:01.902624Z",
     "start_time": "2025-02-13T09:32:36.689088Z"
    }
   },
   "source": [
    "# Visualize the policy in the environment\n",
    "environment.reset()\n",
    "agent.visualize_policy(delay=0.005)"
   ],
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

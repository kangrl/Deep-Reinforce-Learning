{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.17 Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Invariance\n",
    "\n",
    "策略梯度有一个重要的性质：**baseline invariance**。这意味着如果我们给奖励函数加上一个常数，最优策略不会改变，即：\n",
    "$$\n",
    "\\mathbb{E}_{S \\sim \\eta, A \\sim \\pi}[\\nabla_{\\theta} \\log \\pi (A|S, \\theta_t)q_{\\pi}(S, A)] = \\mathbb{E}_{S \\sim \\eta, A \\sim \\pi}[\\nabla_{\\theta} \\log \\pi (A|S, \\theta_t)q_{\\pi}(S, A) + b(S)]\n",
    "$$\n",
    "其中，$b(S)$ 是一个与状态 $S$ 相关的标量函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Function \n",
    "\n",
    "一个简洁的baseline 是动作价值函数的期望——状态价值：\n",
    "$$\n",
    "b(s) = \\mathbb{E}_{A \\sim \\pi}[q_{\\pi}(s, A)]=v_{\\pi}(s), \\ \\ \\ \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "当$b(s)=v_{\\pi}(s)$时，策略梯度算法中的参数更新变为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t+1} &= \\theta_t + \\alpha \\mathbb{E} \\Big[\\nabla_{\\theta_t} \\log \\pi (A|S, \\theta_t) [q_{\\pi}(S, A) - v_{\\pi}(S)] \\Big] \\\\\n",
    "&= \\theta_t + \\alpha \\mathbb{E} \\Big[\\nabla_{\\theta_t} \\log \\pi (A|S, \\theta_t) A^{\\pi}(S) \\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$A^{\\pi}(S)$ 是优势函数（Advantage Function）：\n",
    "$$\n",
    "A^{\\pi}(s, a) = q_{\\pi}(S, A) - v_{\\pi}(S)\n",
    "$$\n",
    "\n",
    "表示一个动作相对于当前状态下其他动作的相对优势。\n",
    "\n",
    "随机梯度近似（Stochastic Gradient Descent）用于最小化损失函数，通过以下公式更新参数：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t+1} &= \\theta_t + \\alpha \\nabla_{\\theta} \\log \\pi (a_t|s_t; \\theta_t)[q_{\\pi}(s_t, a_t) - v_{\\pi}(s_t)] \\\\\n",
    "&= \\theta_t + \\alpha \\nabla_{\\theta} \\log \\pi (a_t|s_t; \\theta_t) A^{\\pi}(s_t, a_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "策略基于$q_{\\pi}(s_t, a_t)$和$v_{\\pi}(s_t)$之间的差异进行更新————直观上理解是算法试图在状态$s$下选择一个相对于其他动作具有更大价值的动作。\n",
    "\n",
    "- 如果$A^{\\pi}(s_t, a_t) > 0$，则策略会倾向于增加该动作的概率；如果$A^{\\pi}(s_t, a_t) < 0$，则策略会倾向于减少该动作的概率。\n",
    "\n",
    "- 如果$q_{\\pi}(s_t, a_t)$和$v_{\\pi}(s_t)$被Monte Carlo学习估计，则称为REINFORCE Witho Baseline算法；如果使用时序差分（TD）学习方法估计，则称为Advantage Actor-Critic算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic Algorithm (A2C)\n",
    "\n",
    "- 初始化Actor策略网络$\\pi(a|s; \\theta)$和Critic价值网络$v(s; w)$的参数$\\theta$和$w$、初始化学习率$\\alpha_{\\theta}$、$\\alpha_{w}$、折扣因子$\\gamma$\n",
    "- $for \\ e \\rightarrow E \\ do:$\n",
    "  - 遵循当前策略$\\pi(a|s_t; \\theta_t)$生成动作$a_t$并与环境交互，观测奖励$r_{t+1}$和下个状态$s_{t+1}$\n",
    "  - Advantage(TD error):\n",
    "    - $A(s_t, a_t) \\leftarrow r_{t+1} + \\gamma v(s_{t+1}; w) - v(s_t; w)$\n",
    "  - Policy Update (Actor):\n",
    "    - $\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_{\\theta} A(s_t, a_t) \\nabla_{\\theta} log\\pi(a_t|s_t; \\theta_t)$\n",
    "  - Value Update (Critic):\n",
    "    - $w_{t+1} \\leftarrow w_t + \\alpha_w A(s_t, a_t) \\nabla_w v(s_t; w_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Matric-Vector form of Policy Iteration\n",
    "**Policy Iteration** 是强化学习中的一个重要算法，与 **Value Iteration** 一样，用于求解马尔可夫决策过程（MDP）的最优策略。与Value Iteration进行价值迭代不同，Policy Iteration 通过迭代地改进策略和价值函数来找到最优策略。每一次迭代包括两个步骤：策略评估（Policy Evaluation）和策略改进（Policy Improvement）。\n",
    "#### Policy Evaluation:\n",
    "在策略评估步骤中，我们使用当前的策略 $\\pi$ 来计算状态值函数 $v_{\\pi_k}$。具体来说，我们通过解贝尔曼方程来更新价值函数：\n",
    "$$\n",
    "v_{\\pi_k} = r_{\\pi_k} + \\gamma P_{\\pi_k} v_{\\pi_k}\n",
    "$$\n",
    "其中，$v_{\\pi_k}$ 是要被计算的状态价值，$\\pi_k$ 是最近一次计算得到的策略，$\\gamma$ 是折扣因子。\n",
    "\n",
    "在之前求解Bellman Equation的时，我们提供了两种解法：\n",
    "- 闭式解（仅用于理论分析）：$$v_{\\pi_k} = (I - \\gamma P_{\\pi_k})^{-1} r_{\\pi_k}$$\n",
    "- 迭代法（用于实际计算）：$$v_{\\pi_{k+1}}^{(j+1)} = r_{\\pi_k} + \\gamma P_{\\pi_k} v_{\\pi_k}^{(j)} \\ \\ \\ \\ j=0,1,2,...$$，其中，$v_{\\pi_k}^{(j)}$ 是在第 $j$ 次迭代中计算得到的状态价值。\n",
    "\n",
    "\n",
    "\n",
    "#### Policy Improvement:\n",
    "在策略改进步骤中，我们使用当前的价值函数 $v^{\\pi_k}$ 来更新策略 $\\pi$。具体来说，求解如下最优化问题：\n",
    "\n",
    "$$ \\pi_{k+1}(s) = \\arg\\max_{\\pi} (r_{\\pi} + \\gamma P_{\\pi} v^{\\pi_k}) $$\n",
    "其中，$r_{\\pi}$ 是奖励函数，$P_{\\pi}$ 是状态转移矩阵。这个最优化问题表示在每个状态下选择能使当前状态的长期期望回报最大的动作。\n",
    "#### Stopping Criterion:\n",
    "策略迭代算法通常会设置一个停止准则来决定何时停止迭代。一种常见的停止准则是价值函数的差异小于某个阈值，即：\n",
    "$$ \\max_{s} |v_{\\pi_{k+1}}(s) - v_{\\pi_k}(s)| < \\epsilon $$\n",
    "其中，$\\epsilon$ 是一个很小的正数，表示允许的最大误差。当价值函数的差异小于这个阈值时，我们就可以认为策略已经收敛到最优策略了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Elementwise form of Policy Iteration\n",
    "**Policy Iteration** 是强化学习中的一个重要算法，与 **Value Iteration** 一样，用于求解马尔可夫决策过程（MDP）的最优策略。与Value Iteration进行价值迭代不同，Policy Iteration 通过迭代地改进策略和价值函数来找到最优策略。每一次迭代包括两个步骤：策略评估（Policy Evaluation）和策略改进（Policy Improvement）。\n",
    "#### Policy Evaluation:\n",
    "在策略评估步骤中，我们使用当前的策略 $\\pi_k$ 来计算状态值函数 $v^{\\pi_k}$。具体来说，我们通过解贝尔曼期望方程来更新价值函数：\n",
    "$$ v_{\\pi_{k}}^{j+1}(s) = \\sum_{a \\in \\cal A(s)} \\pi_k(a|s) \\left( \\sum_r p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi_k}^{j}(s') \\right)  \\ \\ \\ j=1,2,... $$\n",
    "其中，$v_{\\pi_k}(s)$ 是第 $k$ 次迭代中状态 $s$ 的状态价值函数，$r$ 是在状态 $s$ 下采取动作 $a$ 的即时奖励，$p(s'|s, a)$ 是从状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 的概率，$\\gamma$ 是折扣因子。\n",
    "#### Policy Improvement:\n",
    "在策略改进步骤中，改进的目标是求解$ \\pi_{k+1}(s) = \\arg\\max_{\\pi} (r_{\\pi} + \\gamma P_{\\pi} v^{\\pi_k}) $，其Element-wise 表示为：\n",
    "$$\n",
    "\\pi_{k+1}(s) = \\arg\\max_{\\pi} \\sum_{a \\in \\cal A(s)} \\pi (a|s) \\left (\\sum_{r \\in \\cal R(s'|s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v^{\\pi_k}(s') \\right )\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration Algorithm\n",
    "* 对于所有状态动作对$(s, a)$环境模型的$p(r|s, a)$和$p(s'|s, a)$都已知，随机初始化$\\pi_0$\n",
    "* $while \\ v_{\\pi_k} - v_{\\pi_{k-1}} > \\epsilon$ do:\n",
    "* $\\qquad$ $Policy \\ Evaluation:$\n",
    "* $\\qquad$ 随机初始化$v_{\\pi_0}(s)$\n",
    "* $\\qquad$ $while \\ v_{\\pi_k}^{j+1} - v_{\\pi_k}^{j} > \\epsilon \\ do$:\n",
    "* $\\qquad\\qquad$ $for \\ s \\in \\cal S \\ do:$\n",
    "* $\\qquad\\qquad\\qquad$ $v_{\\pi_k}^{j+1}(s) \\leftarrow \\sum_{a \\in \\cal A(s)} \\pi_k (a|s) \\left (\\sum_{r \\in \\cal R(s'|s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi_k}^{j}(s') \\right )$\n",
    "* $\\qquad\\qquad$ $end \\ for$\n",
    "* $\\qquad$ $end \\ while$\n",
    "* $\\qquad$ $Policy \\ Improvement:$\n",
    "* $\\qquad$ $for \\ s \\in \\cal S \\ do:$\n",
    "* $\\qquad\\qquad$ $for \\ a \\in \\cal A(s) \\ do:$\n",
    "* $\\qquad\\qquad\\qquad$ $q_{\\pi_k}(s, a) \\leftarrow \\sum_{r \\in \\cal R(s'|s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi_k}(s')$\n",
    "* $\\qquad\\qquad$ $end \\ for$\n",
    "* $\\qquad\\qquad$ $a_k^*(s) \\leftarrow \\arg\\max_a q_k(s, a)$\n",
    "* $\\qquad\\qquad$ $\\pi_{k+1}(a|s) = 1 \\ if \\ a_k^*(s)=a, \\ else \\ 0$\n",
    "* $\\qquad$ $end \\ for$\n",
    "* $end \\ while$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:42:31.827760Z",
     "start_time": "2025-02-10T14:42:31.750549Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:42:35.055776Z",
     "start_time": "2025-02-10T14:42:35.048839Z"
    }
   },
   "source": [
    "class PolicyIteration:\n",
    "    \"\"\" Policy Iteration Algorithm for FrozenLake \"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma=0.9, delta=0.001):\n",
    "        \"\"\"  \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "\n",
    "        self.values = np.zeros(env.observation_space.n)\n",
    "        self.policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        \"\"\" Policy Evaluation \"\"\"\n",
    "        steps = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(self.env.observation_space.n):\n",
    "                value = 0\n",
    "                for action, action_prob in enumerate(self.policy[state]):\n",
    "                    # Compute the q-values for the given state and action\n",
    "                    for prob, next_state, reward, terminated in self.env.unwrapped.P[state][action]:\n",
    "                        # Compute the q-value for this transition\n",
    "                        value += action_prob * prob * (reward + self.gamma * self.values[next_state] * (1 - terminated))  # p(r|s, a) & p(s'|s, a) -> prob\n",
    "\n",
    "                delta = max(abs(value - self.values[state]), delta)\n",
    "                self.values[state] = value\n",
    "            steps += 1\n",
    "            if delta < self.delta:\n",
    "                break\n",
    "        print(\"Policy Evaluation Steps:\", steps)\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        \"\"\" Policy Improvement \"\"\"\n",
    "        new_policy = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            q_values = np.zeros(self.env.action_space.n)\n",
    "            for action in range(self.env.action_space.n):\n",
    "                # Compute the q-value for this transition\n",
    "                for prob, next_state, reward, terminated in self.env.unwrapped.P[state][action]:\n",
    "                    q_values[action] += prob * (reward + self.gamma * self.values[next_state] * (1 - terminated))  # p(r|s, a) & p(s'|s, a) -> prob\n",
    "            best_action = np.argmax(q_values)\n",
    "            new_policy[state][best_action] = 1.\n",
    "        self.policy = new_policy\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        \"\"\" Policy Iteration \"\"\"\n",
    "\n",
    "        while True:\n",
    "            self.policy_evaluation()\n",
    "            old_policy = self.policy\n",
    "            self.policy_improvement()\n",
    "            new_policy = self.policy\n",
    "            if max(abs(old_policy - new_policy).flatten()) < self.delta: break\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        \"\"\" Visualize the policy in the environment \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:43:30.495967Z",
     "start_time": "2025-02-10T14:42:38.836538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name='8x8', is_slippery=True, render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "agent = PolicyIteration(env, gamma=0.95, delta=1e-6)\n",
    "agent.policy_iteration()\n",
    "print(f\"Optimal Policy: {agent.policy}\")\n",
    "print(f\"Optimal Value Function: {agent.values}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 22:42:39.479 python[50474:38398341] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-10 22:42:39.479 python[50474:38398341] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation Steps: 1\n",
      "Policy Evaluation Steps: 17\n",
      "Policy Evaluation Steps: 112\n",
      "Policy Evaluation Steps: 25\n",
      "Policy Evaluation Steps: 69\n",
      "Policy Evaluation Steps: 44\n",
      "Policy Evaluation Steps: 27\n",
      "Policy Evaluation Steps: 42\n",
      "Policy Evaluation Steps: 46\n",
      "Policy Evaluation Steps: 48\n",
      "Optimal Policy: [[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function: [0.04824965 0.05586823 0.06811733 0.08391774 0.1024676  0.11983666\n",
      " 0.1339629  0.13978542 0.04666127 0.05244059 0.06307241 0.07861815\n",
      " 0.10127771 0.12463208 0.14929248 0.16185687 0.04221569 0.04443586\n",
      " 0.04566753 0.         0.09272473 0.12444588 0.17563021 0.19997764\n",
      " 0.03688309 0.03737425 0.03670503 0.03286886 0.06709138 0.\n",
      " 0.20535161 0.25590053 0.02997239 0.02779427 0.02042478 0.\n",
      " 0.08627395 0.13289275 0.21694807 0.34685482 0.02163405 0.\n",
      " 0.         0.02610771 0.07245997 0.11643931 0.         0.49257568\n",
      " 0.01671286 0.         0.00542554 0.00998553 0.         0.16235038\n",
      " 0.         0.71607165 0.01443224 0.01000018 0.00714829 0.\n",
      " 0.18362624 0.39624609 0.67143111 0.        ]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:42:26.455865Z",
     "start_time": "2025-02-10T14:42:26.454136Z"
    }
   },
   "cell_type": "code",
   "source": "agent.visualize_policy(delay=0.005)",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

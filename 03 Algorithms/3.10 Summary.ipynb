{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "在3.5-3.8节我们学习了Model-Free的方法，其中有两个典型的算法族：Monte Carlo和Temporal Difference Learning。\n",
    "\n",
    "我们首先介绍了Monte Carlo方法，它通过采样完整的episode来估计价值函数。接着，我们引入了TD学习，这是一种无模型的强化学习算法，它通过单步更新来估计价值函数。我们还介绍了SARSA、n-step SARSA和Q-learning这几种典型的TD算法。\n",
    "\n",
    "事实上，可以将Monte Carlo和Temporal Difference Learning统一来看待。\n",
    "\n",
    "令TD-target为$\\tilde q_t$，TD算法可以被表示为一个统一的表达式：\n",
    "$$\n",
    "q_{t+1} (s_t, a_t) \\leftarrow q_t(s_t, a_t) - \\alpha \\tilde q_t\n",
    "$$\n",
    "\n",
    "| Algorithm       | Expression of the TD target   | Equation to be solved          |\n",
    "|-----------------|-------------------------------|--------------------------------|\n",
    "| Sarsa           | $\\tilde{q}_t = r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})$  | $BE: \\ \\ \\ \\ q_\\pi(s, a) = E\\left[R_{t+1} + \\gamma q_\\pi(s_{t+1}, a_{t+1}) \\mid S_t=s, A_t=a\\right]$  |\n",
    "| n-step Sarsa    | $\\tilde{q}_t = r_{t+1} + \\gamma r_{t+2} + \\cdots + \\gamma^n q_t(s_{t+n}, a_{t+n})$ | $BE: \\ \\ \\ \\ q_\\pi(s, a) = E\\left[R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^n q_\\pi(s_{t+n}, a_{t+n}) \\mid S_t=s, A_t=a\\right]$ |\n",
    "| Q-learning      | $\\tilde{q}_t = r_{t+1} + \\gamma \\max_a q_t(s_{t+1}, a)$                          | $BOE: \\ q(s, a) = E\\left[R_{t+1} + \\gamma \\max_a q(s_{t+1}, a) \\mid S_t=s, A_t=a\\right]$  |\n",
    "| Monte Carlo     | $\\tilde{q}_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\ldots$              | $BE: \\ \\ \\ \\ q_\\pi(s, a) = E\\left[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\mid S_t=s, A_t=a\\right]$    |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "Q-Learning:\n",
    "$$\n",
    "\\begin{cases}\n",
    "q_{t+1}(s_t, a_t) = q_t(s_t,a_t) - \\alpha_t(s_t, a_t)[q_t(s_t,a_t) - (r_{t+1} + \\gamma \\max_{a \\in \\cal A(s_{t+1})}q_t(s_{t+1}, a))] & (s, t)=(s_t, a_t) \\\\\n",
    "q_{t+1}(s, a) = q_t(s, a) & (s, a) \\neq (s_t, a_t)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，$t=0,1,2,...$，$\\alpha_t(s_t, a_t)$是一个很小的正数，代表学习率。\n",
    "\n",
    "Q-Learning算法与Sarsa算法非常类似，区别在于更新Q值时，Sarsa是根据下一个状态和动作来计算的，而Q-Learning则是根据下一个状态的所有可能的动作的最大Q值来计算，即TD Target有差异：\n",
    "- Sarsa TD Target: $r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})$\n",
    "- Q-Learning TD Target: $r_{t+1} + \\gamma \\max_{a' \\in \\cal A(s_{t+1})} q_t(s_{t+1}, a')$\n",
    "\n",
    "Sarsa需要知道下一时刻$t+1$的$(r_{t+1}, s_{t+1}, a_{t+1})$；而Q-Learning仅需知道下一时刻$t+1$的$(r_{t+1}, s_{t+1})$。因此，Sarsa是一种On-Policy算法($a_{t+1}$仍然来自于当前策略)；而Q-Learning是一种Off-Policy算法($a'$的产生来自于greedy策略)，既可以用于on-policy也可以用于off-policy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm(On-Policy Version)\n",
    "Goal：学习一个能够指导智能体从初始状态$s_0$到达目标状态的最优路径\n",
    "- 初始化Q值表$q_0(s, a)$、初始化$\\epsilon$-greedy策略$\\pi_0(a|s)$、学习率$\\alpha$、折扣因子$\\gamma$、探索概率$\\epsilon$；\n",
    "- 对于每一个episode:\n",
    "    - 如果$s_t(t=0,1,2,...)$不是终止状态，则：\n",
    "      - 收集状态$s_t$上的经验样本$(a_t, r_{t+1}, s_{t+1})$：遵循策略$\\pi(a|s)$，采取动作$a_t$，与环境交互得到奖励$r_{t+1}$，进入状态$s_{t+1}$；\n",
    "      - 更新Q值表$(s_t, a_t)$：\n",
    "        - $q_{t+1}(s_t, a_t) \\leftarrow q_t(s_t, a_t) - \\alpha [q_t(s_t, a_t) - (r_{t+1} + \\gamma max_{a'} q_t(s_{t+1}, a'))]$\n",
    "      - 更新策略$\\pi(a|s)$：\n",
    "        - $a = \\arg\\max_a q_{t+1}(s_t, a)$， $\\pi_{t+1}(a|s_t)=1-\\frac{\\epsilon}{|\\cal A(s_t)|}(|\\cal A(s_t)| - 1)$，否则$\\pi_{t+1}(a|s_t) = \\frac{\\epsilon}{|\\cal A(s_t)|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningOnPolicy:\n",
    "    \"\"\" Q-Learning On-Policy Algorithm \"\"\"\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.99):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.returns = []\n",
    "        self.q_tables = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\" Take action according to the current policy. \"\"\"\n",
    "\n",
    "        return np.random.choice(range(self.env.action_space.n), p=self.policy[state])\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" Return the best action based on the Q-table \"\"\"\n",
    "\n",
    "        return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def update_policy_and_values(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Update the policy based on the Q-table. \"\"\"\n",
    "\n",
    "        # Update the Q-value for the state and action pair\n",
    "        td_target = reward + self.gamma * self.q_tables[next_state][self.best_action(next_state)]\n",
    "        td_error = self.q_tables[state][action] - td_target\n",
    "        self.q_tables[state][action] -= self.alpha * td_error\n",
    "\n",
    "        # Update the policy based on the Q-table\n",
    "        best_action = self.best_action(state)\n",
    "        policy = np.ones(self.env.action_space.n) * self.epsilon / self.env.action_space.n\n",
    "        policy[best_action] = 1 - self.epsilon / self.env.action_space.n * (self.env.action_space.n - 1)\n",
    "        self.policy[state] = policy\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        \"\"\" Train the agent using Q-learning. \"\"\"\n",
    "\n",
    "        for i in range(10):\n",
    "            with tqdm(total=episodes // 10, desc=f'Episode {i + 1}') as pbar:\n",
    "                for episode in range(episodes // 10):\n",
    "                    state, info = self.env.reset()\n",
    "                    action = self.take_action(state)\n",
    "                    done = False\n",
    "\n",
    "                    gamma_power = 1\n",
    "                    episode_return = 0\n",
    "                    while not done:\n",
    "                        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                        done = terminated or truncated\n",
    "                        reward = self.custom_reward(done, reward)\n",
    "\n",
    "                        self.update_policy_and_values(state, action, reward, next_state, done)\n",
    "                        state, action = next_state, self.best_action(next_state)\n",
    "\n",
    "                        episode_return += reward * gamma_power\n",
    "                        gamma_power *= self.gamma\n",
    "\n",
    "                    self.returns.append(episode_return)\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                'epoch': episodes / 10 * i + episode + 1,\n",
    "                                'return': np.mean(self.returns),\n",
    "                                'epsilon': self.epsilon\n",
    "                            }\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    self.epsilon = max(self.epsilon, 0.01)\n",
    "\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        \"\"\" Visualize the policy learned by the agent \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='rgb_array')\n",
    "environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningOnPolicy(environment, gamma=0.9, epsilon=0.99, alpha=0.1, epsilon_decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 100/100 [00:00<00:00, 489.88it/s, epoch=100, return=-1.02, epsilon=0.366]\n",
      "Episode 2: 100%|██████████| 100/100 [00:00<00:00, 468.57it/s, epoch=200, return=-0.904, epsilon=0.134]\n",
      "Episode 3: 100%|██████████| 100/100 [00:00<00:00, 535.33it/s, epoch=300, return=-0.767, epsilon=0.049]\n",
      "Episode 4: 100%|██████████| 100/100 [00:00<00:00, 486.34it/s, epoch=400, return=-0.667, epsilon=0.018]\n",
      "Episode 5: 100%|██████████| 100/100 [00:00<00:00, 452.41it/s, epoch=500, return=-0.657, epsilon=0.01] \n",
      "Episode 6: 100%|██████████| 100/100 [00:00<00:00, 427.75it/s, epoch=600, return=-0.636, epsilon=0.01]\n",
      "Episode 7: 100%|██████████| 100/100 [00:00<00:00, 442.73it/s, epoch=700, return=-0.6, epsilon=0.01] \n",
      "Episode 8: 100%|██████████| 100/100 [00:00<00:00, 488.31it/s, epoch=800, return=-0.598, epsilon=0.01]\n",
      "Episode 9: 100%|██████████| 100/100 [00:00<00:00, 501.82it/s, epoch=900, return=-0.552, epsilon=0.01]\n",
      "Episode 10: 100%|██████████| 100/100 [00:00<00:00, 482.37it/s, epoch=1e+3, return=-0.528, epsilon=0.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: [[0.9925 0.0025 0.0025 0.0025]\n",
      " [0.0025 0.0025 0.0025 0.9925]\n",
      " [0.9925 0.0025 0.0025 0.0025]\n",
      " [0.0025 0.0025 0.0025 0.9925]\n",
      " [0.9925 0.0025 0.0025 0.0025]\n",
      " [0.25   0.25   0.25   0.25  ]\n",
      " [0.9925 0.0025 0.0025 0.0025]\n",
      " [0.25   0.25   0.25   0.25  ]\n",
      " [0.0025 0.0025 0.0025 0.9925]\n",
      " [0.0025 0.9925 0.0025 0.0025]\n",
      " [0.9925 0.0025 0.0025 0.0025]\n",
      " [0.25   0.25   0.25   0.25  ]\n",
      " [0.25   0.25   0.25   0.25  ]\n",
      " [0.0025 0.0025 0.9925 0.0025]\n",
      " [0.0025 0.9925 0.0025 0.0025]\n",
      " [0.25   0.25   0.25   0.25  ]]\n",
      "Optimal Q-tables: [[-0.52765605 -1.12882894 -1.24917297 -1.12042535]\n",
      " [-1.61576402 -1.76066102 -1.72640039 -0.66240109]\n",
      " [-1.16214959 -1.53644207 -1.53728894 -1.59172428]\n",
      " [-1.76609981 -1.70658526 -1.73265083 -1.12689073]\n",
      " [-0.38094698 -0.96539    -0.94199617 -1.00023535]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-2.21006926 -3.25573529 -3.24559806 -3.22330874]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.56172665 -0.51791    -0.53729618  0.04783567]\n",
      " [-0.5         0.75911197 -0.52612656 -0.5       ]\n",
      " [ 0.92306596 -0.43387128 -0.5764406  -0.5       ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.5        -0.509       2.8613806  -0.51506802]\n",
      " [-0.01        6.95885273  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent.train(1000)\n",
    "print(f\"Optimal policy: {agent.policy}\")\n",
    "print(f\"Optimal Q-tables: {agent.q_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='human')\n",
    "environment.reset()\n",
    "\n",
    "agent.env = environment\n",
    "agent.visualize_policy(delay=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorgorithm(Off-Policy Version)\n",
    "Goal：从遵循行为策略$\\pi_{behavior}(a|s)$生成的经验样本中学习最优策略$\\pi_{target}(a|s)$。\n",
    "\n",
    "- 初始化Q值表$q_0(s, a)$、初始化行为策略$\\pi_{behavior}(a|s)$，$\\alpha$为学习率，$\\gamma$为折扣因子\n",
    "- 对于每一个遵循策略$\\pi_{behavior}(a|s)$生成的episode $\\{ s_0,a_0,r_1,s_1,a_1,r_2,\\cdots \\}$:\n",
    "  - 对于这个episode中的每一步$t$:\n",
    "    - 更新Q值：\n",
    "      - $q_{t+1}(s_t, a_t) \\leftarrow (1-\\alpha) q_t(s_t, a_t) + \\alpha [r_{t} + \\gamma q_t(s_{t+1}, \\arg\\max_a q_t(s_{t+1}, a))]$\n",
    "    - 更新策略$\\pi(a|s)$：\n",
    "      - $\\pi_{target,t+1}(a|s_t)=1, if \\quad a = \\arg\\max_a q_{t+1}(s_t, a) \\quad else \\quad 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningOffPolicy:\n",
    "    \"\"\" Q-Learning Off-Policy Algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.99):\n",
    "\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.returns = []\n",
    "        self.q_tables = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\" Take action according to the current policy. \"\"\"\n",
    "\n",
    "        # return np.random.choice(range(self.env.action_space.n), p=self.policy[state])  # on-policy vs off-policy\n",
    "\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            # Explore (take a random action with probability epsilon)\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit (take the best known action according to Q-values)\n",
    "            action = np.argmax(self.q_tables[state])\n",
    "        return action\n",
    "\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" Return the best action based on the Q-table \"\"\"\n",
    "\n",
    "        return np.argmax(self.q_tables[state])\n",
    "\n",
    "    def generate_episode(self, state):\n",
    "        episode = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            reward = self.custom_reward(done, reward)\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\" Get Optimal Policy from value function \"\"\"\n",
    "\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            policy = np.zeros(self.env.action_space.n)\n",
    "            policy[self.best_action(state)] = 1.0\n",
    "            self.policy[state] = policy\n",
    "\n",
    "        return self.policy\n",
    "\n",
    "    def update_policy_and_values(self, episode):\n",
    "        \"\"\" Update the policy and values using the generated episode. \"\"\"\n",
    "\n",
    "        gamma_power = 1\n",
    "        episode_return = 0\n",
    "        for state, action, reward, next_state in reversed(episode):\n",
    "            td_target = reward + self.gamma * np.max(self.q_tables[next_state])\n",
    "            td_error = self.q_tables[state][action] - td_target\n",
    "            self.q_tables[state][action] -= self.alpha * td_error\n",
    "\n",
    "              # on-policy vs off-policy\n",
    "            # policy = np.zeros(self.env.action_space.n)\n",
    "            # policy[self.best_action(state)] = 1.\n",
    "            # self.policy[state] = policy\n",
    "\n",
    "            episode_return += reward * gamma_power\n",
    "            gamma_power *= self.gamma\n",
    "\n",
    "        return episode_return\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        \"\"\" Train the agent for a specified number of episodes. \"\"\"\n",
    "\n",
    "        for i in range(10):\n",
    "            with tqdm(total=episodes // 10, desc=f'Episode {i + 1}') as pbar:\n",
    "                for idx in range(episodes // 10):\n",
    "                    state, info = self.env.reset()\n",
    "\n",
    "                    episode = self.generate_episode(state)\n",
    "                    episode_return = self.update_policy_and_values(episode)\n",
    "                    self.returns.append(episode_return)\n",
    "\n",
    "                    if (idx + 1) % 10 == 0:\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                'epoch': episodes / 10 * i + idx + 1,\n",
    "                                'return': np.mean(self.returns),\n",
    "                                'epsilon': self.epsilon\n",
    "                            }\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    self.epsilon = max(self.epsilon, 0.01)\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        \"\"\" Visualize the policy learned by the agent \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='rgb_array')\n",
    "environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningOffPolicy(environment, gamma=0.9, epsilon=0.99, alpha=0.1, epsilon_decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 100/100 [00:00<00:00, 1945.05it/s, epoch=100, return=-4.72, epsilon=0.366]\n",
      "Episode 2: 100%|██████████| 100/100 [00:00<00:00, 1075.91it/s, epoch=200, return=-4.44, epsilon=0.134]\n",
      "Episode 3: 100%|██████████| 100/100 [00:00<00:00, 708.21it/s, epoch=300, return=-2.99, epsilon=0.049]\n",
      "Episode 4: 100%|██████████| 100/100 [00:00<00:00, 726.11it/s, epoch=400, return=-2.12, epsilon=0.018]\n",
      "Episode 5: 100%|██████████| 100/100 [00:00<00:00, 509.10it/s, epoch=500, return=-0.968, epsilon=0.01]\n",
      "Episode 6: 100%|██████████| 100/100 [00:00<00:00, 517.54it/s, epoch=600, return=-0.127, epsilon=0.01]\n",
      "Episode 7: 100%|██████████| 100/100 [00:00<00:00, 540.46it/s, epoch=700, return=0.171, epsilon=0.01] \n",
      "Episode 8: 100%|██████████| 100/100 [00:00<00:00, 651.12it/s, epoch=800, return=0.604, epsilon=0.01]\n",
      "Episode 9: 100%|██████████| 100/100 [00:00<00:00, 688.59it/s, epoch=900, return=0.944, epsilon=0.01]\n",
      "Episode 10: 100%|██████████| 100/100 [00:00<00:00, 746.57it/s, epoch=1e+3, return=1.29, epsilon=0.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Q-tables: [[-0.26585427 -0.70959213 -0.61567585 -0.6655357 ]\n",
      " [-2.14415945 -2.29096442 -2.31466029 -0.82013835]\n",
      " [-1.47319414 -1.45481711 -1.45123551 -1.17734321]\n",
      " [-2.14334324 -1.59458302 -1.79249331 -1.17591872]\n",
      " [-0.10646646 -1.8313499  -1.40985262 -2.69788572]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.88377165 -3.44290973 -3.10070478 -3.41517606]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-2.5504639  -0.96482239 -1.7323844   0.33740344]\n",
      " [-2.11035683  1.15634603 -0.86769536 -1.86716592]\n",
      " [ 2.36330065 -1.50077207 -1.32236305 -1.37548157]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.69089204  0.32482169  2.61287462  0.69128996]\n",
      " [ 1.25908531  5.90498667  1.75150706  2.63298534]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent.train(1000)\n",
    "agent.get_optimal_policy()\n",
    "print(f\"Optimal policy: {agent.policy}\")\n",
    "print(f\"Optimal Q-tables: {agent.q_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='human')\n",
    "environment.reset()\n",
    "\n",
    "agent.env = environment\n",
    "agent.visualize_policy(delay=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

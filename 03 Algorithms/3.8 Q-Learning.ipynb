{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "Q-Learning:\n",
    "$$\n",
    "\\begin{cases}\n",
    "q_{t+1}(s_t, a_t) = q_t(s_t,a_t) - \\alpha_t(s_t, a_t)[q_t(s_t,a_t) - (r_{t+1} + \\gamma \\max_{a \\in \\cal A(s_{t+1})}q_t(s_{t+1}, a))] & (s, t)=(s_t, a_t) \\\\\n",
    "q_{t+1}(s, a) = q_t(s, a) & (s, a) \\neq (s_t, a_t)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，$t=0,1,2,...$，$\\alpha_t(s_t, a_t)$是一个很小的正数，代表学习率。\n",
    "\n",
    "Q-Learning算法与Sarsa算法非常类似，区别在于更新Q值时，Sarsa是根据下一个状态和动作来计算的，而Q-Learning则是根据下一个状态的所有可能的动作的最大Q值来计算，即TD Target有差异：\n",
    "- Sarsa TD Target: $r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})$\n",
    "- Q-Learning TD Target: $r_{t+1} + \\gamma \\max_{a' \\in \\cal A(s_{t+1})} q_t(s_{t+1}, a')$\n",
    "\n",
    "Sarsa需要知道下一时刻$t+1$的$(r_{t+1}, s_{t+1}, a_{t+1})$；而Q-Learning仅需知道下一时刻$t+1$的$(r_{t+1}, s_{t+1})$。因此，Sarsa是一种On-Policy算法($a_{t+1}$仍然来自于当前策略)；而Q-Learning是一种Off-Policy算法($a'$的产生来自于greedy策略)，既可以用于on-policy也可以用于off-policy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm(On-Policy Version)\n",
    "Goal：学习一个能够指导智能体从初始状态$s_0$到达目标状态的最优路径\n",
    "- 初始化Q值表$q_0(s, a)$、初始化$\\epsilon$-greedy策略$\\pi_0(a|s)$、学习率$\\alpha$、折扣因子$\\gamma$、探索概率$\\epsilon$；\n",
    "- 对于每一个episode:\n",
    "    - 如果$s_t(t=0,1,2,...)$不是终止状态，则：\n",
    "      - 收集状态$s_t$上的经验样本$(a_t, r_{t+1}, s_{t+1})$：遵循策略$\\pi(a|s)$，采取动作$a_t$，与环境交互得到奖励$r_{t+1}$，进入状态$s_{t+1}$；\n",
    "      - 更新Q值表$(s_t, a_t)$：\n",
    "        - $q_{t+1}(s_t, a_t) \\leftarrow q_t(s_t, a_t) - \\alpha [q_t(s_t, a_t) - (r_{t+1} + \\gamma max_{a'} q_t(s_{t+1}, a'))]$\n",
    "      - 更新策略$\\pi(a|s)$：\n",
    "        - $a = \\arg\\max_a q_{t+1}(s_t, a)$， $\\pi_{t+1}(a|s_t)=1-\\frac{\\epsilon}{|\\cal A(s_t)|}(|\\cal A(s_t)| - 1)$，否则$\\pi_{t+1}(a|s_t) = \\frac{\\epsilon}{|\\cal A(s_t)|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorgorithm(Off-Policy Version)\n",
    "Goal：从遵循行为策略$\\pi_{behavior}(a|s)$生成的经验样本中学习最优策略$\\pi_{target}(a|s)$。\n",
    "\n",
    "- 初始化Q值表$q_0(s, a)$、初始化行为策略$\\pi_{behavior}(a|s)$，$\\alpha$为学习率，$\\gamma$为折扣因子\n",
    "- 对于每一个遵循策略$\\pi_{behavior}(a|s)$生成的episode $\\{ s_0,a_0,r_1,s_1,a_1,r_2,\\cdots \\}$:\n",
    "  - 对于这个episode中的每一步$t$:\n",
    "    - 更新Q值：\n",
    "      - $q_{t+1}(s_t, a_t) \\leftarrow (1-\\alpha) q_t(s_t, a_t) + \\alpha [r_{t} + \\gamma q_t(s_{t+1}, \\arg\\max_a q_t(s_{t+1}, a))]$\n",
    "    - 更新策略$\\pi(a|s)$：\n",
    "      - $\\pi_{target,t+1}(a|s_t)=1, if \\quad a = \\arg\\max_a q_{t+1}(s_t, a) \\quad else \\quad 0$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 03 Algorithms",
   "id": "32d341c4e30b47a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.5 Temporal-Difference Methods",
   "id": "ede854d2ede1d6d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Temporal-Difference Learning of State Values\n",
    "给定策略$\\pi$，我们的目标是：对于所有的$s \\in \\mathcal{S}$，估计状态价值$v_{\\pi}(s)$。假设我们有遵循策略$\\pi$生成的样本集$ (s_0, r_1, s_1, s_1, r_2, s_2, ... ,s_{T-1}, r_T, \\mathcal s_T) $，也可以表示为$\\{(s_i, r_{i+1}, s_{i+1})\\}^{T}_{i=0}$，从状态价值函数的定义\n",
    "$$\n",
    "v_{\\pi}(s_t) = E_{\\pi}[r_{t+1} + \\gamma v_{\\pi}(s_{t+1}) | s_t = s]\n",
    "$$\n",
    "可令：\n",
    "$$\n",
    "g(v_{\\pi}(s_t)) = v_{\\pi}(s_t) - E_{\\pi}[r_{t+1} + \\gamma v_{\\pi}(s_{t+1}) | s_t = s]\n",
    "$$\n",
    "利用Robbins-Monro算法进行求解，我们有：\n",
    "$$\n",
    "\\tilde{g}(v_{\\pi}(s_t)) = v_{\\pi}(s_t) - [(r_{t+1} + \\gamma v_{\\pi}(s_{t+1}))]\n",
    "$$\n",
    "根据 Robbins-Monro算法，$g(v_{\\pi}(s_t)) = 0$的解可以使用一下迭代过程来求解：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{t+1}(s_t) &= v_t(s_t) - \\alpha_t(s_t) \\tilde{g}(v_t(s_t)) \\\\\n",
    "&= v_t(s_t) - \\alpha_t(s_t)[v_t(s_t) - (r_{t+1} + \\gamma v_t(s_{t+1}))]\n",
    "\\end{aligned}\n",
    "$$\n",
    "即为Temporal-Difference方法。\n",
    "\n",
    "\n",
    "Temporal-Difference方法利用这些样本来估计状态价值:\n",
    "$$\n",
    "\\begin{cases}\n",
    "v_{t+1}(s_t) = v_t(s_t) - \\alpha_t(s_t)[v_t(s_t) - (r_{t+1} + \\gamma v_t(s_{t+1}))] & s=s_t \\\\\n",
    "v_{t+1}(s) = v_t(s) & s \\neq s_t\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，$t=0,1,2,...$，$\\alpha_t$是一个很小的正数，代表学习率。\n",
    "\n",
    "Temporal-Difference中的重要名词解释：\n",
    "$$\n",
    "\\underbrace{v_{t+1}(s_t)}_{prediction} = \\underbrace{v_t(s_t)}_{current \\ estimate} - \\alpha_t(s_t)\\overbrace{[v_t(s_t) - (\\underbrace {r_{t+1} + \\gamma v_t(s_{t+1})}_{TD \\ target \\ \\bar v_t})]}^{TD \\ error \\ \\delta_t}\n",
    "$$"
   ],
   "id": "68bbc2546e9f67ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Temporal-Difference Learning VS Monte Carlo Learning\n",
    "| Temporal-Difference Learning                                       | Monte Carlo Learning                                             |\n",
    "|--------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| **Incremental:** TD算法是增量的，它能够在样本集上直接增量更新状态/动作价值；                   | **Non-incremental:** MC方法则是非增量的，必须等到一整个episode结束后才能进行更新；因为MC算法必须计算整个episode的return，这个过程通常比较耗时； |\n",
    "| **Continuing task:** 因为TD算法是增量的，它既能够处理回合制和连续性任务；                   | **Episodic Tasks:** 由于MC是非增量的，它只能处理有限步的回合任务;                     |\n",
    "| **Bootstrapping:** 采用bootstrapping的方法预测当前状态的价值函数（通常利用下一个时刻的状态-动作价值来近似当前的状态-动作价值）,要求对状态/动作价值的猜测值 | **Sampling:** 使用实际回报值进行更新，而不用估计的未来奖励值；                           |\n",
    "| **Low estimation variance:** TD方法通常有较小的方差（因为它是单步更新）；               | **High estimation variance:** 由于MC方法是使用完整的episode，所以它的方差相对较高;    |\n",
    "| **Fastly**: 只利用单步转移后的奖励来更新对状态价值的估计，不需要等待整个回合结束;                    |  **Slowly**：需要完整的轨迹才能计算期望的回报。它们通过对多次经历中的每个状态的实际返回值取平均来估计其期望价值;|"
   ],
   "id": "8072f2676c3c5fe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example",
   "id": "2d715e7cf37e28aa"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:24.660440Z",
     "start_time": "2025-02-20T13:17:24.559824Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:24.678885Z",
     "start_time": "2025-02-20T13:17:24.672338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TemporalDifference:\n",
    "    \"\"\" Temporal Difference learning for estimating the state values\"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma=0.95, alpha=0.1, epsilon=0.05, episodes=500, epsilon_decay=0.99):\n",
    "        \"\"\" Initialize the parameters of temporal difference method \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.values = np.zeros(env.observation_space.n)\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    def select_best_action(self, state):\n",
    "        \"\"\" Select the best action based on the current value function \"\"\"\n",
    "        q_values = np.zeros(self.env.action_space.n)\n",
    "        for action in range(self.env.action_space.n):\n",
    "            for prob, next_state, reward, done in self.env.unwrapped.P[state][action]:\n",
    "                reward = self.custom_reward(done, reward)\n",
    "                q_values[action] += prob * (reward + self.gamma * self.values[state] * (1 - done))\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_reward(done, reward):\n",
    "        if done and reward == 1:\n",
    "            return 10\n",
    "        elif done and reward == 0:\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\" Take an action based on the current policy \"\"\"\n",
    "\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return self.select_best_action(state)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" Update the value function using temporal difference method \"\"\"\n",
    "\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            reward = self.custom_reward(done, reward)\n",
    "\n",
    "            td_target = reward + self.gamma * self.values[next_state]\n",
    "            td_error = self.values[state] - td_target\n",
    "            self.values[state] += -self.alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "\n",
    "        self.env.render()\n",
    "        self.env.close()\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\" Get Optimal Policy from value function \"\"\"\n",
    "\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            policy = np.zeros(self.env.action_space.n)\n",
    "            policy[self.select_best_action(state)] = 1.0\n",
    "            self.policy[state] = policy\n",
    "\n",
    "        return self.policy\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Train the agent for a specified number of episodes \"\"\"\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            self.update()\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, 0.01)\n",
    "            # print(f'{episode} Episode Complete Values:', self.values, '\\n')\n",
    "            print(f'Epsilon:', self.epsilon, '\\n')\n",
    "        print('Training complete')"
   ],
   "id": "9adb2f1bc8c5cf29",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:24.762575Z",
     "start_time": "2025-02-20T13:17:24.754763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='rgb_array')\n",
    "environment.reset()"
   ],
   "id": "80756eb35e00e1a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:24.797859Z",
     "start_time": "2025-02-20T13:17:24.794866Z"
    }
   },
   "cell_type": "code",
   "source": "agent = TemporalDifference(environment, gamma=0.9, episodes=100, epsilon=0.9, alpha=0.1)",
   "id": "f8f10e97d2cc0fdd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:24.828282Z",
     "start_time": "2025-02-20T13:17:24.804373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent.epsilon = 0.99\n",
    "agent.train()\n",
    "print(f\"Optimal Values: {agent.values}\")"
   ],
   "id": "f6eb30c3caac11c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.9801 \n",
      "\n",
      "Epsilon: 0.9702989999999999 \n",
      "\n",
      "Epsilon: 0.96059601 \n",
      "\n",
      "Epsilon: 0.9509900498999999 \n",
      "\n",
      "Epsilon: 0.9414801494009999 \n",
      "\n",
      "Epsilon: 0.9320653479069899 \n",
      "\n",
      "Epsilon: 0.92274469442792 \n",
      "\n",
      "Epsilon: 0.9135172474836407 \n",
      "\n",
      "Epsilon: 0.9043820750088043 \n",
      "\n",
      "Epsilon: 0.8953382542587163 \n",
      "\n",
      "Epsilon: 0.8863848717161291 \n",
      "\n",
      "Epsilon: 0.8775210229989678 \n",
      "\n",
      "Epsilon: 0.8687458127689781 \n",
      "\n",
      "Epsilon: 0.8600583546412883 \n",
      "\n",
      "Epsilon: 0.8514577710948754 \n",
      "\n",
      "Epsilon: 0.8429431933839266 \n",
      "\n",
      "Epsilon: 0.8345137614500874 \n",
      "\n",
      "Epsilon: 0.8261686238355865 \n",
      "\n",
      "Epsilon: 0.8179069375972307 \n",
      "\n",
      "Epsilon: 0.8097278682212583 \n",
      "\n",
      "Epsilon: 0.8016305895390458 \n",
      "\n",
      "Epsilon: 0.7936142836436553 \n",
      "\n",
      "Epsilon: 0.7856781408072188 \n",
      "\n",
      "Epsilon: 0.7778213593991465 \n",
      "\n",
      "Epsilon: 0.7700431458051551 \n",
      "\n",
      "Epsilon: 0.7623427143471035 \n",
      "\n",
      "Epsilon: 0.7547192872036325 \n",
      "\n",
      "Epsilon: 0.7471720943315961 \n",
      "\n",
      "Epsilon: 0.7397003733882802 \n",
      "\n",
      "Epsilon: 0.7323033696543974 \n",
      "\n",
      "Epsilon: 0.7249803359578534 \n",
      "\n",
      "Epsilon: 0.7177305325982748 \n",
      "\n",
      "Epsilon: 0.7105532272722921 \n",
      "\n",
      "Epsilon: 0.7034476949995692 \n",
      "\n",
      "Epsilon: 0.6964132180495735 \n",
      "\n",
      "Epsilon: 0.6894490858690777 \n",
      "\n",
      "Epsilon: 0.682554595010387 \n",
      "\n",
      "Epsilon: 0.6757290490602831 \n",
      "\n",
      "Epsilon: 0.6689717585696803 \n",
      "\n",
      "Epsilon: 0.6622820409839835 \n",
      "\n",
      "Epsilon: 0.6556592205741436 \n",
      "\n",
      "Epsilon: 0.6491026283684022 \n",
      "\n",
      "Epsilon: 0.6426116020847181 \n",
      "\n",
      "Epsilon: 0.6361854860638709 \n",
      "\n",
      "Epsilon: 0.6298236312032323 \n",
      "\n",
      "Epsilon: 0.6235253948912 \n",
      "\n",
      "Epsilon: 0.617290140942288 \n",
      "\n",
      "Epsilon: 0.6111172395328651 \n",
      "\n",
      "Epsilon: 0.6050060671375365 \n",
      "\n",
      "Epsilon: 0.5989560064661611 \n",
      "\n",
      "Epsilon: 0.5929664464014994 \n",
      "\n",
      "Epsilon: 0.5870367819374844 \n",
      "\n",
      "Epsilon: 0.5811664141181095 \n",
      "\n",
      "Epsilon: 0.5753547499769285 \n",
      "\n",
      "Epsilon: 0.5696012024771592 \n",
      "\n",
      "Epsilon: 0.5639051904523876 \n",
      "\n",
      "Epsilon: 0.5582661385478638 \n",
      "\n",
      "Epsilon: 0.5526834771623851 \n",
      "\n",
      "Epsilon: 0.5471566423907612 \n",
      "\n",
      "Epsilon: 0.5416850759668536 \n",
      "\n",
      "Epsilon: 0.536268225207185 \n",
      "\n",
      "Epsilon: 0.5309055429551132 \n",
      "\n",
      "Epsilon: 0.525596487525562 \n",
      "\n",
      "Epsilon: 0.5203405226503064 \n",
      "\n",
      "Epsilon: 0.5151371174238033 \n",
      "\n",
      "Epsilon: 0.5099857462495653 \n",
      "\n",
      "Epsilon: 0.5048858887870696 \n",
      "\n",
      "Epsilon: 0.4998370298991989 \n",
      "\n",
      "Epsilon: 0.49483865960020695 \n",
      "\n",
      "Epsilon: 0.4898902730042049 \n",
      "\n",
      "Epsilon: 0.48499137027416284 \n",
      "\n",
      "Epsilon: 0.4801414565714212 \n",
      "\n",
      "Epsilon: 0.475340042005707 \n",
      "\n",
      "Epsilon: 0.47058664158564995 \n",
      "\n",
      "Epsilon: 0.4658807751697934 \n",
      "\n",
      "Epsilon: 0.4612219674180955 \n",
      "\n",
      "Epsilon: 0.45660974774391455 \n",
      "\n",
      "Epsilon: 0.4520436502664754 \n",
      "\n",
      "Epsilon: 0.44752321376381066 \n",
      "\n",
      "Epsilon: 0.44304798162617254 \n",
      "\n",
      "Epsilon: 0.4386175018099108 \n",
      "\n",
      "Epsilon: 0.4342313267918117 \n",
      "\n",
      "Epsilon: 0.4298890135238936 \n",
      "\n",
      "Epsilon: 0.42559012338865465 \n",
      "\n",
      "Epsilon: 0.4213342221547681 \n",
      "\n",
      "Epsilon: 0.41712087993322045 \n",
      "\n",
      "Epsilon: 0.41294967113388825 \n",
      "\n",
      "Epsilon: 0.40882017442254937 \n",
      "\n",
      "Epsilon: 0.4047319726783239 \n",
      "\n",
      "Epsilon: 0.40068465295154065 \n",
      "\n",
      "Epsilon: 0.39667780642202527 \n",
      "\n",
      "Epsilon: 0.392711028357805 \n",
      "\n",
      "Epsilon: 0.38878391807422696 \n",
      "\n",
      "Epsilon: 0.3848960788934847 \n",
      "\n",
      "Epsilon: 0.38104711810454983 \n",
      "\n",
      "Epsilon: 0.37723664692350434 \n",
      "\n",
      "Epsilon: 0.37346428045426927 \n",
      "\n",
      "Epsilon: 0.36972963764972655 \n",
      "\n",
      "Epsilon: 0.36603234127322926 \n",
      "\n",
      "Epsilon: 0.36237201786049694 \n",
      "\n",
      "Training complete\n",
      "Optimal Values: [-2.36020785 -2.53339958 -1.91989074 -1.93092142 -3.0063646   0.\n",
      " -2.31468843  0.         -3.07111023 -1.92775364 -0.58642456  0.\n",
      "  0.         -0.61117018  2.27882993  0.        ]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:24.891901Z",
     "start_time": "2025-02-20T13:17:24.888843Z"
    }
   },
   "cell_type": "code",
   "source": "agent.get_optimal_policy()",
   "id": "6b88f5c548dc1ede",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:25.922869Z",
     "start_time": "2025-02-20T13:17:25.086034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environment = gym.make('FrozenLake-v1', render_mode='human')\n",
    "environment.reset()\n",
    "\n",
    "agent.env = environment"
   ],
   "id": "2436991b7dc3b78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 21:17:25.684 python[22388:18432467] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-20 21:17:25.684 python[22388:18432467] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:38.596349Z",
     "start_time": "2025-02-20T13:17:25.980762Z"
    }
   },
   "cell_type": "code",
   "source": "agent.visualize_policy(delay=0.005)",
   "id": "dd09fd83bf3e36c1",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

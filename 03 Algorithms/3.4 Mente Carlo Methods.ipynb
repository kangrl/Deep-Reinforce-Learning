{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Mente Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mente Carlo Methods\n",
    "在Policy Iteration计算过程有两个关键步骤：1. Policy Evaluation，2. Policy Improvement。其中Policy Evaluation需要通过迭代求解Bellman Expectation Equation来实现，这个过程的计算量非常大，尤其是当状态空间很大的时候。特别地，在计算状态价值$v_{\\pi_k}(s)$的过程中，是通过计算状态$s$下所有可能的动作$a$的价值期望值来实现的，即：\n",
    "$$v_{\\pi_k}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} p(s'|s, a)[r(s, a, s') + \\gamma v_{\\pi_k}(s')]$$\n",
    "这个公式的计算量非常大，尤其是当状态空间很大的时候。\n",
    "\n",
    "Mente Carlo Methods从动作价值的定义出发（动作价值是从当前状态出发能够获得的期望回报），通过采样大量的episode来估计$v_{\\pi_k}(s)$的值，从而避免了复杂的矩阵运算，称为**Model-free近似**：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi_k} (s, a) &= \\mathbb{E} \\left[ G_t | S_t = s, A_t = a \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s, A_t = a \\right] \\\\\n",
    "&\\approx \\frac{1}{N} \\sum_{i=1}^N g_{\\pi_k}^i(s, a) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$g_{\\pi_k}^i(s, a)$是从状态$s$出发，经过动作$a$后，按照策略$\\pi_k$所得到的总回报，$N$是采样的episode数量。\n",
    "\n",
    "具体来说，在Policy Evaluation的过程中，我们只需要知道当前策略$\\pi$下从状态$s$出发，经历一系列的状态转移到达终止状态所得到的回报之和即可。这个过程不需要考虑状态之间的转移概率和即时奖励，只需要采样足够多的episode，然后计算每个状态的平均回报值即可。\n",
    "\n",
    "**动作价值计算流程:**\n",
    "1. 使用第$k$步的策略$\\pi_k$采样若干条轨迹（episodes），每条轨迹包含多个状态和动作：\n",
    "   $$s_0^{(i)} \\xrightarrow{a_0^{(i)}} s_1^{(i)} \\xrightarrow{a_1^{(i)}} s_2^{(i)} \\xrightarrow{a_2^{(i)}} \\dots s_{T-1}^{(i)} \\xrightarrow{a_{T-1}^{(i)}} s_T^{(i)}$$\n",
    "\n",
    "    $$s_0^{(i)} \\xrightarrow{a_0^{(i)}} r_0^{(i)}, s_1^{(i)} \\xrightarrow{a_1^{(i)}} r_1^{(i)}, s_2^{(i)} \\xrightarrow{a_2^{(i)}} r_2^{(i)}, \\dots s_{T-1}^{(i)} \\xrightarrow{a_{T-1}^{(i)}} r_{T-1}^{(i)}, s_T^{(i)}$$\n",
    "    其中$s_t^{(i)}$表示第$i$条轨迹在第$t$步的状态，$a_t^{(i)}$表示第$i$条轨迹在第$t$步的动作，$r_t^{(i)}$表示第$i$条轨迹在第$t$步的奖励。\n",
    "\n",
    "2. 计算每条轨迹的回报：\n",
    "   $$G_t^{(i)} = r_t^{(i)} + \\gamma r_{t+1}^{(i)} + \\gamma^2 r_{t+2}^{(i)} + \\dots + \\gamma^{T-t} r_{T}^{(i)}$$\n",
    "   其中$\\gamma$是折扣因子。\n",
    "3. 计算每条轨迹的平均回报：\n",
    "   $$\\bar{G}_t = \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)}$$\n",
    "   其中$N$是轨迹的数量。\n",
    "   这里通常使用增量法进行计算，即：\n",
    "   $$ N(s) \\leftarrow N(s) + 1 $$\n",
    "\n",
    "   $$ \\bar{G}(s) \\leftarrow \\bar{G}(s) + \\frac{1}{N(s)} (G_t^{(i)} - \\bar{G}(s)) $$\n",
    "   这样可以避免存储所有的回报值，节省内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mento Carlo Methods Algroithm\n",
    "* 随机初始化$\\pi_0$\n",
    "* $for\\ k = 0, 1, 2, ... \\ do:$\n",
    "* $\\qquad$ $for \\ s \\in \\cal S \\ do:$\n",
    "* $\\qquad\\qquad$ $for \\ a \\in \\cal A(s) \\ do:$\n",
    "* $\\qquad\\qquad\\qquad$ 遵循策略$\\pi_k$，生成从$(s, a)$出发的轨迹，直到终止状态;\n",
    "* $\\qquad\\qquad\\qquad$ $Policy Evaluation:$\n",
    "* $\\qquad\\qquad\\qquad$ 对于轨迹中的每一个状态-动作对$(s, a)$，计算$G_t^{(i)}$，并更新$N(s) \\leftarrow N(s) + 1$和$\\bar{G}(s) \\leftarrow \\bar{G}(s) + \\frac{1}{N(s)} (G_t^{(i)} - \\bar{G}(s))$\n",
    "* $\\qquad\\qquad$ $end \\ for$\n",
    "* $\\qquad\\qquad$ $Policy \\ Improvement:$\n",
    "* $\\qquad\\qquad$ $a_k^*(s) \\leftarrow \\arg\\max_a \\bar{G}(s)$\n",
    "* $\\qquad\\qquad$ $\\pi_{k+1}(a|s) = 1 \\ if \\ a_k^*(s)=a, \\ else \\ 0$\n",
    "* $\\qquad\\qquad$ 如果$\\pi_{k+1} = \\pi_k$，则停止; 否则继续。\n",
    "* $\\qquad$ $end \\ for$\n",
    "* $end \\ for$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:45:02.548301Z",
     "start_time": "2025-02-11T13:45:02.447572Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:45:02.567277Z",
     "start_time": "2025-02-11T13:45:02.559676Z"
    }
   },
   "source": [
    "class MentoCarlo:\n",
    "    \"\"\" Monte Carlo Algorithm for Freezing-Lake environment \"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma=0.95, num_episodes=500, epsilon=0.2):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.values = np.zeros(self.env.observation_space.n)\n",
    "        self.policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_rewards(done, reward):\n",
    "        \"\"\" Custom Rewards for GridWorld \"\"\"\n",
    "        if done and reward == 0.:\n",
    "            return -1\n",
    "        elif done and reward == 1.:\n",
    "            return 10\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def generate_episode(self):\n",
    "        \"\"\" Generate an episode using the current policy \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        episode = []\n",
    "        while not done:\n",
    "            action = np.random.choice(range(self.env.action_space.n), p=self.policy[state])\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            reward = self.custom_rewards(done, reward)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Run the Monte Carlo method to estimate the value function of a given policy \"\"\"\n",
    "        for episode_idx in range(self.num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "            visited_states = set()\n",
    "            returns = 0\n",
    "            for k in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[k]\n",
    "                returns = reward + self.gamma * returns\n",
    "                if state not in visited_states:\n",
    "                    visited_states.add(state)\n",
    "                    self.values[state] = returns\n",
    "            for state, _, _ in episode:\n",
    "                best_action = np.argmax([self.values[next_state] for next_state, _, _ in episode if next_state == state])\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    best_action = np.random.choice(self.env.action_space.n)\n",
    "\n",
    "                policy = np.zeros(self.env.action_space.n)\n",
    "                policy[best_action] = 1\n",
    "                self.policy[state] = policy / sum(policy)\n",
    "\n",
    "    def visualize_policy(self, delay=0.5):\n",
    "        \"\"\" Visualize the policy in the environment \"\"\"\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.policy[state])\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            time.sleep(delay)\n",
    "        self.env.close()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:45:03.446540Z",
     "start_time": "2025-02-11T13:45:02.632143Z"
    }
   },
   "source": [
    "environment = gym.make('FrozenLake-v1', desc=None, map_name='4x4', is_slippery=True, render_mode='human')\n",
    "environment.reset()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:45:03.204 python[1198:40083161] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-11 21:45:03.204 python[1198:40083161] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T14:15:01.253082Z",
     "start_time": "2025-02-11T13:45:03.494099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = MentoCarlo(environment, gamma=0.9, num_episodes=500, epsilon=0.5)\n",
    "agent.run()\n",
    "print(f\"Optimal Policy: {agent.policy}\")\n",
    "print(f\"Optimal Value Function: {agent.values}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [[1.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "Optimal Value Function: [-0.6561   -0.81     -0.9      -0.9      -0.9       0.       -1.\n",
      "  0.       -1.       -0.81     -0.59049   0.        0.       -0.729\n",
      " -0.531441  0.      ]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T14:15:05.908591Z",
     "start_time": "2025-02-11T14:15:01.271891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environment.reset()\n",
    "agent.visualize_policy(delay=0.005)"
   ],
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

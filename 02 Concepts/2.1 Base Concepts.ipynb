{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Base Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Grid Word Example\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/grid-word-example.png\" alt=\"Grid Word Example\">\n",
    "</center>\n",
    "\n",
    "如图所示：\n",
    "- 在一个网格世界中有一个可以移动的机器人（也被称为智能体）；\n",
    "- 每个时刻智能体只能占据一个单元格；\n",
    "- 智能体可以移动到相邻的单元格，但一次只能移动一个单元格；\n",
    "- 白色单元格可以进入、橙色单元格是禁止进入的；\n",
    "- 网格世界中有一个目标单元格，智能体的移动目标就是目标单元格。\n",
    "\n",
    "智能体的目标：\n",
    "- 智能体的最终目标是通过不断的试错与环境交互找到一个“好”策略，是的其从任何单元格出发都能到达目标单元格；\n",
    "- “好”策略指的是智能体在未进入任何禁止单元格、未采取不必要绕行以及未与网格世界边界发生碰撞的情况下到达目标单元格；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### state\n",
    "**状态：** 用于描述智能体相对于环境的状态。\n",
    "\n",
    "在网格世界的例子中，状态对应于代理的位置；由于网格世界例子中是一个3x3的网格，因此有9个状态，分别用$s_1, s_2, \\cdots, s_9$来表示。\n",
    "\n",
    "**状态空间：** 所有状态的集合称为状态空间。\n",
    "\n",
    "在网格世界的例子中，状态空间可以表示为：$\\cal S = \\{s_1, s_2, \\cdots, s_9\\}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "**动作：** 智能体可能采取的行动。\n",
    "\n",
    "在网格世界的例子中，智能体在美中状态下可能采取的动作有——向上移动、向右移动、向下移动、向左移动和保持静止，这五种动作分别用$a_1, a_2, \\cdots, a_5$来表示。\n",
    "\n",
    "**动作空间：** 所有可能动作的集合。\n",
    "\n",
    "在网格世界的例子中，动作空间可以表示为：$\\cal A = \\{a_1, a_2, \\cdots, a_5\\}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../Images/states-actions.png\" alt=\"states and Actions\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition\n",
    "**状态转移：** 智能体从一个状态移动到另一个状态的过程称为状态转移。\n",
    "\n",
    "在网格世界的例子中，如果智能体处于$s_1$并选择动作$a_2$（即向右移动），那么智能体将移动到状态$s_2$，即表示智能体从状态$s_1$转移到状态$s_2$，可以表示为$s_1 \\xrightarrow{a_2} s_2$；当智能体选择动作$a_1$（即向上移动），那么智能体将被弹回，因为智能体无法离开状态空间，这一状态转移可以表示为：$s_1 \\xrightarrow{a_1} s_1$。\n",
    "\n",
    "**在状态空间和动作空间有限的情况下，状态转移可以用表格表示：** 每一行表示一个状态，每一列表示一个动作，每一个行列单元表示在行所在的单元采取列表示的动作下一步能够到达的状态，即从行状态出发采取列动作一次状态转移到达的状态。\n",
    "\n",
    "|     | $a_1$ (upward) | $a_2$ (rightward) | $a_3$ (downward) | $a_4$ (leftward) | $a_5$ (still) |\n",
    "|-----|------------------|--------------------|--------------------|--------------------|----------------|\n",
    "| $s_1$ | $s_1$         | $s_2$            | $s_4$            | $s_1$            | $s_1$        |\n",
    "| $s_2$ | $s_2$         | $s_3$            | $s_5$            | $s_1$            | $s_2$        |\n",
    "| $s_3$ | $s_3$         | $s_3$            | $s_6$            | $s_2$            | $s_3$        |\n",
    "| $s_4$ | $s_1$         | $s_5$            | $s_7$            | $s_4$            | $s_4$        |\n",
    "| $s_5$ | $s_2$         | $s_6$            | $s_8$            | $s_4$            | $s_5$        |\n",
    "| $s_6$ | $s_3$         | $s_6$            | $s_9$            | $s_5$            | $s_6$        |\n",
    "| $s_7$ | $s_4$         | $s_8$            | $s_7$            | $s_7$            | $s_7$        |\n",
    "| $s_8$ | $s_5$         | $s_9$            | $s_8$            | $s_7$            | $s_8$        |\n",
    "| $s_9$ | $s_6$         | $s_9$            | $s_9$            | $s_8$            | $s_9$        |\n",
    "\n",
    "尽管表格表示在描述转台转移上更加直观，但是对于具有随机性的状态转移需要更加一般性的条件概率分布来进行描述，如：用$p(s_2|s_1, a_2)$定义智能体在状态$s_1$下执行动作$a_2$后转移到状态$s_2$的概率。\n",
    "\n",
    "**状态转移函数：** 更一般地，状态转移函数$T(s, a, s')$定义了智能体在状态$s$下执行动作$a$后转移到状态$s'$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "**策略（Policy）：** 策略定义了智能体在给定状态下选择动作的规则，即从状态到动作的映射。策略可以是确定性的，也可以是随机的。确定性策略$\\pi(s)$直接指定智能体在状态$s$下应该执行的动作$a$，而随机策略$\\pi(a|s)$则指定智能体在状态$s$下选择动作$a$的概率。\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/a-deterministic-policy.png\" alt=\"A Deterministic Policy\">\n",
    "</center>\n",
    "\n",
    "**最优策略（Optimal Policy）：** 最优策略$\\pi^*$是使得智能体在长期内获得最大累积奖励的策略。\n",
    "\n",
    "**策略优化（Policy Optimization）：** 策略优化是指通过调整策略参数，使得智能体在环境中获得更高的累积奖励。\n",
    "\n",
    "|    | $a_1$ (upward) | $a_2$ (rightward) | $a_3$ (downward) | $a_4$ (leftward) | $a_5$ (still) |\n",
    "|----|------------------|--------------------|--------------------|--------------------|----------------|\n",
    "| $s_1$ | 0                | 0.5                | 0.5                | 0                  | 0              |\n",
    "| $s_2$ | 0                | 0                  | 1                  | 0                  | 0              |\n",
    "| $s_3$ | 0                | 0                  | 0                  | 1                  | 0              |\n",
    "| $s_4$ | 0                | 1                  | 0                  | 0                  | 0              |\n",
    "| $s_5$ | 0                | 0                  | 1                  | 0                  | 0              |\n",
    "| $s_6$ | 0                | 0                  | 1                  | 0                  | 0              |\n",
    "| $s_7$ | 0                | 1                  | 0                  | 0                  | 0              |\n",
    "| $s_8$ | 0                | 1                  | 0                  | 0                  | 0              |\n",
    "| $s_9$ | 0                | 0                  | 0                  | 0                  | 1              |\n",
    "\n",
    "表格法非常直观，第$ij$位置表示，在第$i$个状态采取动作$j$的概率，可以既可以表示确定性策略，也可以表示随机性策略。\n",
    "\n",
    "**策略函数：** 更一般地，策略函数$\\pi(a|s)$定义了在状态$s$下选择动作$a$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "**奖励（Reward）：** 智能体在某个状态下执行某个动作后能够从环境中获得反馈，记为$r$，与状态$s$和动作$a$有关，又可以记作$r(s, a)$。通常奖励是一个正、负或0的标量。\n",
    "\n",
    "不同的奖励对最终学习到的策略有不同的影响，因此在强化学习中，设计合适的奖励是一个重要的步骤。不能仅仅基于即时奖励来选择行动，需要综合考虑即使奖励和长期奖励来制定有效的策略。\n",
    "\n",
    "|     | a<sub>1</sub> (upward) | a<sub>2</sub> (rightward) | a<sub>3</sub> (downward) | a<sub>4</sub> (leftward) | a<sub>5</sub> (still) |\n",
    "|-----|------------------------|--------------------------|-------------------------|-------------------------|----------------------|\n",
    "| s<sub>1</sub> | r<sub>boundary</sub>   | 0                        | 0                       | r<sub>boundary</sub>    | 0                    |\n",
    "| s<sub>2</sub> | r<sub>boundary</sub>   | 0                        | 0                       | 0                       | 0                    |\n",
    "| s<sub>3</sub> | r<sub>boundary</sub>   | r<sub>boundary</sub>     | r<sub>forbidden</sub>   | 0                       | 0                    |\n",
    "| s<sub>4</sub> | 0                      | 0                        | r<sub>forbidden</sub>   | r<sub>boundary</sub>    | 0                    |\n",
    "| s<sub>5</sub> | 0                      | r<sub>forbidden</sub>    | 0                       | 0                       | 0                    |\n",
    "| s<sub>6</sub> | 0                      | r<sub>boundary</sub>     | r<sub>target</sub>      | 0                       | r<sub>forbidden</sub>|\n",
    "| s<sub>7</sub> | 0                      | 0                        | r<sub>boundary</sub>    | r<sub>boundary</sub>    | r<sub>forbidden</sub>|\n",
    "| s<sub>8</sub> | 0                      | r<sub>target</sub>       | r<sub>boundary</sub>    | r<sub>forbidden</sub>   | 0                    |\n",
    "| s<sub>9</sub> | r<sub>forbidden</sub>  | r<sub>boundary</sub>     | r<sub>boundary</sub>    | 0                       | r<sub>target</sub>   |\n",
    "\n",
    "使用表格非常直观，第$ij$单元格表示在第$i$状态采取动作$j$能够获得的奖励。**值得注意的是：** 表格表示法智能描述确定性奖励过程。\n",
    "\n",
    "**更一般地：** 可以使用条件概率$p(r|s,a)$描述奖励过程——在状态$s$执行动作$a$后，获得奖励$r$的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory\n",
    "**遵循策略，智能体可以从初始状态生成一个状态序列——轨迹（trajectory）——状态-动作-奖励链。**\n",
    "<center>\n",
    "<img src=\"../Images/trajectories-obtained-from-the-policy.png\" alt=\"Trajectories Obtained from the Policy\">\n",
    "</center>\n",
    "\n",
    "如图所示，图中的三条轨迹分别是：\n",
    "$$s_1 \\xrightarrow[r=0]{a_2} s_2 \\xrightarrow[r=0]{a_3} s_5 \\xrightarrow[r=0]{a_3} s_8 \\xrightarrow[r=+1]{a_2} s_9 \\ \\ \\ \\ (轨迹1)$$\n",
    "$$s_1 \\xrightarrow[r=0]{a_3} s_4 \\xrightarrow[r=-1]{a_3} s_7 \\xrightarrow[r=0]{a_2} s_8 \\xrightarrow[r=+1]{a_2} s_9 \\ \\ \\ \\ (轨迹2)$$\n",
    "$$s_7 \\xrightarrow[r=0]{a_2} s_8 \\xrightarrow[r=1]{a_2} s_9 \\ \\ \\ \\ (轨迹3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns\n",
    "**回报（Returns）：** 轨迹上所有奖励的和被称为这条轨迹的回报，又被称为总奖励(Total Rewards)或累积奖励(Cumulative Rewards)。回报被用来评估策略的好坏，通常我们希望最大化期望回报。\n",
    "\n",
    "根据回报定义，上面三条轨迹的回报分别为：\n",
    "- $G_1 = 0 + 0 + 0 + (+1) = 1$\n",
    "- $G_2 = 0 + (-1) + 0 + (+1) = 0$\n",
    "- $G_3 = 0 + (+1) = 1$\n",
    "\n",
    "回报由即时奖励和未来奖励组成。**即时奖励（Immediate Reward）** 是指智能体在初始状态采取行动后立即获得的奖励，而 **未来奖励（Future Rewards）** 则是指从离开初始状态后到轨迹结束为止所有时间步的奖励之和。即时奖励为负而未来奖励为正的情况是可能的，繁殖即时奖励为正而未来奖励为负的情况也是可能的。因此，根据回报(即总奖励)而不是即时奖励来决定采取哪些行动是合理的。\n",
    "\n",
    "**折扣回报(Discounted Returns)：** 在强化学习中，折扣回报（Discounted Return）是一个重要的概念，用于评估智能体在一个序列中的累积奖励。它考虑了时间衰减的因素，使得未来的奖励对当前决策的影响随着时间的推移而减少。\n",
    "具体来说，对于一个长度为 $ T $ 的时间步序列，每个时间步的即时奖励为 $ r_t $，折扣因子为 $ \\gamma $（通常在0到1之间），折扣回报 $ G_t $ 定义为：\n",
    "$$ G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots + \\gamma^{T-t} r_T $$\n",
    "这个公式表示当前时间步的奖励 $ r_t $ 以及未来所有时间步的奖励，每个未来的奖励都要乘以一个折扣因子 $ \\gamma $ 的幂次。\n",
    "\n",
    "**引入折扣率的原因：**\n",
    "- 摒弃终止条件，允许无限长的轨迹：在没有折扣率的情况下，如果环境没有明确的终止状态，那么回报将会是无穷大。通过引入折扣率，我们可以确保回报是有限的；引入折扣率后，随着时间步的增加，未来的奖励会指数级减少，最终使得整个求和过程收敛，这样就可以处理无限长时间范围内的决策问题；\n",
    "- 平衡即时奖励与长期奖励：通过引入折扣率，我们可以权衡当前的奖励和未来的奖励，使智能体能够在两者之间找到一个平衡点；当$\\gamma = 0$时，智能体只考虑当前的奖励，而当$\\gamma = 1$时，智能体会将所有未来奖励等同看待；\n",
    "- 平衡策略的短视与长远规划：在实际应用中，智能体的决策往往需要兼顾短期利益和长期发展。折扣率的存在使得智能体能够在两者之间找到一个平衡点，避免过于短视或过于远视的决策；短视策略可能在一些快速变化的环境中表现得更好，而长远规划则可能在稳定环境中更为有效；远见的策略在需要长期规划和风险承担的情况下更为合适，比如在投资和战略规划中；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode\n",
    "**回合(Episode)：** 表示智能体从初始状态开始，通过与环境互动，知道达到终止状态所经历的一个完整序列——一个回合通常被认为是一条有限的轨迹——智能体与环境交互产生一条有限轨迹的过程被称为一个回合。\n",
    "\n",
    "如果环境或策略是随机的，那么不同的回合可能会导致不同的结果。例如，在某些游戏中，玩家可能通过不同的决策路径达到相同的最终状态，从而拥有不同的回合长度。如果环境和策略不是随机的，那么每个智能体都会遵循固定的规则和策略，因此每个回合的长度将是确定的。\n",
    "\n",
    "**回合制任务(Episode Tasks)：** 有明确的开始和结束标志的任务可以被视为回合制任务。例如，在某些游戏中，一个完整的游戏过程（从开始到结束）可以被视为一个阶段。在这种情况下，每个阶段代表了一个完整的游戏过程。\n",
    "\n",
    "**持续性任务(Continuing Tasks):** 没有明确开始和结束标志的任务通常被称为持续性任务。这些任务会在一段时间内持续进行，例如日常任务或每周任务。\n",
    "\n",
    "事实上，可以使用统一的数学架构将回合制任务转换为持续性任务——当智能体在一个回合中到达终止状态时，重置环境并重新启动一个新任务。这样，原本的回合制任务就变成了持续性任务。\n",
    "- 将终止状态定义为“吸收状态(Absorbing State)”，即一旦到达该状态就无法离开，从而确保任务是持续性的。例如在网格世界中，当智能体抵达$s_9$，定义智能体智能执行动作$a_5$，即$\\cal A(s_9)=\\{a_5\\}$，这样智能体就无法离开该状态；或定义$\\cal A(s_9)={a_1, \\cdots, a_5}$，且$p(s_9|s_9, a_i)$， $i=1, \\cdots, 5$。\n",
    "- 将奖励函数重新定义为仅在吸收状态下给予奖励，而在其他所有状态下不给予奖励。例如在网格世界中，只有当智能体到达$s_9$时才给予奖励$R(s_9)=100$，而在其他任何状态下都没有奖励$R(s) = 0 \\text{ for all } s \\neq s_9$。这样智能体最终会学会永远停留在$s_9$以获得更多的奖励。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

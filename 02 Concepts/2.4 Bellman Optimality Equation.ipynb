{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Example\n",
    "<center>\n",
    "<img src=\"../Images/demonstrating-policy-improvement.png\" alt=\"An Example for Demonstrating Policy Improvement\">\n",
    "</center>\n",
    "\n",
    "如图所示：当前策略不是一个“好”的策略，因为它在状态$s_1$时选择了动作$a_2$而不是$a_3$。\n",
    "\n",
    "**如何对当前策略进行改进？**\n",
    "\n",
    "首先，我们根据计算各个状态的价值：\n",
    "$$\n",
    "v_{\\pi}(s_1) = -1 + \\gamma v_{\\pi}(s_2) = -1 + \\frac{0.9}{1-0.9} = 8 \\\\\n",
    "v_{\\pi}(s_2) = +1 + \\gamma v_{\\pi}(s_4) = +1 + \\frac{0.9}{1-0.9} = 10 \\\\\n",
    "v_{\\pi}(s_3) = +1 + \\gamma v_{\\pi}(s_4) = +1 + \\frac{0.9}{1-0.9} = 10\\\\\n",
    "v_{\\pi}(s_4) = +1 + \\gamma v_{\\pi}(s_4) = +1 + \\frac{0.9}{1-0.9} = 10\\\\\n",
    "$$\n",
    "\n",
    "接着计算状态$s_1$可能采取的动作对应的动作价值：\n",
    "$$\n",
    "q_{\\pi}(s_1, a_1) = -1 + \\gamma v_{\\pi}(s_1) = -1 + \\gamma 8 = 6.2 \\\\\n",
    "q_{\\pi}(s_1, a_2) = -1 + \\gamma v_{\\pi}(s_2) = -1 + \\gamma 10 = 8 \\\\\n",
    "q_{\\pi}(s_1, a_3) = -1 + \\gamma v_{\\pi}(s_3) = 0 + \\gamma 10 = 9 \\\\\n",
    "q_{\\pi}(s_1, a_4) = -1 + \\gamma v_{\\pi}(s_4) = -1 + \\gamma 10 = 8 \\\\\n",
    "q_{\\pi}(s_1, a_5) = -1 + \\gamma v_{\\pi}(s_1) = 0 + \\gamma 8 = 7.2 \\\\\n",
    "$$\n",
    "\n",
    "**Policy Improvement:** 因为$q_{\\pi}(s_1, a_3) \\geq q_{\\pi}(s_1, a_i), \\ \\ i \\neq 3$,故动作$a_3$更好，因此策略更新在状态$s_1$时选择动作$a_3$。\n",
    "\n",
    "**这个示例过程是许多强化学习算法的核心思想：先计算各个状态的价值，并根据价值计算各个状态可能采取的动作对应的动作价值，选择动作价值最大的动作来更新各状态下的策略。**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 State te Values & Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Example 1\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/motivating-example.png\" alt=\"Motivating Example\">\n",
    "</center>\n",
    "\n",
    "如图所示：三个不同策略下，从$s_1$出发获得的回报不同。\n",
    "- 遵循最左侧策略，得到的轨迹是 $s_1 \\rightarrow s_3 \\rightarrow s_4 \\rightarrow s_4 \\cdots$，最终的回报如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_1(s_1) & = p(r=0|s_1, a_3)\\pi(a_3|s_1)R(s_1, a_3) + \\gamma p(r=1|s_3, a_2)\\pi(a_2|s_3)R(s_3, a_2) + \\gamma^2 p(r=1|s_4, a_5)\\pi(a_5|s_4)R(s_4,a_5) + \\cdots \\\\\n",
    "&= R(s_1, a_3) + \\gamma R(s_3, a_2) + \\gamma^2 R(s_4,a_5) + \\cdots\n",
    "&= 0 + \\gamma \\cdot 1 + \\gamma^2 \\cdot 1 + \\cdots \\\\\n",
    "&= \\gamma (1 + \\gamma + \\gamma^2 + \\cdots) \\\\\n",
    "&= \\frac{\\gamma}{1 - \\gamma}\n",
    "\\end{aligned}\n",
    "$$\n",
    "- 遵循中间策略，得到的轨迹是 $s_1 \\rightarrow s_2 \\rightarrow s_4 \\rightarrow s_4 \\cdots$，最终的回报如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_2(s_1) & = R(s_1,a_2) + \\gamma R(s_2,a_3) + \\gamma^2 R(s_4,a_5) + \\cdots \\\\\n",
    "&= -1 + \\gamma \\cdot 1 + \\gamma^2 \\cdot 1 + \\cdots \\\\\n",
    "&= -1 + \\gamma (1 + \\gamma + \\gamma^2 + \\cdots) \\\\\n",
    "&= -1 + \\frac{\\gamma}{1 - \\gamma}\n",
    "\\end{aligned}\n",
    "$$\n",
    "- 遵循最右侧策略，得到可能的两条轨迹：$s_1 \\rightarrow s_3 \\rightarrow s_4 \\rightarrow s_4 \\cdots$ 和 $s_1 \\rightarrow s_2 \\rightarrow s_4 \\rightarrow s_4 \\cdots$。最终的回报如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_3(s_1) & = 0.5 \\times （R(s_1,a_3) + \\gamma R(s_3,a_2) + \\gamma^2 R(s_4,a_5) + \\cdots） + 0.5 \\times （R(s_1,a_2) + \\gamma R(s_2,a_3) + \\gamma^2 R(s_4,a_5) + \\cdots） \\\\\n",
    "&= 0.5 \\times (\\frac{\\gamma}{1 - \\gamma}) + 0.5 \\times （-1 + \\frac{\\gamma}{1 - \\gamma}） \\\\\n",
    "&= -0.5 + \\frac{\\gamma}{1 - \\gamma}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "上面的例子告诉我们：**如果遵循一个策略获得的回报越大，那么这个策略就越好；反之，这个策略就越差。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Example 2\n",
    "<center>\n",
    "<img src=\"../Images/motivating-example-bootstrapping.png\" alt=\"Motivating Example Bootstrapping\">\n",
    "</center>\n",
    "\n",
    "如图所示：在特定策略下，有一个无限轨迹$s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow s_4 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow s_4 ...$\n",
    "\n",
    "则四个状态对应的回报为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v(s_1) &= r_1 + \\gamma r_2 + \\gamma ^ 2 r_3 + \\cdots \\\\\n",
    "v(s_2) &= r_2 + \\gamma r_3 + \\gamma ^ 2 r_4 + \\cdots \\\\\n",
    "v(s_3) &= r_3 + \\gamma r_4 + \\gamma ^ 2 r_1 + \\cdots \\\\\n",
    "v(s_4) &= r_4 + \\gamma r_1 + \\gamma ^ 2 r_2 + \\cdots \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "利用*bootstrapping*思想对上式进行变换：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v(s_1) &= r_1 + \\gamma r_2 + \\gamma ^ 2 r_3 + \\cdots = r_1 + \\gamma (r_2 + \\gamma r_3 + \\cdots) = r_1 +  \\gamma v_2 \\\\\n",
    "v(s_2) &= r_2 + \\gamma r_3 + \\gamma ^ 2 r_4 + \\cdots = r_2 + \\gamma (r_3 + \\gamma r_4 + \\cdots) = r_2 +  \\gamma v_3 \\\\\n",
    "v(s_3) &= r_3 + \\gamma r_4 + \\gamma ^ 2 r_1 + \\cdots = r_3 + \\gamma (r_4 + \\gamma r_1 + \\cdots) = r_3 +  \\gamma v_4 \\\\\n",
    "v(s_4) &= r_4 + \\gamma r_1 + \\gamma ^ 2 r_2 + \\cdots = r_4 + \\gamma (r_1 + \\gamma r_2 + \\cdots) = r_4 +  \\gamma v_1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们可以看到，状态价值函数 $v(s_t)$ 依赖于后续状态的价值函数 $v(s_{t+1})$，这就是**bootstrapping**的体现。通过这种递归的方式，我们可以逐步推导出每个状态的价值。\n",
    "\n",
    "**Matrix-Vector形式：**\n",
    "\\begin{align*}\n",
    "& \\left[ \\begin{array}{c}\n",
    "v_1 \\\\ \n",
    "v_2 \\\\ \n",
    "v_3 \\\\ \n",
    "v_4 \n",
    "\\end{array} \\right] = \\left[ \\begin{array}{c}\n",
    "r_1 \\\\ \n",
    "r_2 \\\\ \n",
    "r_3 \\\\ \n",
    "r_4 \n",
    "\\end{array} \\right] + \\gamma \\left[ \\begin{array}{c}\n",
    "v_2 \\\\ \n",
    "v_3 \\\\ \n",
    "v_4 \\\\ \n",
    "v_1 \n",
    "\\end{array} \\right] = \\left[ \\begin{array}{c}\n",
    "r_1 \\\\ \n",
    "r_2 \\\\ \n",
    "r_3 \\\\ \n",
    "r_4 \n",
    "\\end{array} \\right] + \\gamma \\left( \\left[ \\begin{array}{cccc}\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array} \\right] \\left[ \\begin{array}{c}\n",
    "v_1 \\\\ \n",
    "v_2 \\\\ \n",
    "v_3 \\\\ \n",
    "v_4 \n",
    "\\end{array} \\right] \\right)\n",
    "\\end{align*}\n",
    "\n",
    "进一步地：\n",
    "$$\n",
    "v = r + \\gamma P v\n",
    "$$\n",
    "\n",
    "$v$的显式解：\n",
    "$$\n",
    "v = (I - \\gamma P)^{-1} r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Values\n",
    "**状态价值(State Value)：** 是指智能体遵循给定策略在某个状态下能够获得的平均奖励，通常用 $V(s)$ 表示。它是强化学习中一个重要的概念，用于评估策略的好坏程度。\n",
    "\n",
    "定义一个马尔可夫决策过程：\n",
    "$$\n",
    "S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1}\n",
    "$$\n",
    "其中 $S_t$ 是时间步 $t$ 的状态，$A_t$ 是在状态 $S_t$ 下采取的动作，$R_{t+1}$ 是执行动作 $A_t$ 后获得的奖励，$S_{t+1}$ 是下一个状态；$S_t, S_{t+1} \\in \\mathcal{S}$，$\\mathcal{S}$ 是所有可能的状态集合，$A_t \\in \\mathcal{A}(S_t)$，$\\mathcal{A}(S_t)$ 是在状态 $S_t$ 下可以采取的所有动作的集合, $R_{t+1} \\in \\mathcal{R}(S_t, A_t)$ 是奖励函数。\n",
    "\n",
    "从$t$时刻开始，得到一个$state-action-reward$轨迹：\n",
    "$$\n",
    "S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}...\n",
    "$$\n",
    "\n",
    "对应的，这一轨迹对应回报如下：\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "$$\n",
    "其中$\\gamma \\in (0, 1)$是折扣因子，$G_t$是一个随机变量。\n",
    "\n",
    "根据状态价值的定义有：\n",
    "$$\n",
    "v_{\\pi}(s) = E_{\\pi}[G_t | S_t = s]\n",
    "$$\n",
    "其中$E_{\\pi}$表示在策略$\\pi$下的期望值。\n",
    "\n",
    "称$v_{\\pi}(s)$为状态价值函数或关于状态$s$的状态价值，它给出了在策略$\\pi$下从状态$s$出发的期望回报。\n",
    "\n",
    "**值得强调的是：**\n",
    "- 状态价值函数$v_{\\pi}(s)$依赖状态$s$和策略$\\pi$，因为其定义是从状态$s$出发，按照策略$\\pi$进行决策（遵循策略生成轨迹）得到的期望回报；\n",
    "- 状态价值函数$v_{\\pi}(s)$不依赖时间$t$，因为$t$是当前时间步，一旦策略给定，状态确定，则其后的所有时刻的奖励都是确定的；\n",
    "- 当策略和环境都是确定的时候，从某个状态开始总是会导致相同的轨迹，因此从某个状态开始的回报等于该状态的价值；相反，当策略或环境是随机的时候，从同一个状态开始可能会产生不同的轨迹，因此该状态价值是这些轨迹回报的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Values\n",
    "\n",
    "动作价值函数$q_{\\pi}(s, a)$表示在状态$s$下采取动作$a$后按照策略$\\pi$所带来的期望回报：\n",
    "$$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a]$$\n",
    "当策略和环境都是确定的时候，从某个状态开始总是会导致相同的轨迹，因此从某个状态开始的动作价值等于该动作的价值；相反，当策略或环境是随机的时候，从同一个状态开始可能会产生不同的轨迹，因此该动作价值是这些轨迹回报的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Values and Action Values\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]= \\sum_{a \\in A}\\mathbb{E}[G_t|S_t=s,A_t=a] \\cdot \\pi(a|s)\n",
    "$$\n",
    "\n",
    "进一步地，可以整理为：\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) q_{\\pi}(s, a) \\ \\ \\ \\ \\text{*}\n",
    "$$\n",
    "\n",
    "即状态价值是该状态下所有可能的动作价值函数乘以采取该动作的概率的总和。\n",
    "\n",
    "由于当前状态价值依赖后续状态价值，因此可以用Bootstrap的方法来表示状态价值：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\sum_{a \\in \\cal A} \\pi(a|s) \\sum_{r \\in \\cal R} p(r|s, a) r + \\gamma \\sum_{a \\in  \\cal A} \\pi(a|s) \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi}(s') \\\\\n",
    "&= \\sum_{a \\in \\cal A} \\pi(a|s) \\Big [ \\sum_{r \\in \\cal R} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi}(s') \\Big ]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由(*)式可知：\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\sum_{r \\in \\cal R} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi}(s')\n",
    "$$\n",
    "其中$\\sum_{r \\in \\cal R} p(r|s, a) r$为即时奖励均值，$\\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi}(s')$为未来奖励均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative examples\n",
    "\n",
    "<center>\n",
    "<img src=\"../Images/illustrative-examples.png\" alt=\"Illustrative Examples\">\n",
    "</center>\n",
    "\n",
    "如图所示：考虑随机策略的例子，从状态$s_1$出发有0.5的概率采取动作$a_2$转移到状态$s_2$，有0.5的概率采取动作$a_3$转移到状态$s_3$。\n",
    "\n",
    "状态$s_1$下各个动作价值如下：\n",
    "$$\n",
    "q_{\\pi}(s_1, a_2) = -1 + \\gamma v_{\\pi}(s_2) \\\\\n",
    "q_{\\pi}(s_1, a_3) = 0 + \\gamma v_{\\pi}(s_3) \\\\\n",
    "q_{\\pi}(s_1, a_1) = -1 + \\gamma v_{\\pi}(s_1) \\\\\n",
    "q_{\\pi}(s_1, a_4) = -1 + \\gamma v_{\\pi}(s_1) \\\\\n",
    "q_{\\pi}(s_1, a_5) = 0 + \\gamma v_{\\pi}(s_1) \\\\\n",
    "$$\n",
    "\n",
    "根据(*)式，在策略$\\pi$下状态价值如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s_1) &= 0.5 \\times q_{\\pi}(s_1, a_2) + 0.5 \\times q_{\\pi}(s_1, a_3) \\\\\n",
    "&= 0.5 \\times (-1 + \\gamma v_{\\pi}(s_2)) + 0.5 \\times (0 + \\gamma v_{\\pi}(s_3)) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Additional Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based\n",
    "**Model-Based：** 是强化学习中的一个重要概念，指的是在学习过程需要明确知道或假设环境的动态模型（即知道马尔可夫决策过程的状态转移概率和奖励函数），核心在于利用对环境的理解来指导决策过程。\n",
    "\n",
    "**主要特点：** \n",
    "- 需要对环境的行为或结构有先验知识——状态转移概率和奖励函数已知；\n",
    "- 可以利用模型进行规划，减少与真实环境的实验次数，降低了风险和成本；\n",
    "- 通常比较直观，便于理解和分析；\n",
    "\n",
    "**典型算法：**\n",
    "- 动态规划：值迭代、策略迭代、折损迭代；\n",
    "- 模型预测控制（MPC）：线性二次调节器（LQR）、线性二次高斯（LQG）等；\n",
    "- 蒙特卡洛树搜索（MCTS）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free\n",
    "**Model-Free：** 是强化学习中的一个重要概念，指的是在学习过程中不需要明确知道或假设环境的动态模型（即状态转移概率和奖励函数），Model-Free方法直接通过与环境互动来收集经验数据，并给予这些数据学习最优策略；\n",
    "\n",
    "**主要特点：**\n",
    "- 不依赖环境模型：算法不需要对环境的行为或结构有先验知识，通缩试错的方式让智能体直接与环境互动，观察结果，从而推断出最优策略；\n",
    "- 基于经验数据：智能体通过与环境的交互来学习，收集大量的样本轨迹（状态、动作、奖励），然后利用这些数据来更新策略和价值函数，而不是用来估计环境模型；\n",
    "- 灵活性强：在处理复杂或位置的环境中表现得更加灵活，因为它不需要对环境进行建模，也不需要预先定义好所有的规则和奖励函数。\n",
    "\n",
    "**经典算法：**\n",
    "- **Q-learning**: 一种基于值函数的强化学习算法，通过迭代更新Q值来找到最优策略；\n",
    "- **Deep Q-Network (DQN)**: 使用深度神经网络来近似Q值函数的强化学习算法，特别适用于高维状态空间；\n",
    "- **策略梯度方法**: 直接优化策略参数以最大化累积奖励的期望值。\n",
    "- **SARSA**: 也是一种基于值函数的强化学习算法，与Q-learning类似，但在更新Q值时使用的是下一个状态和动作；\n",
    "- **蒙特卡洛方法**: 通过采样大量的完整轨迹来估计价值函数，然后根据这些估计来优化策略；\n",
    "- **时间差分学习 (TD)**: 结合了蒙特卡洛方法和动态规划的方法，通过采样部分轨迹来更新值函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Policy\n",
    "**On-Policy** 方法指的是代理在学习和执行策略时使用的是同一个策略，即用于生成经验数据的策略和被学习的策略是同一个策略；\n",
    "\n",
    "**主要特点：**\n",
    "- **同策略性**: 学习过程中使用的策略与实际执行的策略是相同的；\n",
    "- **探索要求高**: 为了能够充分探索环境并找到最优策略，因为需要确保收集到的数据覆盖了足够多的状态和动作；\n",
    "- **探索与利用**: 可能面临探索与平衡的问题，调参难度比较高；\n",
    "- **收敛速度慢**: 由于需要在同一个策略下进行学习和执行，可能会导致收敛速度较慢。\n",
    "\n",
    "**经典算法：**\n",
    "- **蒙特卡洛同策略（Monte Carlo On-Policy）**: 使用完整的轨迹进行更新，适用于回合任务；\n",
    "- **SARSA (State-Action-Reward-State-Action)**: 一种时序差分控制方法，用于离散动作空间；\n",
    "- **策略梯度算法（Policy Gradient Methods）**: 如REINFORCE、Actor-Critic等，适用于连续和离散动作空间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy\n",
    "**Off-Policy** 方法使用两个不同的策略：一个是目标策略（Target Policy）用于生成数据，另一个是行为策略（Behavior Policy）用于实际执行。这种方法的主要优点是可以从其他策略的经验中学习，从而提高样本效率和探索能力。\n",
    "\n",
    "**主要特点：**\n",
    "- **数据利用率高：** 可以利用其他策略生成的数据，包括来自不同来源或过去的数据；\n",
    "- **更好的探索：** 通过使用不同的策略进行探索，可以在更广泛的行动空间中学习；\n",
    "- **需要处理偏差：** 由于目标策略和行为策略的差异，可能会引入偏差，需要采取措施来减少这种偏差；\n",
    "\n",
    "**经典算法：**\n",
    "- **Q学习（Q-Learning）：** 一种经典的Off-Policy时序差分学习方法，它学习的是最优策略下的动作价值，而不关心数据是如何生成的；\n",
    "- **深度Q网络（Deep Q-Network, DQN）：** 结合了深度学习和Q学习，用于处理高维状态空间，也是Off-Policy的。\n",
    "- **双重DQN（Double DQN）：** DQN的改进版本，减少了估计值的偏差。\n",
    "- **基于重要性采样的策略梯度方法：** 如Off-Policy Actor-Critic，使用重要性采样来校正不同策略之间的偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based\n",
    "**Value-Based** 方法侧重于学习状态值函数（Value Function）或动作值函数（Q-Value Function），即学习在特定状态或状态-动作对下预期的累积奖励。\n",
    "\n",
    "**主要特点：**\n",
    "- **价值学习：** 专注于学习价值函数，通过价值函数来引导策略；\n",
    "- **离散动作空间：** 不容易处理连续动作空间，通常用于离散动作空间；\n",
    "- **数据利用率高：** 通常使用经验回放（Experience Replay）机制，重用之前的状态转移来稳定训练过程；\n",
    "\n",
    "**经典算法：**\n",
    "- **Q-Learning：** 一种基于值的强化学习算法，通过迭代更新Q值函数来进行决策；\n",
    "- **Deep Q Network (DQN)：** 使用深度神经网络近似Q值函数，处理高维状态空间；\n",
    "- **SARSA：** 另一种基于值的算法，与Q-Learning类似，但在更新时使用下一个状态和动作的估计值；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Based\n",
    "**Policy-Based** 方法直接学习策略函数（Policy Function），即学习在给定状态下应采取的最佳动作。\n",
    "\n",
    "**主要特点：**\n",
    "- 直接学习策略（Policy），通常是一个概率分布，表示在每个状态下采取各个动作的概率；\n",
    "- 不需要显式的价值函数，通过优化策略参数来最大化累积奖励；\n",
    "- 可能收敛到局部最优解，而不是全局最优解；\n",
    "- 可以处理连续动作空间，因为直接学习动作；\n",
    "- 可能需要更多的探索，以确保覆盖足够的状态和动作。\n",
    "\n",
    "**经典算法：**\n",
    "- **Policy Gradient Methods：** 通过梯度上升更新策略参数以最大化累积奖励；\n",
    "- **Actor-Critic：** 结合了策略和值函数的优势，使用一个actor网络来表示策略，一个critic网络来评估状态或动作的价值。\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "\n",
    "**贝尔曼方程(Bellman Equation)：** 用于描述所有状态价值之间关系的一系列线性方程式。\n",
    "\n",
    "状态价值函数可以表示如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb E_{\\pi}[G_t | S_t = s] \\\\\n",
    "&= \\mathbb E_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots| S_t = s] \\\\\n",
    "&= \\mathbb E_{\\pi}[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots)| S_t = s] \\\\\n",
    "&= \\mathbb E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n",
    "&= \\mathbb E_{\\pi}[R_{t+1} | S_t = s] + \\gamma \\mathbb E_{\\pi}[G_{t+1} | S_t = s] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，第一项为即时奖励：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb E_{\\pi}[R_{t+1} | S_t = s] &= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\mathbb E[R_{t+1} | S_t=s, A_t=a] \\\\\n",
    "&= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\sum_{r \\in R(s, a)} p(r | s, a) r\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第二项为未来奖励：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb E_{\\pi}[G_{t+1} | S_t = s] &= \\sum_{s' \\in \\cal S} \\pi(a|s) \\mathbb E[G_{t+1} | S_t=s, S_{t+1}=s']p(s'|s, a) \\\\\n",
    "&= \\sum_{s' \\in \\cal S} \\pi(a|s) \\mathbb E[G_{t+1} |S_{t+1}=s']p(s'|s, a) \\ \\ \\ \\ \\ \\text{Markov Property}\\\\\n",
    "&= \\sum_{s' \\in \\cal S} v_{\\pi}(s') p(s'|s, a) \\\\\n",
    "&= \\sum_{s' \\in \\cal S} v_{\\pi}(s') \\sum_{a \\in \\cal A(s)} p(s' | s, a) \\pi(a | s) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "故，**进一步地：**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb E_{\\pi}[R_{t+1} | S_t = s] + \\gamma \\mathbb E_{\\pi}[G_{t+1} | S_t = s] \\\\\n",
    "&= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\sum_{r \\in R(s, a)} p(r | s, a) r + \\gamma \\sum_{s' \\in \\cal S} v_{\\pi}(s') \\sum_{a \\in \\cal A(s)} p(s' | s, a) \\pi(a | s) \\\\\n",
    "&= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\Big [\\sum_{r \\in R(s, a)} p(r | s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s' | s, a) v_{\\pi}(s') \\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这就是Bellman Equation。\n",
    "\n",
    "进一步地，根据全概率公式有：\n",
    "$$\n",
    "p(s'|s,a) = \\sum_{r \\in R(s,a)} p(s',r|s,a) \\\\\n",
    "p(r|s,a) = \\sum_{s' \\in S} p(s',r|s,a)\n",
    "$$\n",
    "\n",
    "因此，Bellman Equation 可以写成：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\sum_{s' \\in \\cal S} \\sum_{r \\in R(s, a)} p(s', r | s, a) [r + \\gamma v_{\\pi}(s')] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于在一些问题中，奖励$r$依赖于下一步状态$s'$，因此下一状态奖励可以写成$r(s')$，从而$p(s'|s,a) = p(r(s')|s,a)$。所以Bellman Equation 进一步简化为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\sum_{s' \\in \\cal S} p(s'|s,a)[r(s') + \\gamma v_{\\pi}(s')] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-Vector form of the Bellman Equation\n",
    "\n",
    "令:\n",
    "$$\n",
    "r_{\\pi}(s) \\overset{\\cdot}= \\sum_{a \\in \\cal A(s)} \\pi(a|s) \\sum_{r \\in \\cal R(s, a)} p(r|s,a) r\n",
    "$$\n",
    "$$\n",
    "p_{\\pi} (s'|s) \\overset{\\cdot}=\\sum_{a \\in \\cal A(s)} \\pi(a|s) p(s'|s,a)\n",
    "$$\n",
    "\n",
    "则Bellman方程可以写成如下形式:\n",
    "$$\n",
    "v_{\\pi}(s) = r_{\\pi}(s) + \\gamma \\sum_{s' \\in \\cal S} p_{\\pi}(s'|s) v_{\\pi}(s')\n",
    "$$\n",
    "\n",
    "用矩阵向量形式表示为:\n",
    "$$\n",
    "v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}\n",
    "$$\n",
    "其中, $P_{\\pi}$ 是一个转移概率矩阵, 其元素为 $p_{\\pi}(s'|s)$， $P_{\\pi} \\in R^{n \\times n}$；$r_{\\pi}=[r_{\\pi}(s_1), \\cdots r_{\\pi}(s_n)]^T$ 是一个向量; $v_{\\pi}=[v_{\\pi}(s_1), \\cdots v_{\\pi}(s_n)]^T$ 也是一个向量, $n=｜\\cal S｜$是状态数量。\n",
    "\n",
    "**转移矩阵P的性质：**\n",
    "- 非负：因为转移矩阵的元素表示的是从一个状态转移到另一个状态的概率，所以它必须是非负的；\n",
    "- 随机：转移矩阵每一行元素之和为1，因为从一个状态出发，必然会转移到某个状态；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation in terms of Action Values\n",
    "\n",
    "由式\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) q_{\\pi}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\sum_{r \\in \\cal R(s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) v_{\\pi}(s')\n",
    "$$\n",
    "有：\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\sum_{r \\in \\cal R(s, a)} p(r|s, a) r + \\gamma \\sum_{s' \\in \\cal S} p(s'|s, a) \\sum_{a' \\in A(s')} \\pi(a'|s') q_{\\pi}(s', a')\n",
    "$$\n",
    "\n",
    "同样地，可以表示成Matric-Vector形式：\n",
    "$$\n",
    "q_{\\pi} = \\tilde{r} + \\gamma P^{\\pi} \\prod q_{\\pi}\n",
    "$$\n",
    "其中，$q_{\\pi}$是动作价值向量，$[q_{\\pi}]_{(s, a)} = q_{\\pi}(s, a)$；$\\tilde{r}$是与当前状态和执行动作相关的即时奖励向量，$[\\tilde{r}]_{(s, a)} = \\sum_{r \\in R(s, a)}p(r|s, a)r$；$P^{\\pi}$是转移概率矩阵, $[P]_{(s, a), s'} = p(s'|s, a)$；$\\prod$是一个块对角阵，每一个块是一个$1 \\times |\\cal A|$的向量——$\\prod_{s', (s', a')}=\\pi(a'|s')$，其他位置元素为0；$\\gamma$是折扣因子。\n",
    "\n",
    "与状态Bellman Equation不同的是：$\\tilde{r}$和$P$与策略$\\pi$无关，仅由模型决定；策略嵌入在$\\prod$当中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Example of Matric-Vector form\n",
    "<center>\n",
    "<img src=\"../Images/stochastic-policy-example.png\" alt=\"Stochastic Policy Example\">\n",
    "</center>\n",
    "\n",
    "\\begin{align*}\n",
    "& \\underbrace{\\left[\\begin{array}{c}\n",
    "v_\\pi(s_1) \\\\\n",
    "v_\\pi(s_2) \\\\\n",
    "v_\\pi(s_3) \\\\\n",
    "v_\\pi(s_4)\n",
    "\\end{array}\\right]}_{v_{\\pi}}\n",
    "= \\underbrace{\\left[\\begin{array}{c}\n",
    "r_\\pi(s_1) \\\\\n",
    "r_\\pi(s_2) \\\\\n",
    "r_\\pi(s_3) \\\\\n",
    "r_\\pi(s_4)\n",
    "\\end{array}\\right]}_{r_{\\pi}} + \\gamma \\underbrace{\\left[\\begin{array}{cccc}\n",
    "p_\\pi(s_1|s_1) & p_\\pi(s_2|s_1) & p_\\pi(s_3|s_1) & p_\\pi(s_4|s_1) \\\\\n",
    "p_\\pi(s_1|s_2) & p_\\pi(s_2|s_2) & p_\\pi(s_3|s_2) & p_\\pi(s_4|s_2) \\\\\n",
    "p_\\pi(s_1|s_3) & p_\\pi(s_2|s_3) & p_\\pi(s_3|s_3) & p_\\pi(s_4|s_3) \\\\\n",
    "p_\\pi(s_1|s_4) & p_\\pi(s_2|s_4) & p_\\pi(s_3|s_4) & p_\\pi(s_4|s_4)\n",
    "\\end{array}\\right]}_{P_{\\pi}} \\underbrace{\\left[\\begin{array}{c}\n",
    "v_\\pi(s_1) \\\\\n",
    "v_\\pi(s_2) \\\\\n",
    "v_\\pi(s_3) \\\\\n",
    "v_\\pi(s_4)\n",
    "\\end{array}\\right]}_{v_{\\pi}}\n",
    "\\end{align*}\n",
    "\n",
    "即：\n",
    "\\begin{align*}\n",
    "& \\underbrace{\\left[\\begin{array}{c}\n",
    "v_\\pi(s_1) \\\\\n",
    "v_\\pi(s_2) \\\\\n",
    "v_\\pi(s_3) \\\\\n",
    "v_\\pi(s_4)\n",
    "\\end{array}\\right]}_{v_{\\pi}}\n",
    "= \\underbrace{\\left[\\begin{array}{c}\n",
    "0.5 \\times (0) + 0.5 \\times (-1) \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]}_{r_{\\pi}} + \\gamma \\underbrace{\\left[\\begin{array}{cccc}\n",
    "0 & 0.5 & 0.5 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right]}_{P_{\\pi}} \\underbrace{\\left[\\begin{array}{c}\n",
    "v_\\pi(s_1) \\\\\n",
    "v_\\pi(s_2) \\\\\n",
    "v_\\pi(s_3) \\\\\n",
    "v_\\pi(s_4)\n",
    "\\end{array}\\right]}_{v_{\\pi}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving State Values from the Bellman Equation（*Policy Evaluation*）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closed-form Solution\n",
    "由Bellman Equatiion的Matric-Vector形式\n",
    "$$\n",
    "v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}\n",
    "$$\n",
    "有：\n",
    "$$\n",
    "v_{\\pi} = (I - \\gamma P_{\\pi})^{-1}r_{\\pi}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration Solution\n",
    "通常Bellman Equation的Closed-form用于理论分析，在实践中由于Closed-form需要求矩阵逆，当状态较多时计算复杂度较高。因此通常使用迭代方法求解Bellman Equation：\n",
    "$$\n",
    "v_{\\pi}^{k+1} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}^{k}, \\ \\ \\ k=0,1,...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples for illustrating the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deterministic Policy Example\n",
    "<center>\n",
    "<img src=\"../Images/deterministic-policy-example.png\" alt=\"Deterministic Policy Example\">\n",
    "</center\n",
    "\n",
    "由于策略已经给定，因此$\\pi (a=a_3|s=s_1) = 1$且$\\pi (a \\neq a_3|s=s_1) = 0$；同理，$p (a=a_3|s=s_1) = 1$且$p (a \\neq a_3|s=s_1) = 0$, $p (r=0|s=s_1, a=a_3) = 1$且$p (r \\neq 0|s=s_1, a=a_3) = 0$。\n",
    "\n",
    "故，各个状态价值如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s_1) &= 0 + \\gamma v_{\\pi} (s_3) = \\frac{\\gamma}{1-\\gamma} = \\frac{0.9}{1-0.9} = 9 \\\\\n",
    "v_{\\pi}(s_2) &= 1 + \\gamma v_{\\pi} (s_4) = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 10 \\\\\n",
    "v_{\\pi}(s_3) &= 1 + \\gamma v_{\\pi} (s_4) = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 10 \\\\\n",
    "v_{\\pi}(s_4) &= 1 + \\gamma v_{\\pi} (s_4) = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 10 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Policy Example\n",
    "<center>\n",
    "<img src=\"../Images/stochastic-policy-example.png\" alt=\"Stochastic Policy Example\">\n",
    "</center\n",
    "\n",
    "对于随机策略，每个状态有多个可能的动作，每个动作都有一定的概率被选择。假设在状态 $s_1$ 时，选择动作 $a_2$ 的概率为 $\\pi(a_2|s_1) = 0.5$，选择动作 $a_3$ 的概率为 $\\pi(a_3|s_1) = 0.5$；同理，$p (a=a_3|s=s_1) = 1$、p (a=a_2|s=s_1) = 1$且$p (a \\neq a_3 or a_2|s=s_1) = 0$, $p (r=0|s=s_1, a=a_3) = 1$、$p (r=-1|s=s_1, a=a_2) = 1$。\n",
    "\n",
    "故，各个状态价值如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s_1) &= 0.5 \\times (0 + \\gamma v_{\\pi} (s_3)) + 0.5 \\times (-1 + \\gamma v_{\\pi} (s_2)) = -0.5 + \\frac{\\gamma}{1-\\gamma} = -0.5 + \\frac{0.9}{1-0.9} = 8.5 \\\\\n",
    "v_{\\pi}(s_2) &= 1 + \\gamma v_{\\pi} (s_4) = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 10 \\\\\n",
    "v_{\\pi}(s_3) &= 1 + \\gamma v_{\\pi} (s_4) = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 10 \\\\\n",
    "v_{\\pi}(s_4) &= 1 + \\gamma v_{\\pi} (s_4) = \\frac{1}{1-\\gamma} = \\frac{1}{1-0.9} = 10 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
